\section{Statistical and Information Theoretic Foundations of Algorithmic Reasoning}
\cite{blackwell1953equivalent}
Here, we prove that $Arm 1 \simeq Arm 2 < Arm 3$ leveraging a similar breakdown framework as our experiments. We utilize information theory and statistical decision theory to sidestep representing the ambiguity of natural language and differences between NL and Code in a mathematical framework. Using the intuitions we gain in our experiments, we prove that Arm 2 is at least as good as Arm 1, and that Arm 3 is always better than Arm 2. 

\subsection{Arm 1 $\simeq$ Arm 2}
Our empirical results on experiment 3 leads us to hypothesize that natural language reaosning is a post-processing (garbling) of code \cite{blackwell1953equivalent} under a noisy channel paradigm of inference, leading code reasoning to be at least as good as NL reasoning quantified by Bayes Risk. 

% We show that the distribution of traces produced directly from the reasoning model can be approximated by post-processing the code with a fixed function across tasks and models. For each task instance $x$ drawn from a held-out test distribution over 21 different algorithmic problems in CLRS, we consider two conditional distributions: Arm~1: $p_{NL} (\cdot \mid x)$, original NL traces generated by an LLM (Gemini 2.0 Flash in our case) and Arm~2 Translated: $p_{Translated} (\cdot \mid x)$, NL traces obtained by first generating code by an LLM, then translating that code into natural language using a fixed translator.\yu{Need fix: this should be simulation not translation. $p_{code}$ is not defined}

\textbf{Setup.}
Let $X \sim p(x)$ denote the task instance (problem + inputs), drawn from a representative test distribution. Let $\mathcal{Y}$ be an output space (e.g., answer strings), and let $\ell : \mathcal{Y} \times X \to [0,1]$ be a bounded and measurable loss function (0--1 binary loss). In Arm 1 let the generative and bayesian inference process be the latent variable model:
\begin{align*}
    & \mathrm{Arm 1: } \; X \underbrace{\to}_{p_{\mathrm{NL}(Z_{\mathrm{NL}} | x)}} Z_{\mathrm{NL}} \underbrace{\to}_{\delta_{\mathrm{NL}}(y | x, z_{\mathrm{NL}}) } Y_{\mathrm{NL}} \\
    & P(Y_{\mathrm{NL}}|X=x) := \int \underbrace{\delta_{\mathrm{NL}}(y | x, z)}_{\text{Execute}} \,
    \underbrace{p_{\mathrm{NL}}(z | x)}_{\text{Generate}} \, \mathrm{d}z
\end{align*}
With the addition of a fixed LLM translator T (independent of x), let the generative and inference process for Arm 2.5 with translator be: 
\begin{align*}
& \mathrm{ Arm 2.5: } \; X \underbrace{\to}_{p_{\mathrm{C}(Z_{\mathrm{C}} | x)}} Z_{\mathrm{C}} \underbrace{\to}_{T ( \hat Z_\mathrm{NL} | z_{\mathrm{C}}) } \hat Z_{\mathrm{NL}}\underbrace{\to}_{\delta_{\mathrm{NL}}(y | x, \hat z_{\mathrm{NL}}) } \hat Y_{\mathrm{Tr}} \\
& P(\hat Y_{\mathrm{Tr}}|X=x) := \int \underbrace{\delta_{\mathrm{NL}}(y | x, \hat z)}_{\text{Execute}} \, \underbrace{T(\mathrm{d} \hat z | z)}_{\text{Translate}} \, 
\underbrace{p_{\mathrm{Code}}(\mathrm{d}z | x)}_{\text{Generate}} \, \mathrm{d}z
\end{align*}
Let the original Arm 2 be: 
\begin{align*}
& \mathrm{Arm 2: } \; X \underbrace{\to}_{p_{\mathrm{C}(Z_{\mathrm{C}} | x)}} Z_{\mathrm{C}} \underbrace{\to}_{\delta_{\mathrm{Sim}}(y | x, z_{\mathrm{C}}) } Y_{\mathrm{Sim}} \\
& P( Y_{\mathrm{Sim}}|X=x) := \int \underbrace{\delta_{\mathrm{Sim}}(y | x, z)}_{\text{Execute}}  \, 
\underbrace{p_{\mathrm{Code}}(\mathrm{d}z | x)}_{\text{Generate}} \, \mathrm{d}z
\end{align*}

% \begin{align*}
%     & \mathrm{P}(Y \mid X = x) := \int T(z \mid z_{\mathrm{Code}}) \, p_{\mathrm{Code}}(z_{\mathrm{Code}} \mid x) \, \mathrm{d}z_{\mathrm{Code}}
% \end{align*}
% In Arm~1 and Arm~2, each arm corresponds to an intermediate representation $Z$ (e.g. CoT) produced by channel $p(z \mid x)$ (e.g. LLM), and then choosing an output $Y$ via a randomized decision rule $\delta(y \mid x, z)$ (i.e. LLM test-time reasoning). 
We show that for some negligible $\varepsilon$, the Bayes Risk $R^*(Z)$ for code is less than for natural language. 
\begin{align*}
& R^*(Z) := \inf_{\delta} \, \mathbb{E}[\ell(Y, X)] \\
& R^*(Z_{\mathrm{Sim}}) \leq R^*(Z_{\mathrm{NL}}) + O(\varepsilon)
% &\quad X \sim p, \; Z \sim p(\cdot \mid X), \; Y \sim \delta(\cdot \mid X, Z).
\end{align*}
% In the first arm, we observe $Z_{\mathrm{NL}} \sim p_{\mathrm{NL}}(\cdot \mid X)$.
% For the second arm, we have $Z_{\mathrm{Code}} \sim p_{\mathrm{Code}}(\cdot \mid X)$.
$\varepsilon$ is the maximum change in expected loss when replacing the true distribution by the approximated distribution, defined below in Assumption 2. Empirically, we showed that this value was small in experiment 2. 

\textbf{Assumption 1.}
We assume there exists a (near Bayes optimal) translator $T$ mapping code to natural language reasoning $Z_{\mathrm{Code}} \to \hat{Z}_{\mathrm{NL}}$ independent of X.
% , such that the Markov chain $$X \underbrace{\to}_{\text{Arm 2 LLM}} Z_{\mathrm{Code}} \underbrace{\to}_{T} \hat{Z}_{\mathrm{NL}}$$ is representative of LLM test-time reasoning with CoT, with the final decision stage being $\delta(y \mid X, \hat{Z}_{\mathrm{NL}})$. Suppose, $\hat{Z}_{\mathrm{NL}} \sim p_{\mathrm{Tr}}(\cdot \mid x)$, the markov chain is equivalently:
% \begin{align*}
% & p_{\mathrm{Tr}}(z \mid X) := \int T(z \mid z_{\mathrm{Code}}) \, p_{\mathrm{Code}}(z_{\mathrm{Code}} \mid x) \, \mathrm{d}z_{\mathrm{Code}}
% \end{align*}

\textbf{Assumption 2.}
We assume that the original NL reasoning chain of thought is close to the translated NL on average. Let $p_{\mathrm{NL}}$ be the Arm~1 channel and $p_{\mathrm{translated}}(\cdot \mid x)$ be the translated NL channel. Assume an average conditional TV bound:
\[
\mathbb{E}_{X \sim p} \bigl[
d_{\mathrm{TV}}\bigl(p_{\mathrm{NL}}(\cdot \mid X), \, p_{\mathrm{translated}}(\cdot \mid X)\bigr)
\bigr] \leq \varepsilon,
\]
where
\[
d_{\mathrm{TV}}(P, Q) = \sup_{B} |P(B) - Q(B)|.
\]
In other words, averaged over task instances, the NL trace produced by Arm~1 is close in distribution to the NL traces obtained by translating the code trace (Arm~2) using the translator $T$. We empirically verify this in experiment 2. 

\begin{proof}
Our high level strategy is to use Blackwell's simulation principle, and show that NL reasoning is just code generation plus a noisy translation, thus a code-based agent can simulate exactly what the NL agent would do by averaging over that noise internally. In other words, under Assumptions~1--2, for the bounded loss $\ell \in [0,1]$,
\[
R^*(Z_{\mathrm{Code}}) \leq R^*(Z_{\mathrm{NL}}) + O(\varepsilon).
\]
\textbf{Step 1: Simulate NL from code via translation.}
Here, we first show that for any NL-based policy, there exists a code-based policy that induces the same input-output behaviour. 

Here we first translate the input problem into CoT, then execute the CoT via LLM reasoning, similar to what we do in the Arm 2 experiments. We abuse notation here, and $z = z_{\mathrm{Code}}$. Define code simulation to be an implicit translation then reason in the original simulation branch:
\[
\delta_{\mathrm{Sim}}(y \mid x, z)
:= \int \underbrace{\delta_{\mathrm{NL}}(y \mid x, \hat z)}_{\text{Execute}} \,
\underbrace{T(\hat z \mid z)}_{\text{Translate}} \, \mathrm{d}z.
\]
Then, we can apply the law of iterated expectation on Arm 2 to get Arm 2.5:
\begin{align*}
& P( Y_{\mathrm{Sim}}|X=x) := \int \underbrace{\delta_{\mathrm{Sim}}(y | x, z)}_{\text{Execute}}  \, 
\underbrace{p_{\mathrm{Code}}(\mathrm{d}z | x)}_{\text{Generate}} \, \mathrm{d}z \\ 
& = \int \int [ \underbrace{\delta_{\mathrm{NL}}(y | x, \hat z)}_{\text{Execute}} \, \underbrace{T(\mathrm{d} \hat z | z)}_{\text{Translate}}] \, 
\underbrace{p_{\mathrm{Code}}(\mathrm{d}z | x)}_{\text{Generate}} \, \mathrm{d}z \\
&= P(\hat Y_{\mathrm{Tr}}|X=x) 
\end{align*}
The joint distributions $(X, Y_{\mathrm{Sim}})$ and $(X, \hat{Y}_{\mathrm{translated}})$ are the same. Thus,
\[
\mathbb{E}[\ell(Y_{\mathrm{Sim}}, X)] = \mathbb{E}[\ell(\hat{Y}_{\mathrm{translated}}, X)].
\]
\textbf{Step 2: Substitute translated NL and original NL via TV lemma.} Here we show that if two signals look similar, they perform similarly. Even if translated NL is not exactly native NL, bounded loss decision problems cannot exploit small distributional differences, a property of the continuity property of Bayes Risk. 
\begin{lemma}[TV Lemma]
Let $X \sim p(x)$. Let $Z \mid X = x \sim P_x$ and $Z' \mid X = x \sim Q_x$. Let $g(x, z) \in [0, 1]$ be measurable. Then
\[
\mathbb{E}[g(X, Z)] - \mathbb{E}[g(X, Z')] \leq \mathbb{E}_X \bigl[ d_{\mathrm{TV}}(P_X, Q_X) \bigr].
\]
\end{lemma}
For each $x$ and trace $z$, define $g(x, z) := \mathbb{E}_{y|x,z}[\ell(y, x)]$. Then $g(x, z) \in [0, 1]$. Note that
\begin{align*}
\mathbb{E}_{(Y_{\mathrm{NL}}, X)}[\ell(Y_{\mathrm{NL}}, X)] &= \mathbb{E}_{(X, Z_{\mathrm{NL}})}[g(X, Z_{\mathrm{NL}})], \\
\mathbb{E}_{(\hat Y_{\mathrm{Tr}}, X)}[\ell(\hat{Y}_{\mathrm{translated}}, X)] &= \mathbb{E}_{(X, \hat{Z}_{\mathrm{NL}})}[g(X, \hat{Z}_{\mathrm{NL}})].
\end{align*}

Applying the TV lemma with $P_x = p_{\mathrm{NL}}(\cdot \mid x)$ and $Q_x = p_{\mathrm{translated}}(\cdot \mid x)$:
\begin{align*}
&\bigl| \mathbb{E}[\ell(Y_{\mathrm{NL}}, X)] - \mathbb{E}[\ell(\hat{Y}_{\mathrm{translated}}, X)] \bigr| \\
&\quad = \bigl| \mathbb{E}[g(X, Z_{\mathrm{NL}})] - \mathbb{E}[g(X, \hat{Z}_{\mathrm{NL}})] \bigr| \\
&\quad \leq \mathbb{E}_X \bigl[ d_{\mathrm{TV}}(p_{\mathrm{NL}}(\cdot \mid X), \, p_{\mathrm{translated}}(\cdot \mid X)) \bigr]
\leq \varepsilon.
\end{align*}

Therefore, rearranging gives
\begin{align*}
&\mathbb{E}[\ell(\hat{Y}_{\mathrm{translated}}, X)] \leq \mathbb{E}[\ell(Y_{\mathrm{NL}}, X)] + \varepsilon \\
&\mathbb{E}[\ell(Y_{\mathrm{Sim}}, X)]
= \mathbb{E}[\ell(\hat{Y}_{\mathrm{translated}}, X)]
\leq \mathbb{E}[\ell(Y_{\mathrm{NL}}, X)] + \varepsilon.
\end{align*}
We add superscripts to denote the decision rule (LLM executor) that was used. Then, since the inequality holds for arbitrary LLM executors $\delta_{\mathrm{NL}}$ (including the best), we get: 
\[
\inf_{(\delta_\mathrm{Sim}, \delta_\mathrm{NL})} \mathbb{E}[\ell(Y_{\mathrm{Sim}}^{(\delta_{\mathrm{Sim}} )}, X)]
\leq \inf_{(\delta_\mathrm{Sim}, \delta_\mathrm{NL})} \mathbb{E}[\ell(Y_{\mathrm{NL}}^{(\delta_{\mathrm{NL}})}, X)] + \varepsilon.
\]
Therefore, ignoring infimums over independent variables $\delta_\mathrm{NL}$ on the left and $\delta_\mathrm{Sim}$ on the right, we get exactly the Bayes Risk defined before.
\begin{framed}
\qquad \qquad \quad $R^*(Z_{\mathrm{Sim}}) \leq R^*(Z_{\mathrm{NL}}) + \varepsilon$
\end{framed}
\end{proof}

\vspace{-20pt}
\subsection{Arm~2 $<$ Arm~3}
We prove that deterministic execution in Arm 3 yields strictly lower Bayes Risk. 

\textbf{Setup.} Let $X \sim p(x)$ be a task instance and let $Y^*(x)$ denote the corresponding ground-truth. Suppose $Z_{\mathrm{Sim}} \sim p_{\mathrm{Sim}}$ is the code produced by the model. Let $g$ be a deterministic python3 runtime. Define the two arms:
\begin{align*}
    & \mathrm{Arm2}: Y_{\mathrm{Sim}} \sim \delta_{\mathrm{Sim}} (\cdot | X, Z_{\mathrm{Sim}}) \\
    & \mathrm{Arm2}: Y_{\mathrm{Exec}} := g(X, Z_{\mathrm{Sim}})
\end{align*}
\begin{proof}
    Under $\ell(y, x) = \mathbf{1}\{y \neq Y^*(x)\}$, the solver achieves zero risk ($R^*_3 = 0$) whenever the generated code is correct. In contrast, LLM simulation incurs positive risk ($R^*_2 > 0$) whenever $\Pr[Y_2 \neq Y_3] > 0$, which occurs empirically due to execution errors in mental simulation. 
    \begin{framed}
        \qquad \qquad \qquad \qquad \quad $R^*_3 < R^*_2$
    \end{framed} 
\end{proof}
The only scenario where Arm~2 could outperform Arm~3 is when the generated code is \emph{incorrect}, yet the LLM ``recovers'' by reasoning to the correct answer despite the flawed code. We empirically quantify this recovery rate below. 

\textbf{Recovery reduces as tasks get harder.}
To further reinforce this result, we rule out the possibilities of recovery as tasks get harder, eliminating any benefit of running Arm~2:
\begin{enumerate}[leftmargin=*]
    \item Arm~3 produces an incorrect answer (implying incorrect code generation), and
    \item Arm~2 produces the correct answer (implying successful LLM recovery).
\end{enumerate}

\cref{fig:recovery_final} presents the recovery analysis across all tasks and models. The recovery rate remains consistently low (typically $< 5\%$), indicating that LLM simulation rarely compensates for code generation errors. This confirms that Arm~3's advantage stems from reliable solver execution rather than Arm~2's inability to reason about code.

\begin{figure}[t]
    \centering
    \vspace{-5pt}
    \includegraphics[width=0.85\linewidth]{images/recovery_final.png}
    \vspace{-5pt}
    \caption{\textbf{Recovery rate.} How often Arm~2 succeeds when Arm~3 fails. Recovery rates are $<$5\% across all tasks, confirming Arm~3's advantage comes from reliable execution, not LLM compensation.}
    \label{fig:recovery_final}
\end{figure}


