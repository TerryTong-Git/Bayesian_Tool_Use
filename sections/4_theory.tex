\section{Statistical and Information Theoretic Foundations of Algorithmic Reasoning}

We claim that natural language reasoning is a garbling of code \cite{blackwell1953equivalent} under a noisy channel paradigm of inference, leading code reasoning to be at least as good as NL reasoning. \yu{define/introduce garbling}

We show that the distribution of traces produced directly from the reasoning model can be approximated by post-processing the code with a fixed function across tasks and models. For each task instance $x$ drawn from a held-out test distribution over 21 different algorithmic problems in CLRS, we consider two conditional distributions: Arm~1: $p_{NL} (\cdot \mid x)$, original NL traces generated by an LLM (Gemini 2.0 Flash in our case) and Arm~2 Translated: $p_{Translated} (\cdot \mid x)$, NL traces obtained by first generating code by an LLM, then translating that code into natural language using a fixed translator.\yu{Need fix: this should be simulation not translation. $p_{code}$ is not defined}

\textbf{Setup.}
Let $X \sim p(x)$ denote the task instance (problem + inputs), drawn from a representative test distribution. Let $\mathcal{Y}$ be an output space (e.g., answer strings), and let $\ell : \mathcal{Y} \times X \to [0,1]$ be a bounded and measurable loss function (0--1 binary loss). In Arm~1 and Arm~2, each arm corresponds to an intermediate representation $Z$ (e.g. CoT) produced by channel $p(z \mid x)$ (e.g. LLM), and then choosing an output $Y$ via a randomized decision rule $\delta(y \mid x, z)$ (i.e. LLM test-time reasoning).

For any CoT observation $Z$, define the Bayes risk:
\begin{align*}
R^*(Z) &:= \inf_{\delta} \, \mathbb{E}[\ell(Y, X)], \\
&\quad X \sim p, \; Z \sim p(\cdot \mid X), \; Y \sim \delta(\cdot \mid X, Z).
\end{align*}

In the first arm, we observe $Z_{\mathrm{NL}} \sim p_{\mathrm{NL}}(\cdot \mid X)$.
For the second arm, we have $Z_{\mathrm{Code}} \sim p_{\mathrm{Code}}(\cdot \mid X)$.

We show that $R^*(Z_{\mathrm{Code}}) \leq R^*(Z_{\mathrm{NL}}) + O(\varepsilon)$ for some negligible $\varepsilon$. \yu{define $\varepsilon$}

\subsection{Assumptions}

\textbf{Assumption 1.}
We assume there exists a stochastic kernel $T$ \yu{define T, citation} such that the Markov chain $X \to Z_{\mathrm{Code}} \to \hat{Z}_{\mathrm{NL}}$ is representative of CoT, with the final decision stage being $\delta(y \mid X, \hat{Z}_{\mathrm{NL}})$. \yu{explain why this can be a markov chain.} That is,
\begin{align*}
\hat{Z}_{\mathrm{NL}} &\sim p_{\mathrm{translated}}(\cdot \mid x), \; p_{\mathrm{translated}}(z \mid X)  \\
&:= \int T(z \mid z_{\mathrm{Code}}) \, p_{\mathrm{Code}}(z_{\mathrm{Code}} \mid x) \, \mathrm{d}z_{\mathrm{Code}}.
\end{align*}

\textbf{Assumption 2.}
We assume that the original NL reasoning chain of thought is close to the translated NL on average. Let $p_{\mathrm{NL}}$ be the Arm~1 channel and $p_{\mathrm{translated}}(\cdot \mid x)$ be the translated NL channel. Assume an average conditional TV bound:
\[
\mathbb{E}_{X \sim p} \bigl[
d_{\mathrm{TV}}\bigl(p_{\mathrm{NL}}(\cdot \mid X), \, p_{\mathrm{translated}}(\cdot \mid X)\bigr)
\bigr] \leq \varepsilon,
\]
where
\[
d_{\mathrm{TV}}(P, Q) = \sup_{B} |P(B) - Q(B)|.
\]

In other words, averaged over task instances, the NL trace produced by Arm~1 is close in distribution to the NL traces obtained by translating the code trace (Arm~2) using the translator $T$ (Markov kernel).


\subsection{Evaluating Translated NL and NL Distributional Similarity.} 

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{images/translation_additivity_plot.png}
    \caption{Recovery rate analysis: proportion of cases where LLM simulation (Arm~2) produces the correct answer despite incorrect code (Arm~3 failure). Recovery rates remain below 5\% across all task families and models, confirming that solver execution dominates LLM simulation.}
    \label{fig:translation_additivity}
\end{figure}



In this section, we empirically validate the two assumptions stated above. One key hypothesis is that natural language reasoning follows a deeper algorithmic procedure encoded in its representations. If this is the case, it would make sense to surface the code and send it to an executor. We test whether natural language reasoning generated by information contained in the code alone can be distributionally similar to natural language reasoning generated from the prompt alone in Arm~1. Then, we test whether they are functionally similar. 

\subsubsection{Distributional Similarity between Translated NL and original NL} 

\textbf{Evaluation Setup}  We formulate a binary classification task in which a powerful judge model (Claude Opus 4.5) is given a problem instance x, and a single reasoning trace z, and must predict whether z was generated by Arm~1, or the translated Arm~2. This setup corresponds to discriminating between the joint distributions $p(x)p_{NL}(z \mid x)$ and $p(x)p_{Translated}(z \mid x)$. For each evaluation task x, we construct two samples $(x, z_{NL})$, where $z_{NL} \sim p_{NL}(\cdot \mid X)$, labelled Original, and $(x, \hat z_{NL})$ where $\hat z_{NL} \sim p_{Translated} ( \cdot \mid x)$, labelled Translated. The label counts are balanced on the test set, and the judge's accuracy, and AUC are computed on held-out tasks (disjoint from any in-context prompting examples).

In the evaluation, all generation parameters are fixed, model, prompt, and decoding hyperparameters, and the only variability is the seed for both the original Arm~1 and translated Arm~2, i.e. we compare samples from two fixed conditionals given a task instance. 

We ensure the judge has enough discriminative power by asking it to classify between raw code and the native NL reasoning. High accuracy would indicate that the performance results on the main evaluation is not due to an underpowered judge. We prompt the judge zero-shot, instructing it that there are original natural language reasoning traces and translated ones that it must distinguish. 

The translator, gemini-2.0 flash, is given 10 in-context pairs between code in Arm~2 and original natural language reasoning in Arm~1, and asked to learn the mapping and apply on new instances. The 10 in-context examples are 10 different tasks, which are disjoint from the 21 other tasks in CLRS benchmark that we evaluate on. 



\textbf{Results.} Across 2000 samples, we find that the accuracy of the prompt is 49.4\% $\in [47.2\%. 51.5\%]$ Wilson CI, obtaining a 0.479 AUC. The control accuracy is $79.0\%$ so the discriminator is calibrated. We include per-task analysis in the appendix. We see that this is generally true across different models with only a few exceptions between rod cutting and the kruskal algorithm problems. 

% We hypothesize that the natural language reasoning in Arm 1 can be obtained by post-processing Arm 2. We fix a post-processer P, which is an LLM with a fixed prompt in our evaluation. The prompt in P has 10 in-context examples that shows how to translate between code and the original reasoning. We fix these across different tasks, and evaluate on held-out tasks in the CLRS 30 benchmark data (n=1500). We evaluate with a fixed translator (GPT 5.2 Pro) and a fixed evaluator (Claude Opus 4).

\subsubsection{Functional Similarity between Translated NL and original NL}

We wish to verify whether the translated natural language reaosning from the experiment before has the same functionality as the original natural language reasoning produced in Arm~1.

\textbf{Evaluation Setup}. Given a task instance $x$, we prompt the target language model (gemini) three ways. 1. Baseline: $x$ (question only), 2. Arm~1: $x || z_{NL}$ 3. Translated Arm~2: $x || \hat z_{NL}$ where $\hat z_{NL}$ is obtained by translating code to natural language. If the translated NL loses information relative to the original NL, then conditioning on $\hat z_{NL}$ should yield worse performance than conditioning on $z_{NL}$. We run on 1000 samples and report results on held-out tasks disjoint from the ICL example tasks. The same prompt is used for the translator in the previous experiment as the one used here. 

\textbf{Results}. We fail to reject the null hypothesis on 1000 samples, and see that using the original Arm~1 and the translated Arm~2 concatenated to the same original task prompt yield the same results. 

\textbf{Qualitative Analysis} In observing the LLM responses, we notice the model's have a propensity to follow similar reasoning. This happens across individual models, but problem solving methodology is surprisingly similar across models. Show figure here. 





% We wish to verify whether the translated NL and the NL are similar in distribution. By taking a piece of code, then translating and simulating it into natural language reasoning, we get a piece of CoT that is similar to the original. We ask whether this is distinguishable, in order to verify assumption 2. Assumption 1 is validated by design, since we can create a translator T via prompting an LLM. 

% Since the translator $T(Z_nl | Z_code)$ is independent of the task X, we prompt the translator LLM (GPT 5.2) to not see the task, but rather use a fixed prompt with in-context examples from adjacent translating tasks (5-shot). During the translation, test examples' problem tasks are not shown to the translator, only the fixed prompt and piece of code. The evaluation is conducted on held-out tasks not seen during the tuning of the prompt, and the prompt is frozen after tuning. Translator is applied uniformly across tasks, prompts, and decoding params. 



\subsection{Proof}

\begin{proof}
Under Assumptions~1--2, for the bounded loss $\ell \in [0,1]$,
\[
R^*(Z_{\mathrm{Code}}) \leq R^*(Z_{\mathrm{NL}}) + \varepsilon.
\]

\textbf{Step 1: Simulate NL from code via translation.}
Here we first translate the input problem into CoT, then execute the CoT:
\[
\delta_{\mathrm{Code}}(y \mid x, z_{\mathrm{Code}})
:= \int \underbrace{\delta_{\mathrm{NL}}(y \mid x, z)}_{\text{Execute}} \,
\underbrace{T(z \mid z_{\mathrm{Code}})}_{\text{Translate}} \, \mathrm{d}z.
\]

Let $Y_{\mathrm{Code}} \sim \delta_{\mathrm{Code}}(\cdot \mid X, Z_{\mathrm{Code}})$. Let $\hat{Y}_{\mathrm{translated}} \sim \delta_{\mathrm{NL}}(\cdot \mid X, \hat{Z}_{\mathrm{NL}})$, where $\hat{Z}_{\mathrm{NL}}$ is produced from $Z_{\mathrm{Code}}$ via the translator kernel $T$.

The joint distributions $(X, Y_{\mathrm{Code}})$ and $(X, \hat{Y}_{\mathrm{translated}})$ are the same. Thus,
\[
\mathbb{E}[\ell(Y_{\mathrm{Code}}, X)] = \mathbb{E}[\ell(\hat{Y}_{\mathrm{translated}}, X)].
\]

This is because conditional on $X = x$, sampling $Z_{code} \sim p_{code}(\cdot \mid x)$, then $\hat Z_{nl} \sim T(\cdot \mid Z_{code})$, then $Y \sim \delta_{NL} (\cdot \mid x, \hat Z_{NL})$ induces the same conditional distribution over Y as $Y \sim \delta_{code} (\cdot \mid x, Z_{code})$.

\textbf{Step 2: Substitute translated NL and original NL via TV lemma.}

\begin{lemma}[TV Lemma]
Let $X \sim p(x)$. Let $Z \mid X = x \sim P_x$ and $Z' \mid X = x \sim Q_x$. Let $g(x, z) \in [0, 1]$ be measurable. Then
\[
\mathbb{E}[g(X, Z)] - \mathbb{E}[g(X, Z')] \leq \mathbb{E}_X \bigl[ d_{\mathrm{TV}}(P_X, Q_X) \bigr].
\]
\end{lemma}

Suppose we have $Y_{\mathrm{NL}} \sim \delta_{\mathrm{NL}}(\cdot \mid Z_{\mathrm{NL}})$ under the actual channel $p_{\mathrm{NL}}(\cdot \mid x)$. For each $x$ and trace $z$, define $g(x, z) := \mathbb{E}_{y \sim \delta(\cdot \mid z)}[\ell(y, x)]$.

Then $g(x, z) \in [0, 1]$. Note that
\begin{align*}
\mathbb{E}[\ell(Y_{\mathrm{NL}}, X)] &= \mathbb{E}[g(X, Z_{\mathrm{NL}})], \\
\mathbb{E}[\ell(\hat{Y}_{\mathrm{translated}}, X)] &= \mathbb{E}[g(X, \hat{Z}_{\mathrm{NL}})].
\end{align*}

Applying the TV lemma with $P_x = p_{\mathrm{NL}}(\cdot \mid x)$ and $Q_x = p_{\mathrm{translated}}(\cdot \mid x)$:
\begin{align*}
&\bigl| \mathbb{E}[\ell(Y_{\mathrm{NL}}, X)] - \mathbb{E}[\ell(\hat{Y}_{\mathrm{translated}}, X)] \bigr| \\
&\quad = \bigl| \mathbb{E}[g(X, Z_{\mathrm{NL}})] - \mathbb{E}[g(X, \hat{Z}_{\mathrm{NL}})] \bigr| \\
&\quad \leq \mathbb{E}_X \bigl[ d_{\mathrm{TV}}(p_{\mathrm{NL}}(\cdot \mid X), \, p_{\mathrm{translated}}(\cdot \mid X)) \bigr]
\leq \varepsilon.
\end{align*}

Therefore, rearranging gives
\[
\mathbb{E}[\ell(\hat{Y}_{\mathrm{translated}}, X)] \leq \mathbb{E}[\ell(Y_{\mathrm{NL}}, X)] + \varepsilon.
\]
Thus,
\[
\mathbb{E}[\ell(Y_{\mathrm{Code}}, X)]
= \mathbb{E}[\ell(\hat{Y}_{\mathrm{translated}}, X)]
\leq \mathbb{E}[\ell(Y_{\mathrm{NL}}, X)] + \varepsilon.
\]

Since this holds for arbitrary NL rule $\delta_{\mathrm{NL}}$, taking the infimum over $\delta_{\mathrm{NL}}$ on the right-hand side yields $R^*(Z_{\mathrm{Code}}) \leq R^*(Z_{\mathrm{NL}}) + \varepsilon$.
\end{proof}

\subsection{Arm~2 $<$ Arm~3}

We model the comparison between LLM simulation (Arm~2) and solver execution (Arm~3) under the following assumptions:
\begin{enumerate}[leftmargin=*]
    \item \textbf{Solver correctness.} Given correct code $Z$, the solver deterministically produces the ground-truth answer: $Y_3 = g(X, Z) = Y^*(X)$.
    \item \textbf{Noisy simulation.} LLM simulation adds stochastic noise: $Y_2 \sim N(\cdot \mid Y_3, X, Z)$ for some noise kernel $N$.
    \item \textbf{0-1 loss.} We evaluate under $\ell(y, x) = \mathbf{1}\{y \neq Y^*(x)\}$.
\end{enumerate}

Under these assumptions, the solver achieves zero risk ($R_3 = 0$) whenever the generated code is correct. In contrast, LLM simulation incurs positive risk ($R_2 > 0$) whenever $\Pr[Y_2 \neq Y_3] > 0$, which occurs empirically due to execution errors in mental simulation. Therefore, $R_3 < R_2$.

The only scenario where Arm~2 could outperform Arm~3 is when the generated code is \emph{incorrect}, yet the LLM ``recovers'' by reasoning to the correct answer despite the flawed code. We empirically quantify this recovery rate below. 

\textbf{Recovery reduces as tasks get harder.}
To further reinforce this result, we rule out the possibilities of recovery as tasks get harder, eliminating any benefit of running Arm~2:
\begin{enumerate}[leftmargin=*]
    \item Arm~3 produces an incorrect answer (implying incorrect code generation), and
    \item Arm~2 produces the correct answer (implying successful LLM recovery).
\end{enumerate}

\cref{fig:recovery_final} presents the recovery analysis across all tasks and models. The recovery rate remains consistently low (typically $< 5\%$), indicating that LLM simulation rarely compensates for code generation errors. This confirms that Arm~3's advantage stems from reliable solver execution rather than Arm~2's inability to reason about code.

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{images/recovery_final.png}
    \caption{Recovery rate analysis: proportion of cases where LLM simulation (Arm~2) produces the correct answer despite incorrect code (Arm~3 failure). Recovery rates remain below 5\% across all task families and models, confirming that solver execution dominates LLM simulation.}
    \label{fig:recovery_final}
\end{figure}


