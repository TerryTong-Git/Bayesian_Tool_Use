\section{Statistical and Information Theoretic Foundations of Algorithmic Reasoning}

We claim that natural language reasoning is a garbling of code \cite{blackwell1953equivalent} under a noisy channel paradigm of inference, leading code reasoning to be at least as good as NL reasoning. \yu{define/introduce garbling}

We show that the distribution of traces produced directly from the reasoning model can be approximated by post-processing the code with a fixed function across tasks and models. For each task instance $x$ drawn from a held-out test distribution over 21 different algorithmic problems in CLRS, we consider two conditional distributions: Arm~1: $p_{NL} (\cdot \mid x)$, original NL traces generated by an LLM (Gemini 2.0 Flash in our case) and Arm~2 Translated: $p_{Translated} (\cdot \mid x)$, NL traces obtained by first generating code by an LLM, then translating that code into natural language using a fixed translator.\yu{Need fix: this should be simulation not translation. $p_{code}$ is not defined}

\textbf{Setup.}
Let $X \sim p(x)$ denote the task instance (problem + inputs), drawn from a representative test distribution. Let $\mathcal{Y}$ be an output space (e.g., answer strings), and let $\ell : \mathcal{Y} \times X \to [0,1]$ be a bounded and measurable loss function (0--1 binary loss). In Arm~1 and Arm~2, each arm corresponds to an intermediate representation $Z$ (e.g. CoT) produced by channel $p(z \mid x)$ (e.g. LLM), and then choosing an output $Y$ via a randomized decision rule $\delta(y \mid x, z)$ (i.e. LLM test-time reasoning).

For any CoT observation $Z$, define the Bayes risk:
\begin{align*}
R^*(Z) &:= \inf_{\delta} \, \mathbb{E}[\ell(Y, X)], \\
&\quad X \sim p, \; Z \sim p(\cdot \mid X), \; Y \sim \delta(\cdot \mid X, Z).
\end{align*}

In the first arm, we observe $Z_{\mathrm{NL}} \sim p_{\mathrm{NL}}(\cdot \mid X)$.
For the second arm, we have $Z_{\mathrm{Code}} \sim p_{\mathrm{Code}}(\cdot \mid X)$.

We show that $R^*(Z_{\mathrm{Code}}) \leq R^*(Z_{\mathrm{NL}}) + O(\varepsilon)$ for some negligible $\varepsilon$. \yu{define $\varepsilon$}

\subsection{Assumptions}

\textbf{Assumption 1.}
We assume there exists a stochastic kernel $T$ \yu{define T, citation} such that the Markov chain $X \to Z_{\mathrm{Code}} \to \hat{Z}_{\mathrm{NL}}$ is representative of CoT, with the final decision stage being $\delta(y \mid X, \hat{Z}_{\mathrm{NL}})$. \yu{explain why this can be a markov chain.} That is,
\begin{align*}
\hat{Z}_{\mathrm{NL}} &\sim p_{\mathrm{translated}}(\cdot \mid x), \; p_{\mathrm{translated}}(z \mid X)  \\
&:= \int T(z \mid z_{\mathrm{Code}}) \, p_{\mathrm{Code}}(z_{\mathrm{Code}} \mid x) \, \mathrm{d}z_{\mathrm{Code}}.
\end{align*}

\textbf{Assumption 2.}
We assume that the original NL reasoning chain of thought is close to the translated NL on average. Let $p_{\mathrm{NL}}$ be the Arm~1 channel and $p_{\mathrm{translated}}(\cdot \mid x)$ be the translated NL channel. Assume an average conditional TV bound:
\[
\mathbb{E}_{X \sim p} \bigl[
d_{\mathrm{TV}}\bigl(p_{\mathrm{NL}}(\cdot \mid X), \, p_{\mathrm{translated}}(\cdot \mid X)\bigr)
\bigr] \leq \varepsilon,
\]
where
\[
d_{\mathrm{TV}}(P, Q) = \sup_{B} |P(B) - Q(B)|.
\]

In other words, averaged over task instances, the NL trace produced by Arm~1 is close in distribution to the NL traces obtained by translating the code trace (Arm~2) using the translator $T$ (Markov kernel).



% We wish to verify whether the translated NL and the NL are similar in distribution. By taking a piece of code, then translating and simulating it into natural language reasoning, we get a piece of CoT that is similar to the original. We ask whether this is distinguishable, in order to verify assumption 2. Assumption 1 is validated by design, since we can create a translator T via prompting an LLM. 

% Since the translator $T(Z_nl | Z_code)$ is independent of the task X, we prompt the translator LLM (GPT 5.2) to not see the task, but rather use a fixed prompt with in-context examples from adjacent translating tasks (5-shot). During the translation, test examples' problem tasks are not shown to the translator, only the fixed prompt and piece of code. The evaluation is conducted on held-out tasks not seen during the tuning of the prompt, and the prompt is frozen after tuning. Translator is applied uniformly across tasks, prompts, and decoding params. 



\subsection{Proof}

\begin{proof}
Under Assumptions~1--2, for the bounded loss $\ell \in [0,1]$,
\[
R^*(Z_{\mathrm{Code}}) \leq R^*(Z_{\mathrm{NL}}) + \varepsilon.
\]

\textbf{Step 1: Simulate NL from code via translation.}
Here we first translate the input problem into CoT, then execute the CoT:
\[
\delta_{\mathrm{Code}}(y \mid x, z_{\mathrm{Code}})
:= \int \underbrace{\delta_{\mathrm{NL}}(y \mid x, z)}_{\text{Execute}} \,
\underbrace{T(z \mid z_{\mathrm{Code}})}_{\text{Translate}} \, \mathrm{d}z.
\]

Let $Y_{\mathrm{Code}} \sim \delta_{\mathrm{Code}}(\cdot \mid X, Z_{\mathrm{Code}})$. Let $\hat{Y}_{\mathrm{translated}} \sim \delta_{\mathrm{NL}}(\cdot \mid X, \hat{Z}_{\mathrm{NL}})$, where $\hat{Z}_{\mathrm{NL}}$ is produced from $Z_{\mathrm{Code}}$ via the translator kernel $T$.

The joint distributions $(X, Y_{\mathrm{Code}})$ and $(X, \hat{Y}_{\mathrm{translated}})$ are the same. Thus,
\[
\mathbb{E}[\ell(Y_{\mathrm{Code}}, X)] = \mathbb{E}[\ell(\hat{Y}_{\mathrm{translated}}, X)].
\]

This is because conditional on $X = x$, sampling $Z_{code} \sim p_{code}(\cdot \mid x)$, then $\hat Z_{nl} \sim T(\cdot \mid Z_{code})$, then $Y \sim \delta_{NL} (\cdot \mid x, \hat Z_{NL})$ induces the same conditional distribution over Y as $Y \sim \delta_{code} (\cdot \mid x, Z_{code})$.

\textbf{Step 2: Substitute translated NL and original NL via TV lemma.}

\begin{lemma}[TV Lemma]
Let $X \sim p(x)$. Let $Z \mid X = x \sim P_x$ and $Z' \mid X = x \sim Q_x$. Let $g(x, z) \in [0, 1]$ be measurable. Then
\[
\mathbb{E}[g(X, Z)] - \mathbb{E}[g(X, Z')] \leq \mathbb{E}_X \bigl[ d_{\mathrm{TV}}(P_X, Q_X) \bigr].
\]
\end{lemma}

Suppose we have $Y_{\mathrm{NL}} \sim \delta_{\mathrm{NL}}(\cdot \mid Z_{\mathrm{NL}})$ under the actual channel $p_{\mathrm{NL}}(\cdot \mid x)$. For each $x$ and trace $z$, define $g(x, z) := \mathbb{E}_{y \sim \delta(\cdot \mid z)}[\ell(y, x)]$.

Then $g(x, z) \in [0, 1]$. Note that
\begin{align*}
\mathbb{E}[\ell(Y_{\mathrm{NL}}, X)] &= \mathbb{E}[g(X, Z_{\mathrm{NL}})], \\
\mathbb{E}[\ell(\hat{Y}_{\mathrm{translated}}, X)] &= \mathbb{E}[g(X, \hat{Z}_{\mathrm{NL}})].
\end{align*}

Applying the TV lemma with $P_x = p_{\mathrm{NL}}(\cdot \mid x)$ and $Q_x = p_{\mathrm{translated}}(\cdot \mid x)$:
\begin{align*}
&\bigl| \mathbb{E}[\ell(Y_{\mathrm{NL}}, X)] - \mathbb{E}[\ell(\hat{Y}_{\mathrm{translated}}, X)] \bigr| \\
&\quad = \bigl| \mathbb{E}[g(X, Z_{\mathrm{NL}})] - \mathbb{E}[g(X, \hat{Z}_{\mathrm{NL}})] \bigr| \\
&\quad \leq \mathbb{E}_X \bigl[ d_{\mathrm{TV}}(p_{\mathrm{NL}}(\cdot \mid X), \, p_{\mathrm{translated}}(\cdot \mid X)) \bigr]
\leq \varepsilon.
\end{align*}

Therefore, rearranging gives
\[
\mathbb{E}[\ell(\hat{Y}_{\mathrm{translated}}, X)] \leq \mathbb{E}[\ell(Y_{\mathrm{NL}}, X)] + \varepsilon.
\]
Thus,
\[
\mathbb{E}[\ell(Y_{\mathrm{Code}}, X)]
= \mathbb{E}[\ell(\hat{Y}_{\mathrm{translated}}, X)]
\leq \mathbb{E}[\ell(Y_{\mathrm{NL}}, X)] + \varepsilon.
\]

Since this holds for arbitrary NL rule $\delta_{\mathrm{NL}}$, taking the infimum over $\delta_{\mathrm{NL}}$ on the right-hand side yields $R^*(Z_{\mathrm{Code}}) \leq R^*(Z_{\mathrm{NL}}) + \varepsilon$.
\end{proof}

\subsection{Arm~2 $<$ Arm~3}

We model the comparison between LLM simulation (Arm~2) and solver execution (Arm~3) under the following assumptions:
\begin{enumerate}[leftmargin=*]
    \item \textbf{Solver correctness.} Given correct code $Z$, the solver deterministically produces the ground-truth answer: $Y_3 = g(X, Z) = Y^*(X)$.
    \item \textbf{Noisy simulation.} LLM simulation adds stochastic noise: $Y_2 \sim N(\cdot \mid Y_3, X, Z)$ for some noise kernel $N$.
    \item \textbf{0-1 loss.} We evaluate under $\ell(y, x) = \mathbf{1}\{y \neq Y^*(x)\}$.
\end{enumerate}

Under these assumptions, the solver achieves zero risk ($R_3 = 0$) whenever the generated code is correct. In contrast, LLM simulation incurs positive risk ($R_2 > 0$) whenever $\Pr[Y_2 \neq Y_3] > 0$, which occurs empirically due to execution errors in mental simulation. Therefore, $R_3 < R_2$.

The only scenario where Arm~2 could outperform Arm~3 is when the generated code is \emph{incorrect}, yet the LLM ``recovers'' by reasoning to the correct answer despite the flawed code. We empirically quantify this recovery rate below. 

\textbf{Recovery reduces as tasks get harder.}
To further reinforce this result, we rule out the possibilities of recovery as tasks get harder, eliminating any benefit of running Arm~2:
\begin{enumerate}[leftmargin=*]
    \item Arm~3 produces an incorrect answer (implying incorrect code generation), and
    \item Arm~2 produces the correct answer (implying successful LLM recovery).
\end{enumerate}

\cref{fig:recovery_final} presents the recovery analysis across all tasks and models. The recovery rate remains consistently low (typically $< 5\%$), indicating that LLM simulation rarely compensates for code generation errors. This confirms that Arm~3's advantage stems from reliable solver execution rather than Arm~2's inability to reason about code.

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{images/recovery_final.png}
    \caption{Recovery rate analysis: proportion of cases where LLM simulation (Arm~2) produces the correct answer despite incorrect code (Arm~3 failure). Recovery rates remain below 5\% across all task families and models, confirming that solver execution dominates LLM simulation.}
    \label{fig:recovery_final}
\end{figure}


