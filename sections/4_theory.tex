\section{Statistical and Information Theoretic Foundations of Algorithmic Reasoning}
\label{sec:theory}
% Following \cite{blackwell1953equivalent}, we prove that $Arm 1 \simeq Arm 2 < Arm 3$ leveraging a similar breakdown framework as our experiments. We utilize information theory and statistical decision theory to sidestep representing the ambiguity of natural language and differences between NL and Code in a mathematical framework. \yu{Using the intuitions we gain in our hypothesis experiments in section~\ref{sec:exp_similarity}}, we prove that Arm 2 is at least as good as Arm 1, and that Arm 3 is always better than Arm 2. 

Following \citet{blackwell1953equivalent}, we formalize the comparison between our arms by separating (i) the \emph{information structure} available to the agent from (ii) the \emph{decision rule} used to map that information to an answer. Concretely, each arm induces an \emph{experiment} (a channel from task instances to traces) together with a restricted class of \emph{executors} (decision rules) that operate on the trace. This separation clarifies which comparisons are information-theoretic (Blackwell-style) versus computational (executor-class–dependent).

\subsection{Preliminaries}
\paragraph{State, experiment, and decision rule.}
Let $X \sim p(x)$ denote a task instance (problem statement plus inputs) drawn from a test distribution $p$. Each instance has a ground-truth output $Y^*(X) \in \mathcal{Y}$. We evaluate 0--1 loss
\[
\ell(y, x) := \mathbf{1}\{y \neq Y^*(x)\} \in \{0,1\}.
\]
An \emph{experiment} (information structure) is a conditional distribution over traces $Z \in \mathcal{Z}$ given the instance,
\[
E : x \mapsto p_E(z \mid x).
\]
A (possibly randomized) \emph{decision rule} or \emph{executor} is a conditional distribution over outputs given the instance and trace,
\[
\rho : (x,z) \mapsto \rho(y \mid x, z).
\]
Given $(E,\rho)$, the induced output distribution is
\[
P_{\rho,E}(Y=y \mid X=x) \;=\; \int \rho(y \mid x, z)\, p_E(z \mid x)\, dz.
\]

\paragraph{Arm-specific experiments and executors.}
Arm~1 (NL) corresponds to the experiment $E_{\mathrm{NL}}$ with trace $Z_{\mathrm{NL}}$ generated by $p_{\mathrm{NL}}(z_{\mathrm{NL}} \mid x)$ and executed by an NL executor $\rho_{\mathrm{NL}}(y \mid x, z_{\mathrm{NL}})$.
Arm~2 (Code) corresponds to the experiment $E_{\mathrm{Code}}$ with trace $Z_{\mathrm{C}}$ generated by $p_{\mathrm{Code}}(z_{\mathrm{C}} \mid x)$ and executed by a simulator executor $\rho_{\mathrm{Sim}}(y \mid x, z_{\mathrm{C}})$.

Our empirical results in Section~\ref{sec:exp_similarity} suggest that NL traces can be approximately obtained by post-processing (garbling) code traces. In Blackwell’s framework, a \emph{garbling} is a state-independent Markov kernel applied to the signal. Concretely, we posit that there exists a (possibly stochastic) translator
\[
T : \mathcal{Z}_{\mathrm{C}} \to \Delta(\mathcal{Z}_{\mathrm{NL}}), 
\qquad z_{\mathrm{NL}} \sim T(\cdot \mid z_{\mathrm{C}}),
\]
such that the \emph{translated} NL experiment induced by $E_{\mathrm{Code}}$ satisfies
\[
p_{\mathrm{translated}}(z_{\mathrm{NL}} \mid x)
:= \int T(z_{\mathrm{NL}} \mid z_{\mathrm{C}})\, p_{\mathrm{Code}}(z_{\mathrm{C}} \mid x)\, dz_{\mathrm{C}}.
\]
Under Assumptions~\ref{assump1}--\ref{assump2}, this translated NL experiment is close (in average conditional TV distance) to the native NL experiment $E_{\mathrm{NL}}$.

\paragraph{Composing translation with execution.}
Given a translator $T(z_{\mathrm{NL}} \mid z_{\mathrm{C}})$ and any NL executor $\rho_{\mathrm{NL}}(y \mid x, z_{\mathrm{NL}})$, define the induced code-side executor that (i) samples a translated NL trace and (ii) applies the same NL executor:
\[
\rho_{\mathrm{Sim}}(y \mid x, z_{\mathrm{C}})
:= \int \rho_{\mathrm{NL}}(y \mid x, z_{\mathrm{NL}})\, T(z_{\mathrm{NL}} \mid z_{\mathrm{C}})\, dz_{\mathrm{NL}}.
\]
This definition is purely a change of decision rule: the underlying experiment remains $E_{\mathrm{Code}}$.


% \begin{align*}
%     & \mathrm{P}(Y \mid X = x) := \int T(z \mid z_{\mathrm{Code}}) \, p_{\mathrm{Code}}(z_{\mathrm{Code}} \mid x) \, \mathrm{d}z_{\mathrm{Code}}
% \end{align*}
% In Arm~1 and Arm~2, each arm corresponds to an intermediate representation $Z$ (e.g. CoT) produced by channel $p(z \mid x)$ (e.g. LLM), and then choosing an output $Y$ via a randomized decision rule $\delta(y \mid x, z)$ (i.e. LLM test-time reasoning). 
\paragraph{Blackwell idealization and Le Cam approximation.}
In Blackwell’s framework, an experiment $E_2$ is (weakly) more informative than $E_1$ if there exists a state-independent Markov kernel $T$ such that $E_1$ can be obtained by garbling the signal from $E_2$. This \emph{exact} simulability implies that for every bounded-loss decision problem, the Bayes risk achievable under $E_2$ is no worse than under $E_1$ \citep{blackwell1953equivalent}.

In our setting, we do not assume exact equality of trace distributions. Instead, we assume that the native NL experiment is \emph{approximately simulable} from the code experiment via a state-independent kernel, and we quantify this approximation using total variation. This corresponds to a Le Cam–style comparison of experiments: approximate simulation implies an additive $\varepsilon$ bound on achievable risk for bounded losses \citep{lecam1986asymptotic}.

Finally, because our executors are computationally constrained (LLM-style inference), we evaluate these bounds over a restricted executor class $\mathcal{H}$ rather than over all measurable decision rules.
For an experiment $E$ with trace $Z \sim p_E(\cdot \mid X)$, define
\[
R^*_{\mathcal{H}}(E) \;:=\; \inf_{\rho \in \mathcal{H}} \; \mathbb{E}[ \mathbf{1}\{\hat Y \neq Y^*(X)\} ].
,
\qquad \hat Y \sim \rho(\cdot \mid X, Z).
\]
Here $\mathcal{H}$ denotes a \emph{restricted executor class} (a subset of Markov kernels from $(x,z)$ to $\mathcal{Y}$) capturing the computational mechanism available at inference time. In our setting, $\mathcal{H}$ is the family of executors implementable by the same model family under the fixed prompting and decoding protocol used in our experiments (i.e., LLM-based execution/simulation). We emphasize that $R^*(E)$ is \emph{not} the classical Bayes risk (which would optimize over all measurable decision rules), but a computation-constrained risk appropriate for comparing LLM-style executors.

When $E=E_{\mathrm{NL}}$ we write $R^*(E_{\mathrm{NL}})$, and when $E=E_{\mathrm{Code}}$ we write $R^*(E_{\mathrm{Code}})$. Our goal is to show that
\[
R^*(E_{\mathrm{Code}}) \;\le\; R^*(E_{\mathrm{NL}}) + \varepsilon.
\]


\textbf{Assumption 1 (State-independent garbling kernel).}
\label{assump1}
There exists a state-independent Markov kernel (translator) $T(z_{\mathrm{NL}} \mid z_{\mathrm{C}})$ such that the translated experiment
\[
p_{\mathrm{translated}}(z_{\mathrm{NL}} \mid x)
:= \int T(z_{\mathrm{NL}} \mid z_{\mathrm{C}})\, p_{\mathrm{Code}}(z_{\mathrm{C}} \mid x)\, dz_{\mathrm{C}}
\]
is well-defined for all $x$. This assumption only asserts the existence of a state-independent post-processing map; it does not require any notion of optimality for translation.


\textbf{Assumption 2.}
\label{assump2}
We assume that the original NL reasoning chain of thought is close to the translated NL on average. Let $p_{\mathrm{NL}}$ be the Arm~1 channel and $p_{\mathrm{translated}}(\cdot \mid x)$ be the translated NL channel. Assume an average conditional TV bound:
\[
\mathbb{E}_{X \sim p} \bigl[
d_{\mathrm{TV}}\bigl(p_{\mathrm{NL}}(\cdot \mid X), \, p_{\mathrm{translated}}(\cdot \mid X)\bigr)
\bigr] \leq \varepsilon,
\]
where
\[
d_{\mathrm{TV}}(P, Q) = \sup_{B} |P(B) - Q(B)|.
\]
In other words, averaged over task instances, the NL trace produced by Arm~1 is close in distribution to the NL traces obtained by translating the code trace (Arm~2) using the translator $T$. Section~\ref{sec:exp_similarity} provides empirical evidence for this assumption using distributional similarity proxies between native and translated traces.  We only assume approximate simulation of $E_{\mathrm{NL}}$ from $E_{\mathrm{Code}}$; we do not assume (or attempt to show) a reverse simulation of code traces from NL traces. Accordingly, our comparison is one-sided and does not establish Blackwell equivalence (nor zero deficiency in both directions).

\begin{proof}
We apply Blackwell’s simulation idea at the level of experiments: code traces can be post-processed into translated NL traces via the state-independent kernel $T$. We then relate performance under the native NL experiment to performance under the translated experiment using a total-variation continuity bound.

\textbf{Step 1 (Simulation via composed executor).}
Fix any NL executor $\rho_{\mathrm{NL}}(y \mid x, z_{\mathrm{NL}})$. Define $\rho_{\mathrm{Sim}}$ from $\rho_{\mathrm{NL}}$ and $T$ as
\[
\rho_{\mathrm{Sim}}(y \mid x, z_{\mathrm{C}})
:= \int \rho_{\mathrm{NL}}(y \mid x, z_{\mathrm{NL}})\, T(z_{\mathrm{NL}} \mid z_{\mathrm{C}})\, dz_{\mathrm{NL}}.
\]
Then the induced conditional output distribution under $(E_{\mathrm{Code}}, \rho_{\mathrm{Sim}})$ equals the conditional output distribution obtained by first sampling $Z_{\mathrm{C}} \sim p_{\mathrm{Code}}(\cdot \mid x)$, then sampling $Z_{\mathrm{NL}} \sim T(\cdot \mid Z_{\mathrm{C}})$, and finally applying $\rho_{\mathrm{NL}}$:
\[
P_{\rho_{\mathrm{Sim}}, E_{\mathrm{Code}}}(Y=y \mid X=x)
= \int \rho_{\mathrm{NL}}(y \mid x, z_{\mathrm{NL}})\, p_{\mathrm{translated}}(z_{\mathrm{NL}} \mid x)\, dz_{\mathrm{NL}}.
\]
In particular, for any bounded loss, the expected loss under $(E_{\mathrm{Code}},\rho_{\mathrm{Sim}})$ equals the expected loss under the translated experiment paired with $\rho_{\mathrm{NL}}$.

Thus,
\[
\mathbb{E}[\ell(Y_{\mathrm{Sim}}, X)] = \mathbb{E}[\ell(\hat{Y}_{\mathrm{translated}}, X)].
\]
\textbf{Step 2: Substitute translated NL and original NL via TV lemma.} Here we show that if two signals look similar, they perform similarly. Even if translated NL is not exactly native NL, bounded loss decision problems cannot exploit small distributional differences, a property of the continuity property of Bayes Risk. 

\begin{lemma}[Total variation risk continuity]
\label{lem:tv_continuity}
Let $X \sim p(x)$. Let $Z \mid X=x \sim P_x$ and $Z' \mid X=x \sim Q_x$. For any measurable $g:\mathcal{X}\times\mathcal{Z}\to[0,1]$,
\[
\Bigl|\mathbb{E}[g(X,Z)] - \mathbb{E}[g(X,Z')]\Bigr|
\;\le\;
\mathbb{E}_X\!\left[d_{\mathrm{TV}}(P_X,Q_X)\right].
\]
\end{lemma}
We use the convention $d_{\mathrm{TV}}(P,Q):=\sup_{B}|P(B)-Q(B)|$; under this convention, the bound above follows from the dual characterization of total variation (e.g., \citet[Ch. 2, Definition 2.4]{Tsybakov2009}).


For each $x$ and trace $z$, define $g(x, z) := \mathbb{E}_{y|x,z}[\ell(y, x)]$. Then $g(x, z) \in [0, 1]$. Note that
\begin{align*}
\mathbb{E}_{(Y_{\mathrm{NL}}, X)}[\ell(Y_{\mathrm{NL}}, X)] &= \mathbb{E}_{(X, Z_{\mathrm{NL}})}[g(X, Z_{\mathrm{NL}})], \\
\mathbb{E}_{(\hat Y_{\mathrm{Tr}}, X)}[\ell(\hat{Y}_{\mathrm{translated}}, X)] &= \mathbb{E}_{(X, \hat{Z}_{\mathrm{NL}})}[g(X, \hat{Z}_{\mathrm{NL}})].
\end{align*}

Applying the TV lemma with $P_x = p_{\mathrm{NL}}(\cdot \mid x)$ and $Q_x = p_{\mathrm{translated}}(\cdot \mid x)$:
\begin{align*}
&\bigl| \mathbb{E}[\ell(Y_{\mathrm{NL}}, X)] - \mathbb{E}[\ell(\hat{Y}_{\mathrm{translated}}, X)] \bigr| \\
&\quad = \bigl| \mathbb{E}[g(X, Z_{\mathrm{NL}})] - \mathbb{E}[g(X, \hat{Z}_{\mathrm{NL}})] \bigr| \\
&\quad \leq \mathbb{E}_X \bigl[ d_{\mathrm{TV}}(p_{\mathrm{NL}}(\cdot \mid X), \, p_{\mathrm{translated}}(\cdot \mid X)) \bigr]
\leq \varepsilon.
\end{align*}

Therefore, rearranging gives
\begin{align*}
&\mathbb{E}[\ell(\hat{Y}_{\mathrm{translated}}, X)] \leq \mathbb{E}[\ell(Y_{\mathrm{NL}}, X)] + \varepsilon \\
&\mathbb{E}[\ell(Y_{\mathrm{Sim}}, X)]
= \mathbb{E}[\ell(\hat{Y}_{\mathrm{translated}}, X)]
\leq \mathbb{E}[\ell(Y_{\mathrm{NL}}, X)] + \varepsilon.
\end{align*}
Since the construction above holds for an \emph{arbitrary} NL executor $\rho_{\mathrm{NL}} \in \mathcal{H}$, it also holds in particular for an NL executor that (approximately) minimizes risk under $E_{\mathrm{NL}}$. Let $\rho_{\mathrm{NL}}^* \in \arg\min_{\rho \in \mathcal{H}} \mathbb{E}[\ell(\hat Y, X)]$ under $E_{\mathrm{NL}}$, and let $\rho_{\mathrm{Sim}}^*$ be the induced executor on code traces obtained by composing $\rho_{\mathrm{NL}}^*$ with $T$:
\[
\rho_{\mathrm{Sim}}^*(y \mid x, z_{\mathrm{C}})
:= \int \rho_{\mathrm{NL}}^*(y \mid x, z_{\mathrm{NL}})\, T(z_{\mathrm{NL}} \mid z_{\mathrm{C}})\, dz_{\mathrm{NL}}.
\]
By Step~1 and Step~2,
\[
\mathbb{E}\bigl[\ell(\hat Y_{\mathrm{Sim}}^{(\rho_{\mathrm{Sim}}^*)}, X)\bigr]
\le
\mathbb{E}\bigl[\ell(\hat Y_{\mathrm{NL}}^{(\rho_{\mathrm{NL}}^*)}, X)\bigr] + \varepsilon.
\]
Finally, since $R^*(E_{\mathrm{Code}})$ is the infimum over \emph{all} executors in $\mathcal{H}$ applied to $E_{\mathrm{Code}}$, we have
\[
R^*(E_{\mathrm{Code}})
\le
\mathbb{E}\bigl[\ell(\hat Y_{\mathrm{Sim}}^{(\rho_{\mathrm{Sim}}^*)}, X)\bigr]
\le
R^*(E_{\mathrm{NL}}) + \varepsilon.
\]
\begin{framed}
\qquad \qquad \quad $R^*(E_{\mathrm{Code}}) \le R^*(E_{\mathrm{NL}}) + \varepsilon$
\end{framed}

\end{proof}
This proof follows the standard Le Cam/Blackwell logic: simulation of one experiment from another (exactly in Blackwell, approximately in Le Cam) yields risk bounds for bounded losses.

% \vspace{-20pt}
\subsection{Arm~2 $<$ Arm~3}
\textbf{Setup.}
Both Arm~2 and Arm~3 share the same underlying experiment $E_{\mathrm{Code}}$ that generates code traces $Z_{\mathrm{C}} \sim p_{\mathrm{Code}}(\cdot \mid X)$. They differ only in the executor applied to the same trace. Let $\rho_{\mathrm{Sim}} \in \mathcal{H}$ denote the LLM simulation executor, and let $\rho_{\mathrm{Exec}}$ denote deterministic execution by a Python interpreter $g$:
\[
\rho_{\mathrm{Exec}}(y \mid x, z_{\mathrm{C}}) := \mathbf{1}\{y = g(x, z_{\mathrm{C}})\}.
\]
We compare the expected 0--1 loss of these two executors under the shared experiment $E_{\mathrm{Code}}$.
\begin{proof}
Define the event
\[
C := \{ g(X, Z_{\mathrm{C}}) = Y^*(X) \},
\]
corresponding to semantically correct code.
Under $\ell(y,x)=\mathbf{1}\{y\neq Y^*(x)\}$, on the event $C$ the deterministic executor is correct with probability one:
\begin{align*}
& \mathbb{E}\!\left[\ell(\hat Y^{(\rho_{\mathrm{Exec}})},X)\mid C\right]=0 \\
& \mathbb{E}\!\left[\ell(\hat Y^{(\rho_{\mathrm{Sim}})},X)\mid C\right]\ge 0,
\end{align*}

Therefore, conditional on semantically correct code, deterministic execution weakly dominates LLM simulation in terms of conditional expected 0--1 loss under the shared experiment $E_{\mathrm{Code}}$.

\begin{framed}
\qquad \qquad \quad
$\mathbb{E}[\ell(\hat Y^{(\rho_{\mathrm{Exec}})},X)\mid C]
\;\le\;
\mathbb{E}[\ell(\hat Y^{(\rho_{\mathrm{Sim}})},X)\mid C]$
\end{framed}

\end{proof}

The only scenario where Arm~2 could outperform Arm~3 is when the generated code is \emph{incorrect}, yet the LLM ``recovers'' by reasoning to the correct answer despite the flawed code. We empirically quantify this recovery rate below. 

\textbf{Recovery reduces as tasks get harder.}
To assess the practical impact of recovery, we empirically measure the frequency of such events as task difficulty increases:
\begin{enumerate}[leftmargin=*]
    \item Arm~3 produces an incorrect answer (implying incorrect code generation), and
    \item Arm~2 produces the correct answer (implying successful LLM recovery).
\end{enumerate}

\cref{fig:recovery_final} presents the recovery analysis across all tasks and models. The recovery rate remains consistently low (typically $< 5\%$), indicating that LLM simulation rarely compensates for code generation errors. This confirms that Arm~3's advantage stems from reliable solver execution rather than Arm~2's inability to reason about code.

\begin{figure}[t]
    \centering
    \vspace{-5pt}
    \includegraphics[width=0.85\linewidth]{images/recovery_final.png}
    \vspace{-5pt}
    \caption{\textbf{Recovery rate.} How often Arm~2 succeeds when Arm~3 fails. Recovery rates are $<$5\% across all tasks, confirming Arm~3's advantage comes from reliable execution, not LLM compensation.}
    \label{fig:recovery_final}
\end{figure}


