\section{Theoretical Analysis}
\label{sec:theory}

We adopt the three-arms framework introduced in
Section~\ref{sec:framework}. Each arm is represented as a pair
$(E,\rho)$ consisting of a trace generator $E$ and an executor $\rho$,
evaluated via the risk $R(E,\rho)$ and the computation-constrained
optimal risk $R^*_{\mathcal H}(E)$.
Our theoretical contribution is to relate the three arms by
(i) formalizing when natural-language (NL) traces are simulable from
code traces under executor-aligned notions of approximation, and
(ii) decomposing the performance gap between simulation and execution
when both arms share the same code trace.

Unlike classical Blackwell comparisons, the executor observes the full
task instance $X$ in all arms (Section~\ref{sec:framework}); traces
provide auxiliary side information.
Accordingly, all comparisons below are conditional on $X$ and should
be interpreted as statements about the relative usefulness of auxiliary
traces and execution mechanisms under shared observation of $X$.

\subsection{Arm~1 vs.\ Arm~2: One-Sided Simulation from Code to NL}

\paragraph{Conditional garbling with observed side information.}
The empirical results in Section~\ref{sec:rep_analysis} suggest that
natural-language reasoning traces can be approximately obtained by
post-processing code traces.
In classical Blackwell theory, such post-processing is modeled as a
state-independent garbling kernel.
Because the executor already observes $X$, we allow the translator to
depend on $x$ without changing the information available at decision
time; this captures the practical fact that translating code into an NL
reasoning trace often uses problem context.

\textbf{Assumption 1 (Translator / conditional garbling kernel).}
\label{assump1}
There exists a (possibly stochastic) Markov kernel
\[
T:\ (x,z_{\mathrm{C}})\mapsto T(\cdot\mid x,z_{\mathrm{C}})
\in \Delta(\mathcal{Z}_{\mathrm{NL}})
\]
such that the translated NL trace distribution induced by
$E_{\mathrm{Code}}$ is
\[
p_{\mathrm{tr}}(z_{\mathrm{NL}}\mid x)
:= \int T(z_{\mathrm{NL}}\mid x,z_{\mathrm{C}})
\, p_{\mathrm{Code}}(z_{\mathrm{C}}\mid x)\, dz_{\mathrm{C}}.
\]
We denote by $E_{\mathrm{tr}}$ the corresponding translated trace
generator.
This is a one-sided simulation assumption: we do not assume that code
traces are simulable from NL traces, nor do we claim Blackwell
equivalence.

\paragraph{Executor-aligned discrepancy.}
Total variation distance on raw text distributions is often overly
stringent and difficult to estimate reliably for long sequences.
Instead, we require that native and translated NL traces be
approximately indistinguishable \emph{to executors in}
$\mathcal{H}_{\mathrm{NL}}$ under bounded loss.

\textbf{Assumption 2 (Executor-aligned discrepancy bound).}
\label{assump2}
Define the worst-case risk discrepancy
\[
\Delta_{\mathcal{H}_{\mathrm{NL}}}(E_{\mathrm{NL}},E_{\mathrm{tr}})
:= \sup_{\rho\in\mathcal{H}_{\mathrm{NL}}}
\Bigl|
R(E_{\mathrm{NL}},\rho)
-
R(E_{\mathrm{tr}},\rho)
\Bigr|.
\]
Assume
\[
\Delta_{\mathcal{H}_{\mathrm{NL}}}(E_{\mathrm{NL}},E_{\mathrm{tr}})
\le \varepsilon .
\]
Section~\ref{sec:rep_analysis} provides empirical evidence for a small
executor-aligned discrepancy.
Importantly, $\varepsilon$ should be interpreted as an
executor-aligned approximation error, not as a certified total-variation
bound on raw traces.

\paragraph{Closure under translation.}
To compare optimal risks across trace types, we require that code-side
executors can implement ``translate then apply an NL executor'' under
comparable computational constraints.

\textbf{Assumption 3 (Executor closure under translation).}
\label{assump3}
For every $\rho_{\mathrm{NL}}\in\mathcal{H}_{\mathrm{NL}}$, the composed
executor
\[
\rho_{T\circ \mathrm{NL}}(y\mid x,z_{\mathrm{C}})
:=
\int \rho_{\mathrm{NL}}(y\mid x,z_{\mathrm{NL}})
\, T(z_{\mathrm{NL}}\mid x,z_{\mathrm{C}})
\, dz_{\mathrm{NL}}
\]
belongs to $\mathcal{H}_{\mathrm{C}}$.

\paragraph{Main comparison.}
Under these assumptions, code traces are sufficient (up to $\varepsilon$)
to reproduce the performance of the best NL-trace executor.

\begin{theorem}[One-sided simulation bound]
\label{thm:code_ge_nl}
Under Assumptions~\ref{assump1}--\ref{assump3},
\[
R^*_{\mathcal{H}_{\mathrm{C}}}(E_{\mathrm{Code}})
\;\le\;
R^*_{\mathcal{H}_{\mathrm{NL}}}(E_{\mathrm{NL}})
+
\varepsilon .
\]
\end{theorem}
\begin{proof}
Fix any $\delta>0$ and choose $\rho^{\delta}_{\mathrm{NL}}\in\mathcal{H}_{\mathrm{NL}}$ such that
\[
R(E_{\mathrm{NL}},\rho^{\delta}_{\mathrm{NL}})
\le
R^*_{\mathcal{H}_{\mathrm{NL}}}(E_{\mathrm{NL}}) + \delta,
\]
which is possible by the definition of the infimum.

Define the composed code-side executor
\[
\rho^{\delta}_{T\circ \mathrm{NL}}(y\mid x,z_{\mathrm{C}})
:=
\int \rho^{\delta}_{\mathrm{NL}}(y\mid x,z_{\mathrm{NL}})
\, T(z_{\mathrm{NL}}\mid x,z_{\mathrm{C}})
\, dz_{\mathrm{NL}} .
\]
By Assumption~\ref{assump3}, we have $\rho^{\delta}_{T\circ \mathrm{NL}}\in\mathcal{H}_{\mathrm{C}}$.

Now consider the pipeline that samples $Z_{\mathrm{C}}\sim p_{\mathrm{Code}}(\cdot\mid X)$,
then samples $Z_{\mathrm{NL}}\sim T(\cdot\mid X,Z_{\mathrm{C}})$, and finally outputs
$\hat Y\sim \rho^{\delta}_{\mathrm{NL}}(\cdot\mid X,Z_{\mathrm{NL}})$.
By construction of $E_{\mathrm{tr}}$ (Assumption~\ref{assump1}), the marginal conditional law of
$Z_{\mathrm{NL}}$ given $X$ is exactly $p_{\mathrm{tr}}(\cdot\mid X)$; hence this procedure induces the same joint law over $(X,Z_{\mathrm{NL}},\hat Y)$ as sampling
$Z_{\mathrm{NL}}\sim p_{\mathrm{tr}}(\cdot\mid X)$ and applying $\rho^{\delta}_{\mathrm{NL}}$.
Equivalently,
\[
R(E_{\mathrm{Code}},\rho^{\delta}_{T\circ \mathrm{NL}})
=
R(E_{\mathrm{tr}},\rho^{\delta}_{\mathrm{NL}}).
\]

By Assumption~\ref{assump2},
\[
R(E_{\mathrm{tr}},\rho^{\delta}_{\mathrm{NL}})
\le
R(E_{\mathrm{NL}},\rho^{\delta}_{\mathrm{NL}}) + \varepsilon
\le
R^*_{\mathcal{H}_{\mathrm{NL}}}(E_{\mathrm{NL}}) + \delta + \varepsilon.
\]
Finally, since $R^*_{\mathcal{H}_{\mathrm{C}}}(E_{\mathrm{Code}})$ is the infimum over all executors in $\mathcal{H}_{\mathrm{C}}$,
\[
R^*_{\mathcal{H}_{\mathrm{C}}}(E_{\mathrm{Code}})
\le
R(E_{\mathrm{Code}},\rho^{\delta}_{T\circ \mathrm{NL}})
\le
R^*_{\mathcal{H}_{\mathrm{NL}}}(E_{\mathrm{NL}}) + \delta + \varepsilon.
\]
Letting $\delta\to 0$ completes the proof.
\end{proof}

\paragraph{Interpretation and scope.}
Theorem~\ref{thm:code_ge_nl} is existential: it guarantees the existence
of a code-side executor matching the best NL-side executor up to
$\varepsilon$.
It does not assert that any fixed Arm~2 simulation prompt is optimal;
the gap between existence and the specific Arm~2 instantiation is
addressed empirically in Section~\ref{sec:performance}.

\subsection{Arm~2 vs.\ Arm~3: Simulation vs.\ Execution on the Same Code Trace}

As defined in Section~\ref{sec:framework}, Arm~2 and Arm~3 share the same
trace generator $E_{\mathrm{Code}}$ and differ only in the executor.

Let $\rho_{\mathrm{Sim}}\in\mathcal{H}_{\mathrm{C}}$ denote the fixed
LLM-based simulation executor used in Arm~2.
Let $g$ be a deterministic interpreter mapping $(x,z_{\mathrm{C}})$ to
$\mathcal{Y}\cup\{\bot\}$, where $\bot$ denotes execution failure.
Arm~3 uses
\[
\rho_{\mathrm{Exec}}(y\mid x,z_{\mathrm{C}})
:= \mathbf{1}\{y=g(x,z_{\mathrm{C}})\}.
\]

Define the instance-correct execution event
\[
C := \{ g(X,Z_{\mathrm{C}}) = Y^*(X) \}.
\]

\paragraph{Risk decomposition.}
Let
\[
e_C := \Pr(\hat Y_{\mathrm{Sim}}\neq Y^*(X)\mid C),
\qquad
r := \Pr(\hat Y_{\mathrm{Sim}}=Y^*(X)\mid \neg C).
\]
Then
\begin{align*}
R(E_{\mathrm{Code}},\rho_{\mathrm{Exec}}) &= \Pr(\neg C),\\
R(E_{\mathrm{Code}},\rho_{\mathrm{Sim}}) &= \Pr(C)\,e_C + \Pr(\neg C)(1-r),
\end{align*}
and hence
\[
R(E_{\mathrm{Code}},\rho_{\mathrm{Sim}})
-
R(E_{\mathrm{Code}},\rho_{\mathrm{Exec}})
=
\Pr(C)\,e_C
-
\Pr(\neg C)\,r .
\]

\paragraph{Implications.}
Simulation noise on correct-execution instances ($\Pr(C)e_C$) harms
Arm~2, while recovery on incorrect or failed executions
($\Pr(\neg C)r$) helps Arm~2.
Arm~3 dominates whenever the recovery mass is insufficient to offset
simulation noise, a condition quantified empirically in
Section~\ref{sec:performance}.

\paragraph{Empirical recovery as an upper bound.}
A direct way to operationalize the ``recovery'' term in the decomposition is to measure the frequency with which Arm~2 succeeds while Arm~3 fails on the \emph{same} generated code trace, i.e.,
\[
\Pr\!\left(
\hat Y_{\mathrm{Sim}} = Y^*(X),
\;
g(X,Z_{\mathrm{C}})\neq Y^*(X)
\right).
\]
This quantity corresponds to the \emph{recovery mass} $\Pr(\neg C)\,r$ appearing in the risk gap
\[
R(E_{\mathrm{Code}},\rho_{\mathrm{Sim}})
-
R(E_{\mathrm{Code}},\rho_{\mathrm{Exec}})
=
\Pr(C)\,e_C
-
\Pr(\neg C)\,r.
\]
It aggregates both (i) genuine recovery from incorrect code and (ii) cases where execution fails (e.g., $g(X,Z_{\mathrm{C}})=\bot$) but the simulator still answers correctly.
Because the simulator may partially ignore $Z_{\mathrm{C}}$ and answer directly from $X$, this statistic should be interpreted as an \emph{upper bound} on mechanistic ``recovery from flawed code.''

\paragraph{Interpretation.}
Arm~2 can outperform Arm~3 only if the recovery mass $\Pr(\neg C)\,r$ is large enough to offset simulation noise on correct-execution instances $\Pr(C)\,e_C$.
Empirically, we find that recovery mass is consistently small across tasks and models, indicating that Arm~2 rarely compensates for execution failures via recovery.
Combined with the fact that $e_C$ is nonzero in practice (LLM simulation is not perfectly faithful even when execution would be correct), this explains why deterministic execution typically achieves lower end-to-end error than simulation on the same generated code traces.

\begin{figure}[t]
    \centering
    \vspace{-5pt}
    \includegraphics[width=0.85\linewidth]{images/recovery_final.png}
    \vspace{-5pt}
    \caption{\textbf{Recovery mass.} Frequency with which Arm~2 is correct while Arm~3 is incorrect on the \emph{same generated code trace}. This estimates $\Pr(\neg C)\,r$ in the decomposition above (and is an upper bound on ``recovery from flawed code'' due to possible solve-from-$X$ behavior). Recovery remains $<5\%$ across tasks and models.}
    \label{fig:recovery_final}
\end{figure}

\subsection{Relating Theory and Empirical Observations}