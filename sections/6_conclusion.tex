\section{Conclusion}
\label{sec:conclusion}
We introduced a three-route Bayesian framework (\cref{sec:framework}) for comparing code and natural language representations in algorithmic reasoning. By introducing an intermediate route---code generation with LLM-based simulation---we isolate translation effects from execution effects, enabling tractable pairwise analysis. Empirically, code execution achieves 4.01$\times$ higher odds of correctness than NL reasoning ($p<0.001$) across 44 algorithmic tasks and 6 models (\cref{sec:performance}). Our causal intervention experiments (\cref{sec:rep_analysis}) demonstrate that NL reasoning traces are distributionally and functionally equivalent to code-translated traces, supporting the hypothesis that NL reasoning implicitly simulates underlying algorithmic computations for specific models,tasks and prompts. Theoretically, we prove that code-based reasoning achieves lower Bayes risk via Blackwell dominance (\cref{prop:dominance}), providing an information-theoretic explanation for the empirical advantages of solver-based pipelines.

\paragraph{Limitations}

\paragraph{Future Work}

\section{Impact Statement}
This paper presents work whose goal is to advance the field of machine learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here.


% \textbf{Limitations.} (1)~\emph{Task scope}: Our evaluation focuses on algorithmic tasks (arithmetic, DP, ILP) with well-defined ground truth; results may not generalize to open-ended reasoning or tasks without clear algorithmic structure. (2)~\emph{Empirical approximations}: Our distributional similarity tests use finite samples and specific judge models, introducing estimation variance. (3)~\emph{Model coverage}: While we evaluate both frontier (GPT-4o, Claude, Gemini) and open-source models (Mistral, LLaMA), we cannot guarantee findings generalize to all architectures or future models. (4)~\emph{Theoretical assumptions}: The Blackwell dominance result assumes the existence of a garbling channel from code to NL, which we validate empirically but do not prove from first principles.

