\section{Conclusion}
We introduced a three-arm Bayesian framework for comparing code and natural language representations in algorithmic reasoning. By introducing an intermediate arm---code generation with LLM-based simulation---we isolate translation effects from execution effects, enabling tractable pairwise analysis. Empirically, code execution achieves 4.01$\times$ higher odds of correctness than NL reasoning ($p<0.001$) across 44 algorithmic tasks and 6 models. Our causal intervention experiments demonstrate that NL reasoning traces are distributionally and functionally equivalent to code-translated traces, supporting the hypothesis that NL reasoning implicitly simulates underlying algorithmic computations. Theoretically, we prove that code-based reasoning achieves lower Bayes risk via Blackwell dominance, providing an information-theoretic explanation for the empirical advantages of solver-based pipelines.

\textbf{Limitations.} (1)~\emph{Task scope}: Our evaluation focuses on algorithmic tasks (arithmetic, DP, ILP) with well-defined ground truth; results may not generalize to open-ended reasoning or tasks without clear algorithmic structure. (2)~\emph{Empirical approximations}: Our distributional similarity tests use finite samples and specific judge models, introducing estimation variance. (3)~\emph{Model coverage}: While we evaluate both frontier (GPT-4o, Claude, Gemini) and open-source models (Mistral, LLaMA), we cannot guarantee findings generalize to all architectures or future models. (4)~\emph{Theoretical assumptions}: The Blackwell dominance result assumes the existence of a garbling channel from code to NL, which we validate empirically but do not prove from first principles.

