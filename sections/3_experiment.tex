\section{Empirical Evaluation}
\label{sec:empirical}

This section instantiates the three-arms framework introduced in
Section~\ref{sec:framework} and evaluates the central ordering
\[
\text{Arm~1} \simeq \text{Arm~2} < \text{Arm~3}.
\]
We proceed in two stages. First, we compare end-to-end task accuracy
across the three arms under a controlled experimental design.
Second, we analyze whether natural-language reasoning traces contain
decision-relevant information beyond what is present in code
representations, providing empirical motivation for the assumptions
used in Section~\ref{sec:theory}.

\subsection{Experimental Instantiation of the Three Arms}
\label{sec:instantiation}

\paragraph{Arm~1 vs.\ Arm~2.}
Given a test input $X_i$ corresponding to instance $i$, Arm~1 prompts
the language model to reason using natural language only, producing
an output
$
(X_i) \;\mapsto\; Y_i^{(\mathrm{NL})}.
$
Arm~2 uses the same language model to first generate code
$(X_i) \;\mapsto\; C_i,$
and then reason about the code and prompt in natural language,
$(X_i, C_i) \;\mapsto\; Y_i^{(\mathrm{Sim})}.$
Prompt templates are controlled to be as similar as possible, with
Arm~2 differing only by the inclusion of a code-generation and
simulation instruction (Figure~\ref{fig:arm_prompts}).

\paragraph{Arm~2 vs.\ Arm~3.}
Let $X_i$ denote the original problem instance and $C_i$ the generated
code for instance $i$. The tuple $(X_i, C_i)$ is held fixed across both
arms. Arm~2 produces
$(X_i, C_i) \;\mapsto\; Y_i^{(\mathrm{Sim})}$
via language-model-based simulation, while Arm~3 executes the same code
using an external Python runtime,
$(X_i, C_i) \;\mapsto\; Y_i^{(\mathrm{Exec})}.$
For notational symmetry, $X_i$ is included in both mappings, although it
is ignored by the external executor.

\paragraph{Observed outcomes.}
For each problem instance $i$, we observe paired Bernoulli outcomes
\[
\bigl(Y_i^{(\mathrm{NL})},\; Y_i^{(\mathrm{Sim})},\; Y_i^{(\mathrm{Exec})}\bigr).
\]

\paragraph{Data and models.}
We evaluate on the CLRS-30 benchmark ($n=500$), the NP-Hard-Eval
benchmark ($n=540$), and a custom fine-grained evaluation suite
($n=540$), across three random seeds. Tasks span Arithmetic, Dynamic
Programming, and Integer Linear Programming (ILP), with difficulty
controlled by a parameter $\tau$.
We evaluate both frontier models (Haiku~4.5, GPT-4o, Gemini~2.0 Flash)
and open-source models (Mixtral, Codestral). 

% Models with more than
% 50\% JSON parse failures are excluded to avoid confounding instruction
% following with reasoning ability.

\paragraph{Prompting and execution.}
In Arm~1, models are instructed to reason without using code and to
output a structured rationale and answer.
In Arm~2, the same prompt is augmented with instructions to generate
code and simulate its execution.
In Arm~3, the generated function is executed directly in a Python~3
runtime with access to standard scientific libraries.
Structured JSON outputs are enforced for all arms
(Figure~\ref{fig:arm_prompts}).

\subsection{End-to-End Performance Comparison}
\label{sec:performance}

\paragraph{Pairwise statistical tests.}
We evaluate pairwise arm differences using McNemar tests on paired
Bernoulli outcomes.
For Arm~1 vs.\ Arm~2, the null hypothesis is
\begin{align*}
& H_0:\;
\Pr(Y^{(\mathrm{NL})}=1,\,Y^{(\mathrm{Sim})}=0) \\
& =
\Pr(Y^{(\mathrm{NL})}=0,\,Y^{(\mathrm{Sim})}=1),
\end{align*}
with an analogous null for Arm~2 vs.\ Arm~3.

Effect sizes are reported as paired accuracy differences, e.g.,
\[
\Delta_{\mathrm{Sim-NL}}
=
\mathrm{Acc}(\mathrm{Sim}) - \mathrm{Acc}(\mathrm{NL}),
\]
with 95\% confidence intervals estimated via cluster bootstrap
resampling over instances.
Holm--Bonferroni correction is applied to control family-wise error rate
at $\alpha=5\%$. %what family wise? Across difficulties? Add tau analysis later. 

\paragraph{Difficulty scaling.}
To analyze how performance varies with task difficulty, we fit a
generalized linear mixed-effects model (GLMM) with a logistic link:
\[
Y_i
=
\alpha
+
\beta_{\mathrm{arm}_i}
+
\gamma\,\tau_i
+
\delta_{\mathrm{arm}_i}\tau_i
+
\varepsilon_i,
\qquad
\varepsilon_i \sim \mathrm{Bernoulli}.
\]
Arm and difficulty are modeled as fixed effects, with instance and seed
as random effects.

\paragraph{Results.}
Across all benchmarks, we reject the global null hypothesis
($p < 0.001$ after correction).
Post-hoc tests show a statistically significant advantage of Arm~3
over Arm~2, with a positive paired accuracy gap excluding zero in the
95\% confidence interval.
For Arm~1 vs.\ Arm~2, although the null is rejected ($p = 0.04$), the
bootstrap confidence interval overlaps zero, indicating that we cannot
conclude a meaningful difference between natural-language reasoning
and code simulation under this evaluation.
Performance gaps widen with increasing task difficulty: Arm~3 remains
stable while Arm~1 degrades sharply, with code execution achieving
approximately $4.1\times$ higher odds of correctness.

% \subsection{Representation-Level Analysis: Does Language Add Information?}
% \label{sec:rep_analysis}

% The theoretical results in Section~\ref{sec:theory} rely on the premise
% that, for the evaluated tasks, models, and prompts, natural-language
% reasoning traces do not contain decision-relevant information beyond
% what is already present in code representations.
% We empirically evaluate this premise by analyzing both distributional
% and functional similarity between native NL reasoning and NL obtained
% by translating code.
\subsection{Representation-Level Analysis: Does Language Add Information?}
\label{sec:rep_analysis}

The end-to-end results in Section~\ref{sec:performance} are consistent with the hypothesis that, for the evaluated tasks, models, and prompts, natural-language chain-of-thought (NL CoT) does not provide decision-relevant information beyond what is already present in code representations.
Under this hypothesis, delegating execution to an external runtime is well-motivated: NL simulation offers no systematic benefit over code representations, while direct execution avoids simulation noise and can achieve strictly lower error when the generated code is correct.
We test this hypothesis in two steps.
First, we test whether NL traces translated from code are \emph{distributionally similar} to native NL CoT (\S\ref{sec:dist_similarity}).
Second, we test whether translated NL is \emph{functionally similar} to native NL in the sense of preserving decision-relevant information for downstream accuracy (\S\ref{sec:func_similarity}).
These results empirically motivate assumptions used in Section~\ref{sec:theory}.

\begin{figure*}[t]
    \centering
    \vspace{-5pt}
    \includegraphics[width=0.68\textwidth]{images/fig_prompts_combined.png}
    \vspace{-5pt}
    \caption{\textbf{Translation and discrimination prompts.}
    \textbf{(a) Translator}: Converts code to NL reasoning step-by-step, mimicking native reasoning.
    \textbf{(b) Discriminator}: Judge models classify traces as ``Native NL'' or ``Translated''.
    Used in Section~\ref{sec:rep_analysis}.}
    \label{fig:translator_discriminator}
\end{figure*}

\subsubsection{Distributional Similarity between Translated NL and Native NL}
\label{sec:dist_similarity}

We test whether the distribution of reasoning traces produced in Arm~1 can be approximated by post-processing code traces with a fixed transformation.
Concretely, for each task instance $x$ and corresponding code $c$, we form two trace conditions:
\begin{align*}
    & (x, z_{\mathrm{NL}}), \quad z_{\mathrm{NL}} \sim p_{\mathrm{NL}}(\cdot \mid x), \quad \text{(\emph{Native})},\\
    & (x, \hat z_{\mathrm{NL}}), \quad \hat z_{\mathrm{NL}} \sim p_{\mathrm{Tr}}(\cdot \mid c), \quad \text{(\emph{Translated})},
\end{align*}
where $p_{\mathrm{Tr}}(\cdot\mid c)$ is induced by a fixed translator $T$ applied to code, i.e.,
$\hat z_{\mathrm{NL}} = T(c)$ with fixed prompting/in-context examples (Figure~\ref{fig:translator_discriminator}a).

\paragraph{Discrimination task.}
We formulate a binary classification task: a powerful zero-shot judge model is given $(x, z)$ and must predict whether $z$ is \emph{Native} or \emph{Translated} (Figure~\ref{fig:translator_discriminator}b).
This corresponds to distinguishing the joint distributions
\[
p(x)p_{\mathrm{NL}}(z\mid x)
\quad\text{vs.}\quad
p(x)p_{\mathrm{Tr}}(z\mid x).
\]
We use multiple judge models (e.g., Claude Opus 4.0, Gemini 2.5 Pro, Grok 4.1 Fast) to reduce dependence on a single discriminator.

\paragraph{Sampling and controls.}
We sample problem instances $x$ from a pool of previously collected CoT examples spanning different models, seeds, tasks, and difficulties, and construct paired examples with balanced labels.
Tasks used for discrimination are held out from the translator’s in-context examples to avoid trivial leakage.
As a calibration control, we also ask the same judges to discriminate \emph{raw code} vs.\ \emph{native NL} traces; high control accuracy indicates the judges are not underpowered.

\paragraph{Metrics.}
We report judge accuracy with Wilson confidence intervals, both overall and stratified by task/model when informative.

\begin{figure*}[t]
    \centering
    \vspace{-5pt}
    \includegraphics[width=0.68\textwidth]{images/discrimination_by_task.png}
    \vspace{-5pt}
    \caption{\textbf{Discrimination results.}
    Judge models classify traces as Native vs.\ Translated.
    Accuracy near chance indicates translated NL is distributionally similar to native NL.
    The control task (code vs.\ NL) remains high, indicating calibrated judges.}
    \label{fig:discrimination_by_task}
\end{figure*}

\paragraph{Results.}
Across 2000 samples per judge (6000 total across three judges), discrimination accuracy is near chance (49.4\% with 95\% Wilson CI [47.2\%, 51.5\%]; Figure~\ref{fig:discrimination_by_task}).
The control discrimination between raw code and native NL is substantially higher (e.g., $\approx$79\%), indicating the judges can detect obvious representation differences but fail to distinguish translated vs.\ native NL.
This suggests that, under our translation procedure, code-derived NL traces are distributionally similar to native NL CoT for most tasks.
We observe a small number of task-level exceptions (e.g., rod-cutting and Kruskal-style MST) where discrimination may exceed chance, which we treat as potential failure modes of a single fixed translator or as task-specific stylistic artifacts.

\subsubsection{Functional Similarity between Translated NL and Native NL}
\label{sec:func_similarity}

Distributional similarity alone does not guarantee that translated NL preserves \emph{decision-relevant} information.
We therefore test functional similarity using a downstream accuracy intervention.

\paragraph{Intervention setup.}
For a task instance $x$, we prompt a target language model in three conditions:
\begin{align*}
    & \text{(1) Baseline:} && x,\\
    & \text{(2) Native:} && x \,\|\, z_{\mathrm{NL}},\\
    & \text{(3) Translated:} && x \,\|\, \hat z_{\mathrm{NL}},
\end{align*}
where $z_{\mathrm{NL}} \sim p_{\mathrm{NL}}(\cdot\mid x)$ is the native Arm~1 trace and
$\hat z_{\mathrm{NL}}$ is obtained by translating the corresponding code to NL using the same translator procedure as in \S\ref{sec:dist_similarity}.
If translated NL loses decision-relevant information relative to native NL, we would expect
\[
\mathrm{Acc}(x \,\|\, \hat z_{\mathrm{NL}}) < \mathrm{Acc}(x \,\|\, z_{\mathrm{NL}}).
\]

\paragraph{Protocol and models.}
We run this test on 1000 held-out instances across multiple target models (e.g., Claude Opus 4.0, Gemini 2.5 Pro, Grok 4.1 Fast), using tasks disjoint from the translator’s in-context examples.
The translator prompt is fixed (frozen) across all runs.
Crucially, in this experiment the translator model is matched to the original code generator (i.e., we translate code produced by the same model family), to avoid confounding functional losses with cross-model stylistic mismatch.

\paragraph{Results.}
We fail to reject the null hypothesis of equal performance, and observe overlapping 95\% confidence intervals between the Native and Translated conditions (Figure~\ref{fig:translation_additivity}).
This suggests that, under our evaluated settings, translating code into NL does not destroy decision-relevant information for downstream answering, consistent with the claim that NL CoT does not systematically add algorithmic advantage beyond what is in code.

\begin{figure}[t]
    \centering
    \vspace{-5pt}
    \includegraphics[width=0.85\linewidth]{images/translation_additivity_plot.png}
    \vspace{-5pt}
    \caption{\textbf{Functional equivalence.}
    Overlapping 95\% confidence intervals between Native and Translated conditions indicate code-to-NL translation preserves decision-relevant information for end-task accuracy.}
    \label{fig:translation_additivity}
\end{figure}
