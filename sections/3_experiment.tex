



\section{Evaluating Translated NL and NL Distributional Similarity.}

One key hypothesis is that natural language reasoning follows a deeper algorithmic procedure encoded in its representations. If this is the case, it would make sense to surface the code and send it to an executor. We test whether natural language reasoning generated by information contained in the code alone can be distributionally similar to natural language reasoning generated from the prompt alone in Arm~1. Then, we test whether they are functionally similar.

\subsection{Distributional Similarity between Translated NL and original NL}

We show that the distribution of traces produced directly from the reasoning model can be approximated by post-processing the code with a fixed function across tasks and models. For each task instance $x$ drawn from a held-out test distribution over 21 different algorithmic problems in CLRS, we consider two conditional distributions: Arm~1: $p_{NL} (\cdot \mid x)$, original NL traces generated by an LLM (Gemini 2.0 Flash in our case) and Arm~2 Translated: $p_{Translated} (\cdot \mid x)$, NL traces obtained by first generating code by an LLM, then translating that code into natural language using a fixed translator.

\textbf{Evaluation Setup}  We formulate a binary classification task in which a powerful judge model (Claude Opus 4.5) is given a problem instance x, and a single reasoning trace z, and must predict whether z was generated by Arm~1, or the translated Arm~2. This setup corresponds to discriminating between the joint distributions $p(x)p_{NL}(z \mid x)$ and $p(x)p_{Translated}(z \mid x)$. For each evaluation task x, we construct two samples $(x, z_{NL})$, where $z_{NL} \sim p_{NL}(\cdot \mid X)$, labelled Original, and $(x, \hat z_{NL})$ where $\hat z_{NL} \sim p_{Translated} ( \cdot \mid x)$, labelled Translated. The label counts are balanced on the test set, and the judge's accuracy, and AUC are computed on held-out tasks (disjoint from any in-context prompting examples).

In the evaluation, all generation parameters are fixed, model, prompt, and decoding hyperparameters, and the only variability is the seed for both the original Arm~1 and translated Arm~2, i.e. we compare samples from two fixed conditionals given a task instance.

We ensure the judge has enough discriminative power by asking it to classify between raw code and the native NL reasoning. High accuracy would indicate that the performance results on the main evaluation is not due to an underpowered judge. We prompt the judge zero-shot, instructing it that there are original natural language reasoning traces and translated ones that it must distinguish.

The translator, gemini-2.0 flash, is given 10 in-context pairs between code in Arm~2 and original natural language reasoning in Arm~1, and asked to learn the mapping and apply on new instances. The 10 in-context examples are 10 different tasks, which are disjoint from the 21 other tasks in CLRS benchmark that we evaluate on. \yu{To me the translator should be the same as the arm~1 model, also a better model cannot disambiguate them doesn't mean that it has similar distribution}

\begin{figure*}[t]
    \centering
    \includegraphics[width=1\linewidth]{images/discrimination_by_task.png}
    \caption{Discrimination accuracy by task: analysis of how well different representations (code vs. natural language) discriminate between algorithm types across task families.}
    \label{fig:discrimination_by_task}
\end{figure*}

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{images/translation_additivity_plot.png}
    \caption{Translation additivity analysis: functional similarity between translated NL and original NL reasoning traces.}
    \label{fig:translation_additivity}
\end{figure}

\textbf{Results.} Across 2000 samples, we find that the accuracy of the prompt is 49.4\% $\in [47.2\%. 51.5\%]$ Wilson CI, obtaining a 0.479 AUC. The control accuracy is $79.0\%$ so the discriminator is calibrated. We include per-task analysis in the appendix. We see that this is generally true across different models with only a few exceptions between rod cutting and the kruskal algorithm problems.

% We hypothesize that the natural language reasoning in Arm 1 can be obtained by post-processing Arm 2. We fix a post-processer P, which is an LLM with a fixed prompt in our evaluation. The prompt in P has 10 in-context examples that shows how to translate between code and the original reasoning. We fix these across different tasks, and evaluate on held-out tasks in the CLRS 30 benchmark data (n=1500). We evaluate with a fixed translator (GPT 5.2 Pro) and a fixed evaluator (Claude Opus 4).

\subsection{Functional Similarity between Translated NL and original NL}

We wish to verify whether the translated natural language reaosning from the experiment before has the same functionality as the original natural language reasoning produced in Arm~1.

\textbf{Evaluation Setup}. Given a task instance $x$, we prompt the target language model (gemini) three ways. 1. Baseline: $x$ (question only), 2. Arm~1: $x || z_{NL}$ 3. Translated Arm~2: $x || \hat z_{NL}$ where $\hat z_{NL}$ is obtained by translating code to natural language. If the translated NL loses information relative to the original NL, then conditioning on $\hat z_{NL}$ should yield worse performance than conditioning on $z_{NL}$. We run on 1000 samples and report results on held-out tasks disjoint from the ICL example tasks. The same prompt is used for the translator in the previous experiment as the one used here.

\textbf{Results}. We fail to reject the null hypothesis on 1000 samples, and see that using the original Arm~1 and the translated Arm~2 concatenated to the same original task prompt yield the same results.

\textbf{Qualitative Analysis} In observing the LLM responses, we notice the model's have a propensity to follow similar reasoning. This happens across individual models, but problem solving methodology is surprisingly similar across models. Show figure here.




% We wish to verify whether the translated NL and the NL are similar in distribution. By taking a piece of code, then translating and simulating it into natural language reasoning, we get a piece of CoT that is similar to the original. We ask whether this is distinguishable, in order to verify assumption 2. Assumption 1 is validated by design, since we can create a translator T via prompting an LLM.

% Since the translator $T(Z_nl | Z_code)$ is independent of the task X, we prompt the translator LLM (GPT 5.2) to not see the task, but rather use a fixed prompt with in-context examples from adjacent translating tasks (5-shot). During the translation, test examples' problem tasks are not shown to the translator, only the fixed prompt and piece of code. The evaluation is conducted on held-out tasks not seen during the tuning of the prompt, and the prompt is frozen after tuning. Translator is applied uniformly across tasks, prompts, and decoding params.
