



\section{Evaluating Translated NL and NL Distributional Similarity.}

One key hypothesis is that natural language reasoning follows a deeper algorithmic computation encoded in its representations. If this is the case, it would make sense to surface the code and delegate the execution to an external runtime, rather than simulating noisy execution. We test whether natural language reasoning generated by information contained in the code alone can be distributionally similar to natural language reasoning generated from the prompt alone in Arm~1. Then, we test whether they are functionally similar.

\subsection{Distributional Similarity between Translated NL and original NL}
We show that the distribution of traces produced directly from the reasoning model can be approximated by post-processing the code with a fixed task-independent transformation. 

\textbf{Evaluation Setup} For each evaluation task $x$ and corresponding code $c$, we construct two samples from CoT we observed in experiment 1. We leverage the distribution of Arm~1 reasoning traces $p_{\mathrm{NL}}(\cdot \mid x)$ and a matched distribution generated by a translator $T$ (GPT-4o with 10 in-context examples) $p_{\mathrm{Tr}}(\cdot \mid c)$ :
\begin{align*}
    & (x, z_{\mathrm{NL}}), \quad z_{\mathrm{NL}} \sim p_{\mathrm{NL}}(\cdot \mid x), \quad \text{labelled Native}. \\
    & (x, \hat z_{\mathrm{NL}}), \quad \hat z_{\mathrm{NL}} \sim p_{\mathrm{Tr}}(\cdot \mid c), \quad \text{labelled Translated}.
\end{align*}
We then formulate a binary classification task in which a powerful zero-shot judge model (Claude Opus 4.0, Gemini 2.5 Pro, Grok 4.1 Fast) is given a problem instance $X$, and a single reasoning trace $Z$, and prompted to predict whether z was generated by Arm~1, or the translated Arm~2. This setup corresponds to discriminating between the joint distributions $$p(x)p_{\mathrm{NL}}(z \mid x) \text{  and  } p(x)p_{\mathrm{Tr}}(z \mid x)$$
We sample problem instances $x$ randomly from a pool of 30,000 CoT examples from before spread across different models, seeds, tasks, hardness. Fixing these, we obtain paired outcomes corresponding to an $x$. We ensure that label counts between $\hat z_{\mathrm{NL}}$ and $ z_{\mathrm{NL}}$ are balanced on the test set, and tasks (21 tasks from CLRS 30 Benchmark) are disjoint from any in-context prompting examples. 

As a control, we report the judge's discriminative power by asking it to classify between raw code and the Native NL reasoning, where high accuracy would indicate that performance results on the main evaluation is not due to an underpowered judge. 

\begin{figure*}[t]
    \centering
    \includegraphics[width=1\linewidth]{images/discrimination_by_task.png}
    \caption{Discrimination accuracy by task: analysis of how well different representations (code vs. natural language) discriminate between algorithm types across task families.}
    \label{fig:discrimination_by_task}
\end{figure*}

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{images/translation_additivity_plot.png}
    \caption{Translation additivity analysis: functional similarity between translated NL and original NL reasoning traces.}
    \label{fig:translation_additivity}
\end{figure}

\textbf{Results.} Across 2000 samples and 3 judge models (6000 samples total), we find that the accuracy of the prompt is 49.4\% $\in [47.2\%. 51.5\%]$ Wilson CI, the control accuracy is $79.0\%$ so the discriminators are calibrated. This holds within-model and within-task with a few exceptions being rod-cutting 1D dynamic programming and kruskal's MST algorithm problems. These findings are surprising since one might expect models to have different reasoning patterns, but our results explain that there exists a post-processing model that can yield the same reasoning process under the right prompting conditions. 


\subsection{Functional Similarity between Translated NL and original NL}

We wish to verify whether the translated natural language reasoning from the experiment before has the same functionality as the original natural language reasoning produced in Arm~1.

\textbf{Evaluation Setup}. Given a task instance $x$, we prompt the target language model three ways. 
\begin{align*}
    & 1. \; \text{Baseline:} \; x \; \text{(question only)} \\
    & 2. \; \text{Arm~1:} \; x || z_{NL} \\
    & 3. \; \text{Translated Arm~2:} \; x || \hat z_{NL}
\end{align*}
 where $\hat z_{NL}$ is obtained by translating code to natural language. If the translated NL loses information relative to the original NL, then conditioning on $\hat z_{NL}$ should yield worse performance than conditioning on $z_{NL}$. We run on 1000 samples across 3 models (Claude Opus 4.0, Gemini 2.5 Pro, Grok 4.1 Fast) and report results on held-out tasks disjoint from the ICL example tasks. The same prompt is used for the translator in the previous experiment as the one used here. Crucially, the translator models we use are the same as the models that generated the original code $x$, unlike the experiment from before. 

\textbf{Results}. We fail to reject the null hypothesis, and see overlapping 95\% confidence intervals for Native and Translated. This leads us to believe that post-processed code and natural language reasoning have similar functionality quantified by end-task accuracy. Since we fixed the natural language reasoners here (same model), and only vary whether we feed in the original prompt and the generated code (with prompt masked), we infer that code and the original prompt share similar information, i.e. code does not lose much information. We also conclude that the algorithmic representations in code are simulated by the natural language reasoning. 

% \textbf{Qualitative Analysis} In observing the LLM responses, we notice the model's have a propensity to follow similar reasoning. This happens across individual models, but problem solving methodology is surprisingly similar across models. Show figure here.




% We wish to verify whether the translated NL and the NL are similar in distribution. By taking a piece of code, then translating and simulating it into natural language reasoning, we get a piece of CoT that is similar to the original. We ask whether this is distinguishable, in order to verify assumption 2. Assumption 1 is validated by design, since we can create a translator T via prompting an LLM.

% Since the translator $T(Z_nl | Z_code)$ is independent of the task X, we prompt the translator LLM (GPT 5.2) to not see the task, but rather use a fixed prompt with in-context examples from adjacent translating tasks (5-shot). During the translation, test examples' problem tasks are not shown to the translator, only the fixed prompt and piece of code. The evaluation is conducted on held-out tasks not seen during the tuning of the prompt, and the prompt is frozen after tuning. Translator is applied uniformly across tasks, prompts, and decoding params.
