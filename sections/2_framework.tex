\section{Evaluation Framework}\label{method}
We formalize our central claim as $\mathrm{Acc}(\mathrm{Arm~1}) \leq \mathrm{Acc}(\mathrm{Arm~2}) < \mathrm{Acc}(\mathrm{Arm~3})$. The following sections detail how we break down the problem and evaluate pairwise $\mathrm{Acc}(\mathrm{Arm~1}) \leq \mathrm{Acc}(\mathrm{Arm~2})$ and then $\mathrm{Acc}(\mathrm{Arm~2}) \leq \mathrm{Acc}(\mathrm{Arm~3})$.

\subsection{Methodology}
\textbf{Arm~1 $\leq$ Arm~2 Setup.} Similarly, given test input $X_i$ corresponding to instance i, we prompt the LLM to reason about it using natural language in Arm~1, mapping $(X_i) \to Y_i^{(\mathrm{NL})}$. Likewise, Arm 2 can be written as using the same LLM to first $(X_i) \to C_i$ and then $(X_i, C_i) \to Y_i^{(\mathrm{Sim})}$. Example prompts are shown in Figure 2. We control the prompts to be as similar as possible, with the Arm 2 simply concatenating a section that says it should generate code and simulate it.

\textbf{Arm~2 $<$ Arm~3 Setup.} Let the original problem statement corresponding to instance i be $X_i$, and let $C_i$ be the corresoponding code generated for instance i. Let tuple $(X_i, C_i)$ be fixed inputs in our experimental design. Let Arm~2 denote the output $Y_i^{(\mathrm{Sim})}$ which is mapped to by an LLM $(X_i, C_i) \to Y_i^{(\mathrm{Sim})}$, and let Arm~3 denote the output $Y_i^{(\mathrm{Exec})}$  mapped to by an external python runtime $(X_i, C_i) \to Y_i^{(\mathrm{Exec})}$ \footnote{$X_i$ is ignored by executor, but we include it for notational symmetry}. Example prompts are show in Figure 2.

\textbf{Arm~1 $\leq$ Arm~2 $<$ Arm~3 Setup.}
For each problem instance $i$, we observe bernoulli outcomes $(Y_i^{(\mathrm{NL})}, Y_i^{(\mathrm{Sim})},Y_i^{(\mathrm{Exec})})$. We first test the overall arm effect using Cochran's Q test, evaluating the global null: 
$$ H_0 = \mathrm{E}[Y_i^{(NL)}] = \mathrm{E}[Y_i^{(Sim)}] = \mathrm{E}[Y_i^{(Exec)}]$$
i.e. that all arms have marginal success probabilities. Rejection of this null indicates at least one arm differs in accuracy. 

\begin{figure*}[t]
    \centering
    \includegraphics[width=1\linewidth]{images/fig_prompts_combined.png}
    \caption{Recovery rate analysis: proportion of cases where LLM simulation (Arm~2) produces the correct answer despite incorrect code (Arm~3 failure). Recovery rates remain below 5\% across all task families and models, confirming that solver execution dominates LLM simulation.}
    \label{fig:recovery_rate}
\end{figure*}
\subsection{Experiments}

Condition on rejecting the global null, following our framework, we break down the tests into Arm 1 and Arm 2 $(Y_i^{(\mathrm{NL})}, Y_i^{(\mathrm{Sim})})$, and Arm 2 and Arm 3 $(Y_i^{(\mathrm{Sim})},Y_i^{(\mathrm{Exec})})$. For these paired bernoulli outcomes, we run the McNemar test under the null that: 
\begin{align*}
& H_0: \Pr(Y^{(\mathrm{Sim})}=1,\,Y^{(\mathrm{Exec})}=0) \\
& =\Pr(Y^{(\mathrm{Sim})}=0,\,Y^{(\mathrm{Exec})}=1)
\end{align*}
 that is, when pairs disagree, each one (Sim $>$ NL) and (NL $<$ Sim) occur equally as often. We test an analogous null for Arm 2 and Arm 3. To quantify effect size, we take paired accuracies $$\Delta_{\mathrm{Sim - NL}} = \mathrm{Acc(Sim) - Acc(NL)}$$ and estimate its sampling distribution and 95\% confidence interval via a cluster bootstrap, resampling over instances $i$ with preserved pair outcomes $(Y_i^{(\mathrm{NL})},Y_i^{(\mathrm{Sim})})$. We run the same procedure for Arm 2 and Arm 3. 

Since we apply multiple statistical tests, we add Holm-Bonferoni corrections to control family wise error rate at $ \alpha = 5\%$.

Finally, to analyze how task difficulty interacts with different arms, we run a generalized linear mixed-effects model (GLMM) with a logistic link. $$ Y_i = \alpha + \beta_{\mathrm{arm}_i} + \gamma\,\tau_i + \delta_{\mathrm{arm}_i}\tau_i + \varepsilon_i,
\quad
\varepsilon_i \sim \text{Bernoulli noise}$$ We model arm and task hardness as fixed effects, along with their interaction, with problem instance and seed as random effects. 

\textbf{Data and Models. } We use the CLRS 30 Benchmark (n=500), NPHardEval Benchmark (n=540), and a custom fine-grained evaluation suite (n=540), across three seeds. We define our own task suite---Arithmetic, Dynamic Programming, Integer Linear Programming (ILP)---to modulate hardness with parameter $\tau$. For arithmetic, $\tau$ controls digit length; for DP, it controls table dimensionality; for ILP, it controls the constraint matrix size. We assume our algorithms are representative enough of the distribution. We select frontier models (Haiku 4.5, GPT-4o, Gemini 2.0 Flash) as well as open-source models (Mixtral, Codestral). Since we require structured output, we filter out models that give $>$50\% JSON Parse Error, since this is indicative of instruction-following failures, rather than lack of coding fidelity.

\textbf{Prompting to generate Code and Reasoning Traces. }
We prompt the LLM in Arm~1 to never use any code in its reasoning, and to give a structured output of the rationale and answer to the algorithmic problem \cref{fig:arm_prompts}. Holding all other parts of the prompt the same, we replace the section instructing the model not to use code with another asking it to generaate code first before reasoning through it. Models have access to native python packages, and numpy, pandas, scipy, PuLP, and pytorch. For Arm~3, we take the generated function (no prompt), and execute it in a python3 runtime. We ask model's to output a structured json following a corresponding schema for that arm, and example is show in \cref{fig:arm_prompts}.

\textbf{Arm~1 $\leq$ Arm~2 $<$ Arm~3 Results. } 
For Arm~1 = Arm~2 = Arm~3, we strongly reject the global null hypothesis (p $<$ 0.001 after controlling for FWER). 

Our post-hoc paired tests Arm~2 $<$ Arm~3, strongly reject the null hypothesis too (p $<$ 0.001). We observe the paired accuracy gap is strictly large and positive, excluding zero in 95\% bootstrap confidence interval. Observing the distribution, the result is not driven by outliers, given the tightness of the distribution, indicating advantages over instance pairs and not just the average. This means that LLM execution via natural language reasoning of a piece of code has statistically significantly worse performances than code execution under the representative benchmarks and suites of task.

For post-hoc paired test Arm~1 $\leq$ Arm~2, despite running 30,000+ samples total and rejecting the null (p=0.04), our cluster bootstraps' 95\% confidence interval overlaps with 0.0, indicating that we cannot conclude that code simulation performs differently than natural language under our test benchmarks and sampling experimental design. This leads us to conclude that code and natural language are approximately the same. 

Observing \cref{fig:main}, we see that the dominance trends for Arm~2 $<$ Arm~3 and Arm~2 $\simeq$ Arm~3 holds across both open and closed models.



\textbf{Advantages of Arm~3 emerge as tasks get harder.} Observing \cref{fig:main}, we see that across 8 different algorithmic tasks, code execution strongly outperforms NL both on average and individually. Using the GLMM, we extrapolate the data on a log scale and observe that code and NL correctness probabilities diverges as tasks get harder, code remains stable whereas NL diverges to 0. Code has $4.1\times$ odds of getting the answer correct compared to natural language. 