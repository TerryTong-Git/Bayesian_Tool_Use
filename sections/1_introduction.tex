\section{Introduction} \label{sec:intro}

Many agentic systems orchestrate symbolic solvers, LLMs and other tools, and demonstrate state-of-the-art performance ~\cite{gao2023pal,yao2023react, 10.5555/3666122.3669119, yang2024sweagent,wang2025mcpbenchbenchmarkingtoolusingllm}. Prior research empirically demonstrates that translating problems into solver-executable code representations (hereafter coined Route 2) and delegating execution often outperforms reasoning end-to-end in NL (hereafter coined Route 1) \yu{I am a bit confused what is hereafter coined Route 1, also you should link figure 1 in the introduction} on logic- and algorithmic-style complex reasoning tasks ~\cite{lyu2023faithful, pan2023logic}. However, it is unclear whether benefits come from the more structured code representation itself used in the planning and reasoning process, or the reliability of external solvers, or both. Little progress has been made in this direction since the problem itself is ill-posed: two routes learn fundamentally different objects, so there is no common formal target and metric to compare. 

\begin{figure}[t]
    \centering
    \vspace{-5pt}
    \includegraphics[width=\linewidth]{images/CleanShot 2026-01-28 at 08.37.18@2x.png}
    \vspace{-25pt}
    \caption{\textbf{Three-Route Framework.} We decompose algorithmic reasoning into: (1) \emph{Translation} into NL or code, and (2) \emph{Execution} via LLM or solver. This yields three routes: \textbf{Route~1} (Direct NL), \textbf{Route~2} (Code + NL simulation), \textbf{Route~3} (Code + Solver Execution). Prior work compares only Route~1 vs.\ Route~3, confounding translation and execution. Our Route~2 isolates these factors. }
    \label{fig:three_routes}
\end{figure}

This paper presents a systematic three-route framework (\cref{sec:framework}) breaks down the question into tractable pairwise comparisons between the different routes. We decouple the end-to-end pipeline into a trace generation phase (e.g. Chain of Thought \cite{wei2022chain}) and an execution phase (see \cref{def:trace}). This allows us to fix one phase, and make causal claims on the effects of phase on end-to-end reasoning performance. We first instantiate this framework to verify the overall ordering Route 1 (NL + NL Reasoning) $\leq$ Route 2 (Code + NL Reasoning) $<$ Route 3 (Code +  Python Execution) holds (\cref{sec:empirical}). Solver-based execution results (+29.9\% v.s. NL reasoning)  in best performance across different models \texttt{(Mistral}~\cite{jiang2023mistral}, \texttt{OpenAI}~\cite{achiam2023gpt4}, \texttt{Gemini}~\cite{team2023gemini}, \texttt{Claude}~\cite{anthropic2024claude}).), tasks \texttt{(CLRS30}~\cite{velivckovic2022clrs}, \texttt{NP-Hard-Eval}~\cite{fan2023nphardeval}, Custom Eval Suite) and seeds $\{0,1,2\}$. Our results suggest Route 3 (Code + Python Execution) is optimal (\cref{fig:main,fig:per_task_accuracy}). To understand where language is bottlenecked, we operationalize our framework below. 

When analyzing the pair Route 1 v.s. Route 2 (\cref{sec:rep_analysis}), we fix the execution phase to be a LLM with NL reasoning, and vary the representation \yu{representation modality} of its Chain-of-Thought to be either code or NL. Because our goal is to optimize the end-to-end reasoning pipeline rather than any particular instantiation of it, we evaluate representations and executors in terms of their computation-constrained optimal risk (hereafter optimal risk; \cref{def:risk}). This criterion captures the best achievable performance within a model family and abstracts away suboptimal prompt or executor choices. We propose sufficient conditions such that the optimal risk of code is never worse than language up to a small amount of error $\varepsilon$ (\cref{prop:dominance}). Furthermore, we empirically verify these conditions through two experiments that show that natural language is an approximate garbling of code (\cref{sec:dist_similarity,sec:func_similarity}).

We further analyze Route 2 v.s. Route 3 (\cref{sec:route2v3}), identifying execution to be the bottleneck. We find that when code is correct, execution always achieves lower expected loss than language, and when code is incorrect, the probability that Route 2 $>$ Route 3 is small. On an analysis of hardness scaling, we find that code execution (Route 3) scales much better as problems become more difficult, and that the probabilities of Route 2 $>$ Route 3 when code is wrong reduces as problems get harder. 
% \yu{too dense, each paragraph should only have one focus, and some claims sound handwavy/overconfident, e.g., hinting at trace generation not being the bottleneck, (i.e. looks similar),}



% that works suggest that the \textbf{solver route}, translating problems into solver-executable code representations and delegating execution, often outperforms the \textbf{direct route}, reasoning end-to-end in NL, on logic- and algorithmic-style complex reasoning tasks. However, for algorithmic reasoning alone, there is still no systematic analysis comparing the two routes, clarifying why LLMs perform better via the solver route versus the direct route. 
% \yu{TODO: add more recent citations.}

% Despite these trends, for algorithmic reasoning it remains underexplored whether LLMs perform better via the \textbf{direct route}, reasoning end-to-end in NL, or the \textbf{solver route}, translating problems into solver-executable representations and delegating execution (Todo: despite.. add reference for there are many empirical results showing code is better).}


% A principled direct comparison is challenging since the routes operate over different representation spaces, i.e., NL traces versus solver-executable programs, and rely on different execution mechanisms, which prevents step-by-step alignment.
% Specifically, sample-complexity comparisons~\cite{bai2023transformers} are ill-posed here because the two routes learn fundamentally different objects, so there is no common formal target and metric to compare. Computational-complexity arguments~\cite{merrill2023expressive} are also not a clean discriminator here because the two routes incur fundamentally different execution-dependent costs. We therefore compare the two routes via statistical difficulty, using optimal achievable end-task Bayes risk~\cite{xie2021explanation}.

% In this paper, we propose a three-route framework that makes this comparison tractable by introducing an additional intermediate \textbf{simulation route}, where the model performs the same code translation but simulates execution in NL, other than the \textbf{direct route} and \textbf{solver route}, and verbalizes the representation using Chain-of-Thought~\cite{wei2022chain} as shown in Fig.~\ref{fig:three_routes}. Using this framework, we characterize why algorithmic reasoning favors the solver route, and empirically show that solver-based pipelines are significantly better for a broad class of algorithmic tasks. This framework also enables for a tractable theoretical comparison of the routes.

% showing that the simulation route is at least as good as the direct route under mild regularity assumptions: the direct route is a Blackwell-garbled (information-degraded) version of the simulation route, and thus cannot be more informative for downstream decision-making. Finally, we empirically demonstrate that the solver route substantially outperforms the simulation route, highlighting additional gains from reliable external execution.

\begin{figure*}[t]
    \centering
    \vspace{-5pt}
    \includegraphics[width=1\textwidth]{images/CleanShot 2026-01-28 at 16.25.56@2x.png}
    \vspace{-25pt}
    \caption{\textbf{Prompt templates for three-route evaluation.} \textbf{Route~1 (NL)}: LLM reasons in natural language only, code forbidden. \textbf{Route~2 (Sim)}: LLM generates Python \texttt{solution()} then simulates execution in NL. \textbf{Route~3 (Code)}: Same code executed in Python runtime. This isolates execution mechanism while controlling translation.}
    \label{fig:route_prompts}
\end{figure*}

% Using our framework, we consistently observe a three-route ordering across algorithmic reasoning tasks: the solver route performs best, followed by the simulation route, and then the direct route (\cref{fig:main,fig:per_task_accuracy}). Moreover, the solver route's advantage widens as task difficulty increases, with 4.01$\times$ odds of getting a correct answer for code over natural language reasoning. We evaluate our framework on CLRS30~\cite{velivckovic2022clrs}, NP-Hard-Eval~\cite{fan2023nphardeval}, and a custom suite of algorithmic problems with controllable difficulty (addition, multiplication, LCS, rod cutting, knapsack, and ILP variants: assignment, production, and partition) across a broad range of models, spanning weaker open-source LLMs (e.g., Mistral~\cite{jiang2023mistral}, LLaMA~\cite{touvron2023llama}, Qwen~\cite{yang2024qwen2}) and stronger closed-source systems (e.g., OpenAI~\cite{achiam2023gpt4}, Gemini~\cite{team2023gemini}, Claude~\cite{anthropic2024claude}). We find that, averaged over tasks and models, the solver route achieves XX\% accuracy, outperforming the simulation route (XX\%) and the direct route (XX\%) with statistical significance (XX). 

% Theoretically, we formalize the three reasoning routes as statistical experiments in the sense of Blackwell~\cite{}. We show that, under mild regularity assumptions, natural language reasoning traces can be viewed as a garbling of code-based representations, inducing a Blackwell ordering in which code is at least as informative as natural language for downstream decision-making. This implies that code-based reasoning achieves weakly lower Bayes risk for algorithmic tasks. We further model execution as an information channel and show that delegating execution to an external solver corresponds to a deterministic channel, while LLM-based simulation introduces additional stochastic noise. Together, these results establish a principled ordering—natural language $\leq$ code simulation $<$ code execution—providing an information-theoretic explanation for the empirical advantages of solver-based pipelines.

%Introduce the setting and the problem. Make sure to narrow down the scope. 
% Consider the algorithmic task of computing arithmetic operations encoded in natural language. We wish to compute:
% \vspace{-5pt}
% \begin{equation*}
%     p(\texttt{Y=3} | \texttt{X=What is one plus two?})
% \end{equation*}
% \vspace{-5pt}
% The language model forward pass could decide (1) to generate the solution directly with natural language reasoning \cite{wei2022chain} (2) translate the problem into code and use a solver \cite{gao2023pal}. This paper provides empirical evidence that $\mathrm{Arm~1} < \mathrm{Arm~3}$ quantified by end-task accuracy. A body of work shows that this pipeline is generally effective \cite{lyu2023faithful, pan2023logic}, further evidenced by the empirical success of tool-use\footnote{Here we primarily refer to solver-based tool-use as opposed to knowledge-intensive tool-use like RAG}. However, little progress has been made on explaining \emph{why} solver-based tools lead to higher end-task accuracy than natural language reasoning.

%Why the problem is hard theoretically. 
% One might be tempted to prove a statistical advantage by showing that sample complexity to learn code is less than natural language because code is structured, but will quickly find that this problem becomes intractable due to the hardness of capturing natural language under a mathematical framework. The same problem arises when attempting to use approximation theory to provide evidence that DNNs can better learn compositional or structured languages (like code) than natural language. Broadly speaking, the problem is challenging because the inputs and outputs are different, one is a structured language, the other is an unstructured language. This drastically complicates comparison. Or, one might be tempted to show a computational advantage by showing that code unlocks a new level of expressivity \cite{merrill2023expressive}. However, one will find that whether using a solver or not will not overcome the hardness of the problem. E.g., we can prove by contradiction that using a solver will not allow us to better solve NP-Hard problems, unless P=NP.  

%What we did 
% As a solution, we leverage a Bayesian Inference paradigm to reason about the two different settings \cite{xie2021explanation}. Doing so, enables us to break down the algorithmic reasoning pipeline into two distinct phases (1) Translation $\in \{\mathrm{Code}, \mathrm{NL}\}$  (2) Execution $\in \{\mathrm{LLM~Reasoning}, \mathrm{Solver~Execution}\}$ \cite{lyu2023faithful, pan2023logic}. Enumerating the valid combinations, we obtain three pairs, Arm~1: $\{\mathrm{NL~Gen}, \mathrm{LLM~Reasoning}\}$, Arm~2: $\{\mathrm{Code~Gen}, \mathrm{LLM~Reasoning}\}$, Arm~3: $\{\mathrm{Code~Gen}, \mathrm{Solver~Execution}\}$. Introducing Arm~2 makes the problem tractable. 

%Why the problem hasn't been solved empirically. 
% Empirically, this framework enables controlled comparisons. A rigorous comparison has not been instantiated because it is hard to control the experiment and determine what representation is actually being used---whether code, natural language, or something else. We overcome this by verbalizing the representation using Chain-of-Thought. We provide statistical evidence for the alternative hypothesis $\mathrm{Arm~3} > \mathrm{Arm~2} > \mathrm{Arm~1}$ on (Gemma, Deepseek, Llama), we demonstrate that generating code and executing leads to 78\% accuracy on (ILP, DP, Arithmetic) tasks, over 30\% for code simulation and just over 20\% for natural language reasoning across 5 models. Our results are computed over 5 seeds, and a Friedman Chi-square gives results a test statistic of 9277.32 and 8369.34 for deepseek and llama, enabling us to reject the null in favor of the alternative. 

% Theoretically, we first compare $\mathrm{Arm~1} < \mathrm{Arm~2}$. Bayesian Inference shows us that the LLM implicitly does multi-class classification to the right algorithm. We utilize information theory to capture natural language and code under the same framework, forgoing using grammars or other mathematical frameworks that are intractable. We reduce the comparison of Bayes Error to that of comparing cross-entropy. A intermediate step using mutual information makes the proof interpretable: we prove that the mutual information between the CoT and the final answer is higher when conditioned on code representations over natural language representations. We variationally lower bound the mutual information using cross-entropy of a proposal distribution parameterized by a logistic regression. Since we only care about orderings, we subtract to overcome the intractability of estimating the differential entropy, reducing the comparison of mutual information to that of cross-entropy. The cross-entropy is measured empirically using logistic regression on TF-IDF features and Bert-base-uncased features, showing that code has lower cross-entropy than NL and achieves higher accuracy when classifying the correct algorithm. The difference is statistically significant (F-test, $p < 0.05$). %no need to say the details of reduction and not using LLM uncalibrated probs? 

% Then we compare $\mathrm{Arm~2} < \mathrm{Arm~3}$. This difference is easily explained using a communication channel model of LLM forward-pass. We show that in the case where Arm~2 $>$ Arm~3, i.e. when the code generated is not executable or wrong, yet the LLM reasoning obtains the correct answer, that this occurs rarely. In other words, generally $\mathrm{Arm~3} > \mathrm{Arm~2}$. 

% Piecing these results together, we show that $\mathrm{Arm~1} < \mathrm{Arm~2} < \mathrm{Arm~3}$, verifying the hypothesis.




% Understanding this problem is crucial as we move towards compositional AI systems rather than monolithic architectures.
                 
Our main contributions are:
\begin{enumerate}
    \item A \textbf{three-route framework} (\cref{sec:framework}) for tractable comparison between code and natural language representations via an intermediary (code generation with LLM execution).
    \item \textbf{Empirical validation} (\cref{sec:empirical}) demonstrating that our framework's ordering holds such that Route 1 $\leq$ Route 2 $<$ Route 3.
    \item A \textbf{systematic study} (\cref{sec:rep_analysis,sec:route2v3}) ruling out trace generation, and solidifying execution as the bottleneck in end-to-end algorithmic reasoning performance when using language.
\end{enumerate}


\begin{figure*}[t]
    \centering
    \vspace{-5pt}
    \includegraphics[width=1\textwidth]{images/combined_accuracy_delta.png}
    \vspace{-25pt}
    \caption{\textbf{Code + Solver Execution performs better than Direct NL reasoning quantified by end-task accuracy overall and within paired instances.} Paired instance contrasts are shown in the bootstrap distributions. Results are averaged over CLR30, NPHardEval, Custom Eval Suite, 3 seeds and 6 models. }
    \label{fig:main}
\end{figure*}