\section{Related Work and Discussion}

\textbf{Neuro-symbolic Learning.} This paper builds on research in neuro-symbolic integration \cite{graves_neural_2014, velickovic_neural_2021, reed_neural_2016, graves_hybrid_2016}, which combines neural networks with symbolic reasoning systems. These approaches are motivated by cognitive science \cite{schneider_controlled_2003, risko_cognitive_2016, anderson_neural_2010}, hierarchical reinforcement learning \cite{kolter_hierarchical_2007, dietterich_hierarchical_2000}, and compositionality research \cite{hudson_compositional_2018, hupkes_compositionality_2020, andreas_neural_2017, poggio2017and}. An orthogonal line of work explores direct execution of algorithms by neural networks \cite{velickovic_neural_2021, mahdavi_towards_2023, ibarz_generalist_2022, yan_neural_2020}. Unlike these approaches that focus on \emph{how} to integrate neural and symbolic components, our work addresses \emph{why} symbolic execution outperforms neural reasoning for algorithmic tasks.

\textbf{LLM Reasoning.} Recent work has explored various reasoning paradigms for LLMs, including symbolic reasoning \cite{marra_integrating_2019, olausson_linc_2023, han_folio_2024}, chain-of-thought prompting \cite{altabaa2025cot, zelikman_star_2022, merrill_expressive_2024, altabaa_cot_2025}, and in-context learning \cite{xie2021explanation, garg2022can, akyurek2022learning, zhang2024trained}. \citet{xie2021explanation} model in-context learning as implicit Bayesian inference, which we extend to compare different reasoning representations. While prior work demonstrates \emph{that} certain prompting strategies improve performance, we provide a theoretical framework explaining \emph{why} code representations lead to lower Bayes error.

\textbf{LLM Tool-Use.} Tool-augmented LLMs have achieved strong empirical results \cite{shen_llm_2024, schick_toolformer_nodate, qin_toolllm_2023, tang_toolalpaca_2023, parisi_talm_2022}. Code generation for tool-use can be viewed as a form of semantic parsing \cite{shin_few-shot_2022, krishnamurthy_neural_2017, berant_semantic_2013, dong_language_2016} or function calling \cite{puri_codenet_2021, alon_code2vec_2019, chen_neural_2018}. Our work complements this literature by providing theoretical justification for the observed empirical advantages of code-based tool-use over direct natural language reasoning. 

% \section{Discussion}

% \textbf{Summary.} This paper addresses whether algorithmic problems encoded in natural language should be solved via direct reasoning or by translating to code and executing with a solver. Our three-arm framework demonstrates that code execution consistently outperforms both code simulation and natural language reasoning, with theoretical backing from information theory.

% \textbf{Interpretation.} The theoretical analysis reveals that code representations yield higher mutual information with target algorithms than natural language representations. This explains the empirical observation: code serves as a more discriminative intermediate representation for implicit algorithm classification during LLM inference. The Bayesian framework makes this comparison tractable by decomposing the problem into translation and execution phases.
