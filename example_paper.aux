\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{}
\citation{}
\citation{lyu2023faithful,pan2023logic}
\citation{}
\citation{}
\citation{}
\newlabel{intro}{{1}{1}{}{section.1}{}}
\newlabel{intro@cref}{{[section][1][]1}{[1][1][]1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:three_arms}{{1}{1}{Bayesian Inference model showing the \emph {three arms methodology} in \cref {method}. Given an algorithmic problem, we may split it into two steps (1) Translate (2) Execute. The (1) translation $\in \{\mathrm {Code}, \mathrm {NL}\}$. Then (2) execution $\in \{\mathrm {LLM~Reasoning}, \mathrm {Solver~Execution}\}$. We have three pairs, Arm~1: $\{\mathrm {NL~Gen}, \mathrm {LLM~Reasoning}\}$, Arm~2: $\{\mathrm {Code~Gen}, \mathrm {LLM~Reasoning}\}$, Arm~3: $\{\mathrm {Code~Gen}, \mathrm {Solver~Execution}\}$. Typically, the problem is tackled by comparing Arm~1 and Arm~3 in neuro-symbolic literature, which is intractable theoretically, and uncontrolled since multiple variables are changing. By introducing Arm~2, the problem becomes tractable. In the diagram, the \emph {shaded} circles correspond to observed R.V.\ and \emph {white} correspond to unobserved. The notation for R.V.s is correspondingly used in \cref {method}}{figure.caption.1}{}}
\newlabel{fig:three_arms@cref}{{[figure][1][]1}{[1][1][]1}}
\citation{}
\citation{}
\citation{}
\citation{}
\citation{}
\citation{}
\citation{}
\citation{}
\citation{}
\newlabel{fig:arm_prompts}{{2}{2}{Prompt templates for the three-arm evaluation framework. Arm~1 instructs the model to reason purely in natural language without code. Arm~2 instructs the model to generate code and simulate its execution. Arm~3 uses the same code generation prompt but executes the output in a Python runtime rather than simulating}{figure.caption.2}{}}
\newlabel{fig:arm_prompts@cref}{{[figure][2][]2}{[1][1][]2}}
\newlabel{fig:main}{{3}{3}{Accuracy scaling across task difficulty ($\tau $) for Arithmetic, Dynamic Programming, and ILP tasks. Arm~3 (code execution) maintains high accuracy as problems get harder, while Arms~1 and~2 degrade. The widening gap demonstrates that solver execution becomes increasingly advantageous for challenging algorithmic problems}{figure.caption.3}{}}
\newlabel{fig:main@cref}{{[figure][3][]3}{[1][2][]3}}
\newlabel{method}{{2}{3}{}{section.2}{}}
\newlabel{method@cref}{{[section][2][]2}{[1][3][]3}}
\newlabel{fig:prompts_combined}{{4}{4}{Recovery rate analysis: proportion of cases where LLM simulation (Arm~2) produces the correct answer despite incorrect code (Arm~3 failure). Recovery rates remain below 5\% across all task families and models, confirming that solver execution dominates LLM simulation}{figure.caption.4}{}}
\newlabel{fig:prompts_combined@cref}{{[figure][4][]4}{[1][3][]4}}
\newlabel{fig:recovery_rate}{{5}{5}{Recovery rate analysis: proportion of cases where LLM simulation (Arm~2) produces the correct answer despite incorrect code (Arm~3 failure). Recovery rates remain below 5\% across all task families and models, confirming that solver execution dominates LLM simulation}{figure.caption.5}{}}
\newlabel{fig:recovery_rate@cref}{{[figure][5][]5}{[1][3][]5}}
\citation{blackwell1953equivalent}
\citation{blackwell1953equivalent}
\newlabel{fig:discrimination_by_task}{{6}{6}{Discrimination accuracy by task: analysis of how well different representations (code vs. natural language) discriminate between algorithm types across task families}{figure.caption.6}{}}
\newlabel{fig:discrimination_by_task@cref}{{[figure][6][]6}{[1][5][]6}}
\newlabel{fig:translation_additivity}{{7}{6}{Translation additivity analysis: functional similarity between translated NL and original NL reasoning traces}{figure.caption.7}{}}
\newlabel{fig:translation_additivity@cref}{{[figure][7][]7}{[1][5][]6}}
\citation{graves_neural_2014,velickovic_neural_2021,reed_neural_2016,graves_hybrid_2016}
\citation{schneider_controlled_2003,risko_cognitive_2016,anderson_neural_2010}
\citation{kolter_hierarchical_2007,dietterich_hierarchical_2000}
\citation{hudson_compositional_2018,hupkes_compositionality_2020,andreas_neural_2017,poggio2017and}
\citation{velickovic_neural_2021,mahdavi_towards_2023,ibarz_generalist_2022,yan_neural_2020}
\newlabel{fig:recovery_final}{{8}{8}{Recovery rate analysis: proportion of cases where LLM simulation (Arm~2) produces the correct answer despite incorrect code (Arm~3 failure). Recovery rates remain below 5\% across all task families and models, confirming that solver execution dominates LLM simulation}{figure.caption.8}{}}
\newlabel{fig:recovery_final@cref}{{[figure][8][]8}{[1][8][]8}}
\citation{marra_integrating_2019,olausson_linc_2023,han_folio_2024}
\citation{altabaa2025cot,zelikman_star_2022,merrill_expressive_2024,altabaa_cot_2025}
\citation{xie2021explanation,garg2022can,akyurek2022learning,zhang2024trained}
\citation{xie2021explanation}
\citation{shen_llm_2024,schick_toolformer_nodate,qin_toolllm_2023,tang_toolalpaca_2023,parisi_talm_2022}
\citation{shin_few-shot_2022,krishnamurthy_neural_2017,berant_semantic_2013,dong_language_2016}
\citation{puri_codenet_2021,alon_code2vec_2019,chen_neural_2018}
\bibdata{example_paper}
\bibcite{akyurek2022learning}{{1}{2022}{{Aky{\"u}rek et~al.}}{{Aky{\"u}rek, Schuurmans, Andreas, Ma, and Zhou}}}
\bibcite{alon_code2vec_2019}{{2}{2019}{{Alon et~al.}}{{Alon, Zilberstein, Levy, and Yahav}}}
\bibcite{altabaa2025cot}{{3}{2025{a}}{{Altabaa et~al.}}{{Altabaa, Montasser, and Lafferty}}}
\bibcite{altabaa_cot_2025}{{4}{2025{b}}{{Altabaa et~al.}}{{Altabaa, Montasser, and Lafferty}}}
\bibcite{anderson_neural_2010}{{5}{2010}{{Anderson}}{{}}}
\bibcite{andreas_neural_2017}{{6}{2017}{{Andreas et~al.}}{{Andreas, Rohrbach, Darrell, and Klein}}}
\bibcite{berant_semantic_2013}{{7}{2013}{{Berant et~al.}}{{Berant, Chou, Frostig, and Liang}}}
\bibcite{blackwell1953equivalent}{{8}{1953}{{Blackwell}}{{}}}
\bibcite{chen_neural_2018}{{9}{2018}{{Chen \& Zhou}}{{Chen and Zhou}}}
\bibcite{dietterich_hierarchical_2000}{{10}{2000}{{Dietterich}}{{}}}
\bibcite{dong_language_2016}{{11}{2016}{{Dong \& Lapata}}{{Dong and Lapata}}}
\bibcite{garg2022can}{{12}{2022}{{Garg et~al.}}{{Garg, Tsipras, Liang, and Valiant}}}
\bibcite{graves_neural_2014}{{13}{2014}{{Graves et~al.}}{{Graves, Wayne, and Danihelka}}}
\bibcite{graves_hybrid_2016}{{14}{2016}{{Graves et~al.}}{{Graves, Wayne, Reynolds, Harley, Danihelka, Grabska-Barwińska, Colmenarejo, Grefenstette, Ramalho, Agapiou, Badia, Hermann, Zwols, Ostrovski, Cain, King, Summerfield, Blunsom, Kavukcuoglu, and Hassabis}}}
\bibcite{han_folio_2024}{{15}{2024}{{Han et~al.}}{{Han, Schoelkopf, Zhao, Qi, Riddell, Zhou, Coady, Peng, Qiao, Benson, Sun, Wardle-Solano, Szabo, Zubova, Burtell, Fan, Liu, Wong, Sailor, Ni, Nan, Kasai, Yu, Zhang, Fabbri, Kryscinski, Yavuz, Liu, Lin, Joty, Zhou, Xiong, Ying, Cohan, and Radev}}}
\bibcite{hudson_compositional_2018}{{16}{2018}{{Hudson \& Manning}}{{Hudson and Manning}}}
\bibcite{hupkes_compositionality_2020}{{17}{2020}{{Hupkes et~al.}}{{Hupkes, Dankers, Mul, and Bruni}}}
\bibcite{ibarz_generalist_2022}{{18}{2022}{{Ibarz et~al.}}{{Ibarz, Kurin, Papamakarios, Nikiforou, Bennani, Csordás, Dudzik, Bošnjak, Vitvitskyi, Rubanova, Deac, Bevilacqua, Ganin, Blundell, and Veličković}}}
\bibcite{kolter_hierarchical_2007}{{19}{2007}{{Kolter et~al.}}{{Kolter, Abbeel, and Ng}}}
\bibcite{krishnamurthy_neural_2017}{{20}{2017}{{Krishnamurthy et~al.}}{{Krishnamurthy, Dasigi, and Gardner}}}
\bibcite{lyu2023faithful}{{21}{2023}{{Lyu et~al.}}{{Lyu, Havaldar, Stein, Zhang, Rao, Wong, Apidianaki, and Callison-Burch}}}
\bibcite{mahdavi_towards_2023}{{22}{2023}{{Mahdavi et~al.}}{{Mahdavi, Swersky, Kipf, Hashemi, Thrampoulidis, and Liao}}}
\bibcite{marra_integrating_2019}{{23}{2019}{{Marra et~al.}}{{Marra, Giannini, Diligenti, and Gori}}}
\bibcite{merrill_expressive_2024}{{24}{2024}{{Merrill \& Sabharwal}}{{Merrill and Sabharwal}}}
\bibcite{olausson_linc_2023}{{25}{2023}{{Olausson et~al.}}{{Olausson, Gu, Lipkin, Zhang, Solar-Lezama, Tenenbaum, and Levy}}}
\bibcite{pan2023logic}{{26}{2023}{{Pan et~al.}}{{Pan, Albalak, Wang, and Wang}}}
\bibcite{parisi_talm_2022}{{27}{2022}{{Parisi et~al.}}{{Parisi, Zhao, and Fiedel}}}
\bibcite{poggio2017and}{{28}{2017}{{Poggio et~al.}}{{Poggio, Mhaskar, Rosasco, Miranda, and Liao}}}
\bibcite{puri_codenet_2021}{{29}{2021}{{Puri et~al.}}{{Puri, Kung, Janssen, Zhang, Domeniconi, Zolotov, Dolby, Chen, Choudhury, Decker, Thost, Buratti, Pujar, Ramji, Finkler, Malaika, and Reiss}}}
\bibcite{qin_toolllm_2023}{{30}{2023}{{Qin et~al.}}{{Qin, Liang, Ye, Zhu, Yan, Lu, Lin, Cong, Tang, Qian, Zhao, Hong, Tian, Xie, Zhou, Gerstein, Li, Liu, and Sun}}}
\bibcite{reed_neural_2016}{{31}{2016}{{Reed \& Freitas}}{{Reed and Freitas}}}
\bibcite{risko_cognitive_2016}{{32}{2016}{{Risko \& Gilbert}}{{Risko and Gilbert}}}
\bibcite{schick_toolformer_nodate}{{33}{}{{Schick et~al.}}{{Schick, Dessì, Raileanu, Lomeli, Zettlemoyer, Cancedda, and Scialom}}}
\bibcite{schneider_controlled_2003}{{34}{2003}{{Schneider \& Chein}}{{Schneider and Chein}}}
\bibcite{shen_llm_2024}{{35}{2024}{{Shen}}{{}}}
\bibcite{shin_few-shot_2022}{{36}{2022}{{Shin \& Durme}}{{Shin and Durme}}}
\bibcite{tang_toolalpaca_2023}{{37}{2023}{{Tang et~al.}}{{Tang, Deng, Lin, Han, Liang, Cao, and Sun}}}
\bibcite{velickovic_neural_2021}{{38}{2021}{{Veličković \& Blundell}}{{Veličković and Blundell}}}
\bibcite{xie2021explanation}{{39}{2021}{{Xie et~al.}}{{Xie, Raghunathan, Liang, and Ma}}}
\bibcite{yan_neural_2020}{{40}{2020}{{Yan et~al.}}{{Yan, Swersky, Koutra, Ranganathan, and Hashemi}}}
\bibcite{zelikman_star_2022}{{41}{2022}{{Zelikman et~al.}}{{Zelikman, Wu, Mu, and Goodman}}}
\bibcite{zhang2024trained}{{42}{2024}{{Zhang et~al.}}{{Zhang, Frei, and Bartlett}}}
\bibstyle{icml2025}
\gdef \@abspage@last{11}
