\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{wei2022chain}
\citation{gao2023pal}
\citation{lyu2023faithful,pan2023logic}
\citation{merrill2023expressive}
\newlabel{intro}{{1}{1}{}{section.1}{}}
\newlabel{intro@cref}{{[section][1][]1}{[1][1][]1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:three_arms}{{1}{1}{Bayesian Inference model showing the \emph {three arms methodology} in \cref {exp:0}. Given an algorithmic problem, we may split it into two steps (1) Translate (2) Execute. The (1) translation $\in \{ \mathrm {Code}, \mathrm {NL }\}$. Then (2) execution $\in \{ \mathrm {LLM \ Reasoning}, \mathrm {Solver \ Execution}\}$. We have three pairs, Arm 1: $\{\mathrm {NL \ Gen},\mathrm {LLM \ Reasoning}\}$, Arm 2: $\{\mathrm {Code \ Gen}, \mathrm {LLM \ Reasoning}\}$, Arm 3: $\{\mathrm {Code \ Gen} , \mathrm {Solver \ Execution}\}$. Typically, the problem is tackled by comparing Arm 1 and Arm 3 in neuro-symbolic literature, which is intractable theoretically, and uncontrolled since multiple variables are changing. By introducing Arm 2, the problem becomes tractable. In the diagram, the \emph {shaded} circles correspond to observed R.V. and \emph {white} correspond to unobserved. The notation for R.V.s is correspondingly used in \cref {prelim}}{figure.caption.1}{}}
\newlabel{fig:three_arms@cref}{{[figure][1][]1}{[1][1][]1}}
\citation{xie2021explanation}
\citation{lyu2023faithful,pan2023logic}
\citation{graves_neural_2014,velickovic_neural_2021,reed_neural_2016,graves_hybrid_2016}
\citation{schneider_controlled_2003,risko_cognitive_2016,anderson_neural_2010}
\citation{kolter_hierarchical_2007,dietterich_hierarchical_2000}
\citation{hudson_compositional_2018,hupkes_compositionality_2020,andreas_neural_2017,poggio2017and}
\citation{velickovic_neural_2021,mahdavi_towards_2023,ibarz_generalist_2022,yan_neural_2020}
\citation{marra_integrating_2019,olausson_linc_2023,han_folio_2024}
\citation{altabaa2025cot,zelikman_star_2022,merrill_expressive_2024,altabaa_cot_2025}
\citation{xie2021explanation,garg2022can,akyurek2022learning,zhang2024trained}
\citation{xie2021explanation}
\citation{shen_llm_2024,schick_toolformer_nodate,qin_toolllm_2023,tang_toolalpaca_2023,parisi_talm_2022}
\citation{shin_few-shot_2022,krishnamurthy_neural_2017,berant_semantic_2013,dong_language_2016}
\citation{puri_codenet_2021,alon_code2vec_2019,chen_neural_2018}
\citation{xie2021explanation}
\newlabel{prelim}{{3}{3}{}{section.3}{}}
\newlabel{prelim@cref}{{[section][3][]3}{[1][3][]3}}
\newlabel{exp:0}{{4}{3}{}{section.4}{}}
\newlabel{exp:0@cref}{{[section][4][]4}{[1][3][]3}}
\newlabel{fig:main}{{2}{4}{Accuracy comparison across the three reasoning arms for Deepseek-Coder and Gemma-2 models. Code execution (Arm 3) consistently outperforms code simulation (Arm 2) and natural language reasoning (Arm 1) across all task families: arithmetic, dynamic programming, and integer linear programming}{figure.caption.2}{}}
\newlabel{fig:main@cref}{{[figure][2][]2}{[1][4][]4}}
\newlabel{fig:line}{{3}{4}{Accuracy as a function of problem difficulty (hardness parameter $\tau $). As task complexity increases, the performance gap between arms widens, with code execution maintaining higher accuracy at greater difficulty levels}{figure.caption.3}{}}
\newlabel{fig:line@cref}{{[figure][3][]3}{[1][4][]4}}
\newlabel{fig:pval}{{4}{4}{Pairwise $p$-values from McNemar tests comparing accuracy between arms. All comparisons are statistically significant ($p < 0.05$), confirming the ordering Arm 1 $<$ Arm 2 $<$ Arm 3. We use non-parametric tests (Kruskal-Wallis for overall comparison, McNemar for pairwise) to avoid distributional assumptions}{figure.caption.4}{}}
\newlabel{fig:pval@cref}{{[figure][4][]4}{[1][4][]4}}
\citation{ton2024understanding,altabaa2025cot}
\citation{poole2019variational}
\newlabel{lem:1}{{5.0.1}{5}{}{lemma.5.0.1}{}}
\newlabel{lem:1@cref}{{[lemma][1][]5.0.1}{[1][5][]5}}
\newlabel{lem:2}{{5.0.2}{5}{Variational Lower Bound}{lemma.5.0.2}{}}
\newlabel{lem:2@cref}{{[lemma][2][]5.0.2}{[1][5][]5}}
\newlabel{thm:1}{{5.1}{5}{}{theorem.5.1}{}}
\newlabel{thm:1@cref}{{[theorem][1][5]5.1}{[1][5][]5}}
\citation{bai2023transformers}
\newlabel{cor:1}{{5.1.1}{6}{}{corollary.5.1.1}{}}
\newlabel{cor:1@cref}{{[corollary][1][]5.1.1}{[1][6][]6}}
\newlabel{exp:1}{{6}{6}{}{section.6}{}}
\newlabel{exp:1@cref}{{[section][6][]6}{[1][6][]6}}
\newlabel{exp:2}{{6.2}{6}{}{subsection.6.2}{}}
\newlabel{exp:2@cref}{{[subsection][2][6]6.2}{[1][6][]6}}
\newlabel{fig:vlb}{{5}{7}{Cross-entropy comparison between code and natural language representations. Code achieves significantly lower cross-entropy (higher mutual information bound) than NL for both TF-IDF and BERT features, validating \cref {cor:1}}{figure.caption.5}{}}
\newlabel{fig:vlb@cref}{{[figure][5][]5}{[1][7][]7}}
\newlabel{fig:mi_v_acc}{{6}{7}{Relationship between estimated mutual information and task accuracy. Each point represents a task-representation pair. Higher mutual information (lower cross-entropy) correlates with improved accuracy ($r = 0.73$, $p < 0.01$), supporting the theoretical prediction that code's informational advantage translates to performance gains}{figure.caption.6}{}}
\newlabel{fig:mi_v_acc@cref}{{[figure][6][]6}{[1][7][]7}}
\bibdata{example_paper}
\bibcite{akyurek2022learning}{{1}{2022}{{Aky{\"u}rek et~al.}}{{Aky{\"u}rek, Schuurmans, Andreas, Ma, and Zhou}}}
\bibcite{alon_code2vec_2019}{{2}{2019}{{Alon et~al.}}{{Alon, Zilberstein, Levy, and Yahav}}}
\newlabel{fig:recovery_final}{{7}{8}{Recovery rate analysis: proportion of cases where LLM simulation (Arm 2) produces the correct answer despite incorrect code (Arm 3 failure). Recovery rates remain below 5\% across all task families and models, confirming that solver execution dominates LLM simulation}{figure.caption.7}{}}
\newlabel{fig:recovery_final@cref}{{[figure][7][]7}{[1][7][]8}}
\newlabel{fig:recovery_powerlaw}{{8}{8}{Recovery rate as a function of task complexity (hardness parameter $\tau $). Recovery follows a power-law decay, indicating that the advantage of symbolic execution over LLM simulation grows with problem difficulty}{figure.caption.8}{}}
\newlabel{fig:recovery_powerlaw@cref}{{[figure][8][]8}{[1][7][]8}}
\newlabel{fig:kde}{{9}{8}{Kernel density estimation of accuracy distributions across the three arms. Arm 3 (code execution) exhibits a mode near 100\% accuracy, while Arms 1 and 2 show broader, lower-accuracy distributions. The bimodal structure of Arm 3 reflects the dichotomy between correct and incorrect code generation---when code is correct, execution is perfect}{figure.caption.9}{}}
\newlabel{fig:kde@cref}{{[figure][9][]9}{[1][7][]8}}
\bibcite{altabaa2025cot}{{3}{2025{a}}{{Altabaa et~al.}}{{Altabaa, Montasser, and Lafferty}}}
\bibcite{altabaa_cot_2025}{{4}{2025{b}}{{Altabaa et~al.}}{{Altabaa, Montasser, and Lafferty}}}
\bibcite{anderson_neural_2010}{{5}{2010}{{Anderson}}{{}}}
\bibcite{andreas_neural_2017}{{6}{2017}{{Andreas et~al.}}{{Andreas, Rohrbach, Darrell, and Klein}}}
\bibcite{bai2023transformers}{{7}{2023}{{Bai et~al.}}{{Bai, Chen, Wang, Xiong, and Mei}}}
\bibcite{berant_semantic_2013}{{8}{2013}{{Berant et~al.}}{{Berant, Chou, Frostig, and Liang}}}
\bibcite{chen_neural_2018}{{9}{2018}{{Chen \& Zhou}}{{Chen and Zhou}}}
\bibcite{dietterich_hierarchical_2000}{{10}{2000}{{Dietterich}}{{}}}
\bibcite{dong_language_2016}{{11}{2016}{{Dong \& Lapata}}{{Dong and Lapata}}}
\bibcite{gao2023pal}{{12}{2023}{{Gao et~al.}}{{Gao, Madaan, Zhou, Alon, Liu, Yang, Callan, and Neubig}}}
\bibcite{garg2022can}{{13}{2022}{{Garg et~al.}}{{Garg, Tsipras, Liang, and Valiant}}}
\bibcite{graves_neural_2014}{{14}{2014}{{Graves et~al.}}{{Graves, Wayne, and Danihelka}}}
\bibcite{graves_hybrid_2016}{{15}{2016}{{Graves et~al.}}{{Graves, Wayne, Reynolds, Harley, Danihelka, Grabska-Barwińska, Colmenarejo, Grefenstette, Ramalho, Agapiou, Badia, Hermann, Zwols, Ostrovski, Cain, King, Summerfield, Blunsom, Kavukcuoglu, and Hassabis}}}
\bibcite{han_folio_2024}{{16}{2024}{{Han et~al.}}{{Han, Schoelkopf, Zhao, Qi, Riddell, Zhou, Coady, Peng, Qiao, Benson, Sun, Wardle-Solano, Szabo, Zubova, Burtell, Fan, Liu, Wong, Sailor, Ni, Nan, Kasai, Yu, Zhang, Fabbri, Kryscinski, Yavuz, Liu, Lin, Joty, Zhou, Xiong, Ying, Cohan, and Radev}}}
\bibcite{hudson_compositional_2018}{{17}{2018}{{Hudson \& Manning}}{{Hudson and Manning}}}
\bibcite{hupkes_compositionality_2020}{{18}{2020}{{Hupkes et~al.}}{{Hupkes, Dankers, Mul, and Bruni}}}
\bibcite{ibarz_generalist_2022}{{19}{2022}{{Ibarz et~al.}}{{Ibarz, Kurin, Papamakarios, Nikiforou, Bennani, Csordás, Dudzik, Bošnjak, Vitvitskyi, Rubanova, Deac, Bevilacqua, Ganin, Blundell, and Veličković}}}
\bibcite{kolter_hierarchical_2007}{{20}{2007}{{Kolter et~al.}}{{Kolter, Abbeel, and Ng}}}
\bibcite{krishnamurthy_neural_2017}{{21}{2017}{{Krishnamurthy et~al.}}{{Krishnamurthy, Dasigi, and Gardner}}}
\bibcite{lyu2023faithful}{{22}{2023}{{Lyu et~al.}}{{Lyu, Havaldar, Stein, Zhang, Rao, Wong, Apidianaki, and Callison-Burch}}}
\bibcite{mahdavi_towards_2023}{{23}{2023}{{Mahdavi et~al.}}{{Mahdavi, Swersky, Kipf, Hashemi, Thrampoulidis, and Liao}}}
\bibcite{marra_integrating_2019}{{24}{2019}{{Marra et~al.}}{{Marra, Giannini, Diligenti, and Gori}}}
\bibcite{merrill2023expressive}{{25}{2023}{{Merrill \& Sabharwal}}{{Merrill and Sabharwal}}}
\bibcite{merrill_expressive_2024}{{26}{2024}{{Merrill \& Sabharwal}}{{Merrill and Sabharwal}}}
\bibcite{olausson_linc_2023}{{27}{2023}{{Olausson et~al.}}{{Olausson, Gu, Lipkin, Zhang, Solar-Lezama, Tenenbaum, and Levy}}}
\bibcite{pan2023logic}{{28}{2023}{{Pan et~al.}}{{Pan, Albalak, Wang, and Wang}}}
\bibcite{parisi_talm_2022}{{29}{2022}{{Parisi et~al.}}{{Parisi, Zhao, and Fiedel}}}
\bibcite{poggio2017and}{{30}{2017}{{Poggio et~al.}}{{Poggio, Mhaskar, Rosasco, Miranda, and Liao}}}
\bibcite{poole2019variational}{{31}{2019}{{Poole et~al.}}{{Poole, Ozair, Van Den~Oord, Alemi, and Tucker}}}
\bibcite{puri_codenet_2021}{{32}{2021}{{Puri et~al.}}{{Puri, Kung, Janssen, Zhang, Domeniconi, Zolotov, Dolby, Chen, Choudhury, Decker, Thost, Buratti, Pujar, Ramji, Finkler, Malaika, and Reiss}}}
\bibcite{qin_toolllm_2023}{{33}{2023}{{Qin et~al.}}{{Qin, Liang, Ye, Zhu, Yan, Lu, Lin, Cong, Tang, Qian, Zhao, Hong, Tian, Xie, Zhou, Gerstein, Li, Liu, and Sun}}}
\bibcite{reed_neural_2016}{{34}{2016}{{Reed \& Freitas}}{{Reed and Freitas}}}
\bibcite{risko_cognitive_2016}{{35}{2016}{{Risko \& Gilbert}}{{Risko and Gilbert}}}
\bibcite{schick_toolformer_nodate}{{36}{}{{Schick et~al.}}{{Schick, Dessì, Raileanu, Lomeli, Zettlemoyer, Cancedda, and Scialom}}}
\bibcite{schneider_controlled_2003}{{37}{2003}{{Schneider \& Chein}}{{Schneider and Chein}}}
\bibcite{shen_llm_2024}{{38}{2024}{{Shen}}{{}}}
\bibcite{shin_few-shot_2022}{{39}{2022}{{Shin \& Durme}}{{Shin and Durme}}}
\bibcite{tang_toolalpaca_2023}{{40}{2023}{{Tang et~al.}}{{Tang, Deng, Lin, Han, Liang, Cao, and Sun}}}
\bibcite{ton2024understanding}{{41}{2024}{{Ton et~al.}}{{Ton, Taufiq, and Liu}}}
\bibcite{velickovic_neural_2021}{{42}{2021}{{Veličković \& Blundell}}{{Veličković and Blundell}}}
\bibcite{wei2022chain}{{43}{2022}{{Wei et~al.}}{{Wei, Wang, Schuurmans, Bosma, Xia, Chi, Le, Zhou, et~al.}}}
\bibcite{xie2021explanation}{{44}{2021}{{Xie et~al.}}{{Xie, Raghunathan, Liang, and Ma}}}
\bibcite{yan_neural_2020}{{45}{2020}{{Yan et~al.}}{{Yan, Swersky, Koutra, Ranganathan, and Hashemi}}}
\bibcite{zelikman_star_2022}{{46}{2022}{{Zelikman et~al.}}{{Zelikman, Wu, Mu, and Goodman}}}
\bibcite{zhang2024trained}{{47}{2024}{{Zhang et~al.}}{{Zhang, Frei, and Bartlett}}}
\bibstyle{icml2025}
\newlabel{app:cor1_proof}{{A}{11}{}{appendix.A}{}}
\newlabel{app:cor1_proof@cref}{{[appendix][1][2147483647]A}{[1][11][]11}}
\gdef \@abspage@last{11}
