\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{wei2022chain}
\citation{gao2023pal}
\citation{lyu2023faithful,pan2023logic}
\citation{merrill2023expressive}
\newlabel{intro}{{1}{1}{}{section.1}{}}
\newlabel{intro@cref}{{[section][1][]1}{[1][1][]1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:three_arms}{{1}{1}{Bayesian Inference model showing the \emph {three arms methodology} in \cref {exp:0}. Given an algorithmic problem, we may split it into two steps (1) Translate (2) Execute. The (1) translation $\in \{ \mathrm {Code}, \mathrm {NL }\}$. Then (2) execution $\in \{ \mathrm {LLM \ Reasoning}, \mathrm {Solver \ Execution}\}$. We have three pairs, Arm 1: $\{\mathrm {NL \ Gen},\mathrm {LLM \ Reasoning}\}$, Arm 2: $\{\mathrm {Code \ Gen}, \mathrm {LLM \ Reasoning}\}$, Arm 3: $\{\mathrm {Code \ Gen} , \mathrm {Solver \ Execution}\}$. Typically, the problem is tackled by comparing Arm 1 and Arm 3 in neuro-symbolic literature, which is intractable theoretically, and uncontrolled since multiple variables are changing. By introducing Arm 2, the problem becomes tractable. In the diagram, the \emph {shaded} circles correspond to observed R.V. and \emph {white} correspond to unobserved. The notation for R.V.s is correspondingly used in \cref {method}}{figure.caption.1}{}}
\newlabel{fig:three_arms@cref}{{[figure][1][]1}{[1][1][]1}}
\citation{xie2021explanation}
\citation{lyu2023faithful,pan2023logic}
\newlabel{fig:arm_prompts}{{2}{2}{Prompt templates for the three-arm evaluation framework. Arm 1 instructs the model to reason purely in natural language without code. Arm 2 instructs the model to generate code and simulate its execution. Arm 3 uses the same code generation prompt but executes the output in a Python runtime rather than simulating}{figure.caption.2}{}}
\newlabel{fig:arm_prompts@cref}{{[figure][2][]2}{[1][1][]2}}
\newlabel{method}{{2}{3}{}{section.2}{}}
\newlabel{method@cref}{{[section][2][]2}{[1][3][]3}}
\newlabel{fig:line}{{3}{3}{Average accuracy of different models and tasks (Y axis) across different arms (X axis). Code execution (Arm 3) is better than Code simulation (Arm 2) which is better than natural language reasoning (Arm 1) on CLRS$_{30}$ (n=500), NPHardEval (n=270), Fine-Grained evaluation (n=270) benchmarks across 3 seeds. Statistical significance measured by Wilcoxon Signed Rank test between adjacent branches. Analysis uses model-level Boostrappedd Wilson CI}{figure.caption.3}{}}
\newlabel{fig:line@cref}{{[figure][3][]3}{[1][3][]3}}
\newlabel{fig:main}{{4}{4}{Different fine-grained tasks as problems get harder}{figure.caption.4}{}}
\newlabel{fig:main@cref}{{[figure][4][]4}{[1][3][]4}}
\citation{graves_neural_2014,velickovic_neural_2021,reed_neural_2016,graves_hybrid_2016}
\citation{schneider_controlled_2003,risko_cognitive_2016,anderson_neural_2010}
\citation{kolter_hierarchical_2007,dietterich_hierarchical_2000}
\citation{hudson_compositional_2018,hupkes_compositionality_2020,andreas_neural_2017,poggio2017and}
\citation{velickovic_neural_2021,mahdavi_towards_2023,ibarz_generalist_2022,yan_neural_2020}
\newlabel{fig:recovery_final}{{5}{5}{Recovery rate analysis: proportion of cases where LLM simulation (Arm 2) produces the correct answer despite incorrect code (Arm 3 failure). Recovery rates remain below 5\% across all task families and models, confirming that solver execution dominates LLM simulation}{figure.caption.5}{}}
\newlabel{fig:recovery_final@cref}{{[figure][5][]5}{[1][5][]5}}
\citation{marra_integrating_2019,olausson_linc_2023,han_folio_2024}
\citation{altabaa2025cot,zelikman_star_2022,merrill_expressive_2024,altabaa_cot_2025}
\citation{xie2021explanation,garg2022can,akyurek2022learning,zhang2024trained}
\citation{xie2021explanation}
\citation{shen_llm_2024,schick_toolformer_nodate,qin_toolllm_2023,tang_toolalpaca_2023,parisi_talm_2022}
\citation{shin_few-shot_2022,krishnamurthy_neural_2017,berant_semantic_2013,dong_language_2016}
\citation{puri_codenet_2021,alon_code2vec_2019,chen_neural_2018}
\bibdata{example_paper}
\bibcite{akyurek2022learning}{{1}{2022}{{Aky{\"u}rek et~al.}}{{Aky{\"u}rek, Schuurmans, Andreas, Ma, and Zhou}}}
\bibcite{alon_code2vec_2019}{{2}{2019}{{Alon et~al.}}{{Alon, Zilberstein, Levy, and Yahav}}}
\bibcite{altabaa2025cot}{{3}{2025{a}}{{Altabaa et~al.}}{{Altabaa, Montasser, and Lafferty}}}
\bibcite{altabaa_cot_2025}{{4}{2025{b}}{{Altabaa et~al.}}{{Altabaa, Montasser, and Lafferty}}}
\bibcite{anderson_neural_2010}{{5}{2010}{{Anderson}}{{}}}
\bibcite{andreas_neural_2017}{{6}{2017}{{Andreas et~al.}}{{Andreas, Rohrbach, Darrell, and Klein}}}
\bibcite{berant_semantic_2013}{{7}{2013}{{Berant et~al.}}{{Berant, Chou, Frostig, and Liang}}}
\bibcite{chen_neural_2018}{{8}{2018}{{Chen \& Zhou}}{{Chen and Zhou}}}
\bibcite{dietterich_hierarchical_2000}{{9}{2000}{{Dietterich}}{{}}}
\bibcite{dong_language_2016}{{10}{2016}{{Dong \& Lapata}}{{Dong and Lapata}}}
\bibcite{gao2023pal}{{11}{2023}{{Gao et~al.}}{{Gao, Madaan, Zhou, Alon, Liu, Yang, Callan, and Neubig}}}
\bibcite{garg2022can}{{12}{2022}{{Garg et~al.}}{{Garg, Tsipras, Liang, and Valiant}}}
\bibcite{graves_neural_2014}{{13}{2014}{{Graves et~al.}}{{Graves, Wayne, and Danihelka}}}
\bibcite{graves_hybrid_2016}{{14}{2016}{{Graves et~al.}}{{Graves, Wayne, Reynolds, Harley, Danihelka, Grabska-Barwińska, Colmenarejo, Grefenstette, Ramalho, Agapiou, Badia, Hermann, Zwols, Ostrovski, Cain, King, Summerfield, Blunsom, Kavukcuoglu, and Hassabis}}}
\bibcite{han_folio_2024}{{15}{2024}{{Han et~al.}}{{Han, Schoelkopf, Zhao, Qi, Riddell, Zhou, Coady, Peng, Qiao, Benson, Sun, Wardle-Solano, Szabo, Zubova, Burtell, Fan, Liu, Wong, Sailor, Ni, Nan, Kasai, Yu, Zhang, Fabbri, Kryscinski, Yavuz, Liu, Lin, Joty, Zhou, Xiong, Ying, Cohan, and Radev}}}
\bibcite{hudson_compositional_2018}{{16}{2018}{{Hudson \& Manning}}{{Hudson and Manning}}}
\bibcite{hupkes_compositionality_2020}{{17}{2020}{{Hupkes et~al.}}{{Hupkes, Dankers, Mul, and Bruni}}}
\bibcite{ibarz_generalist_2022}{{18}{2022}{{Ibarz et~al.}}{{Ibarz, Kurin, Papamakarios, Nikiforou, Bennani, Csordás, Dudzik, Bošnjak, Vitvitskyi, Rubanova, Deac, Bevilacqua, Ganin, Blundell, and Veličković}}}
\bibcite{kolter_hierarchical_2007}{{19}{2007}{{Kolter et~al.}}{{Kolter, Abbeel, and Ng}}}
\bibcite{krishnamurthy_neural_2017}{{20}{2017}{{Krishnamurthy et~al.}}{{Krishnamurthy, Dasigi, and Gardner}}}
\bibcite{lyu2023faithful}{{21}{2023}{{Lyu et~al.}}{{Lyu, Havaldar, Stein, Zhang, Rao, Wong, Apidianaki, and Callison-Burch}}}
\bibcite{mahdavi_towards_2023}{{22}{2023}{{Mahdavi et~al.}}{{Mahdavi, Swersky, Kipf, Hashemi, Thrampoulidis, and Liao}}}
\bibcite{marra_integrating_2019}{{23}{2019}{{Marra et~al.}}{{Marra, Giannini, Diligenti, and Gori}}}
\bibcite{merrill2023expressive}{{24}{2023}{{Merrill \& Sabharwal}}{{Merrill and Sabharwal}}}
\bibcite{merrill_expressive_2024}{{25}{2024}{{Merrill \& Sabharwal}}{{Merrill and Sabharwal}}}
\bibcite{olausson_linc_2023}{{26}{2023}{{Olausson et~al.}}{{Olausson, Gu, Lipkin, Zhang, Solar-Lezama, Tenenbaum, and Levy}}}
\bibcite{pan2023logic}{{27}{2023}{{Pan et~al.}}{{Pan, Albalak, Wang, and Wang}}}
\bibcite{parisi_talm_2022}{{28}{2022}{{Parisi et~al.}}{{Parisi, Zhao, and Fiedel}}}
\bibcite{poggio2017and}{{29}{2017}{{Poggio et~al.}}{{Poggio, Mhaskar, Rosasco, Miranda, and Liao}}}
\bibcite{puri_codenet_2021}{{30}{2021}{{Puri et~al.}}{{Puri, Kung, Janssen, Zhang, Domeniconi, Zolotov, Dolby, Chen, Choudhury, Decker, Thost, Buratti, Pujar, Ramji, Finkler, Malaika, and Reiss}}}
\bibcite{qin_toolllm_2023}{{31}{2023}{{Qin et~al.}}{{Qin, Liang, Ye, Zhu, Yan, Lu, Lin, Cong, Tang, Qian, Zhao, Hong, Tian, Xie, Zhou, Gerstein, Li, Liu, and Sun}}}
\bibcite{reed_neural_2016}{{32}{2016}{{Reed \& Freitas}}{{Reed and Freitas}}}
\bibcite{risko_cognitive_2016}{{33}{2016}{{Risko \& Gilbert}}{{Risko and Gilbert}}}
\bibcite{schick_toolformer_nodate}{{34}{}{{Schick et~al.}}{{Schick, Dessì, Raileanu, Lomeli, Zettlemoyer, Cancedda, and Scialom}}}
\bibcite{schneider_controlled_2003}{{35}{2003}{{Schneider \& Chein}}{{Schneider and Chein}}}
\bibcite{shen_llm_2024}{{36}{2024}{{Shen}}{{}}}
\bibcite{shin_few-shot_2022}{{37}{2022}{{Shin \& Durme}}{{Shin and Durme}}}
\bibcite{tang_toolalpaca_2023}{{38}{2023}{{Tang et~al.}}{{Tang, Deng, Lin, Han, Liang, Cao, and Sun}}}
\bibcite{velickovic_neural_2021}{{39}{2021}{{Veličković \& Blundell}}{{Veličković and Blundell}}}
\bibcite{wei2022chain}{{40}{2022}{{Wei et~al.}}{{Wei, Wang, Schuurmans, Bosma, Xia, Chi, Le, Zhou, et~al.}}}
\bibcite{xie2021explanation}{{41}{2021}{{Xie et~al.}}{{Xie, Raghunathan, Liang, and Ma}}}
\bibcite{yan_neural_2020}{{42}{2020}{{Yan et~al.}}{{Yan, Swersky, Koutra, Ranganathan, and Hashemi}}}
\bibcite{zelikman_star_2022}{{43}{2022}{{Zelikman et~al.}}{{Zelikman, Wu, Mu, and Goodman}}}
\bibcite{zhang2024trained}{{44}{2024}{{Zhang et~al.}}{{Zhang, Frei, and Bartlett}}}
\bibstyle{icml2025}
\gdef \@abspage@last{9}
