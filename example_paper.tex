%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{url}
\usepackage[most]{tcolorbox}
\usepackage{wrapfig}
\usepackage{enumitem} %for leftmargin
\usepackage{tabularx} % For adjustable-width columns
\usepackage{caption}
\usepackage{longtable}
\usepackage{tabularx}
\usepackage{ltablex}
\usepackage{multirow}
\usepackage{mathtools}
\usepackage[table]{xcolor}
\usepackage{makecell}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{bbm}
\usepackage{amsmath,amsthm,amssymb}
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such (already loaded above)
% \usepackage{cleveref}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}{Lemma}[theorem]
\newtheorem{corollary}{Corollary}[theorem]

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Neuro-Symbolic Language Models for Scaling Algorithmic Reasoning}

\newcommand{\mc}{\mathcal}
\newcommand{\mb}{\mathbb}
\newcommand{\e}{\epsilon}
\newcommand{\g}{\Gamma}
\newcommand{\del}{\delta}
\newcommand{\li}[1]{\lim\limits_{#1 \to \infty}} %limit to infinity
\newcommand{\lz}[1]{\lim\limits_{#1 \to 0}} %limit to zero
\newcommand{\s}[2]{\sum\limits_{#1=1}^{#2}} %sum
\newcommand{\p}[2]{\prod\limits_{#1=1}^{#2}} %sum

\newcommand{\Hc}{\mathcal{H}} %hypothesis class
\newcommand{\N}{\mathbb{N}_+} %positive naturals
\newcommand{\Rd}{\mathbb{R}} %real numbers domain


\newcommand{\Sig}{|\Sigma|} %alphabet

\begin{document}

\twocolumn[
\icmltitle{Neuro-symbolic language models for scaling algorithmic reasoning}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Terry Tong}{yyy}
\icmlauthor{Yu Feng}{yyy}
\icmlauthor{Surbhi Goel}{yyy}
\icmlauthor{Dan Roth}{yyy}
% \icmlauthor{Firstname5 Lastname5}{yyy}
% \icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
% \icmlauthor{Firstname7 Lastname7}{comp}
%\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{University of Pennsylvania}
% \icmlaffiliation{comp}{Company Name, Location, Country}
% \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Terry Tong}{tongt1@seas.upenn.edu}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Neuro-symbolic AI, Tool Use, Bayesian Inference, Algorithmic Reasoning, Code Generation}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
\vspace{-2pt}
% Context: Establish the problem domain
Large language models can solve algorithmic problems either through direct natural language reasoning or by generating executable code delegated to an external solver. However, little theoretical progress has been made on explaining \emph{why} code-based approaches consistently outperform natural language reasoning.

% Gap + Contribution: What's missing and what we provide
We introduce a three-arm framework that makes this comparison tractable by introducing an intermediary step---code generation with LLM-based execution---enabling pairwise theoretical analysis via Bayesian inference and information theory.

% Results: Concrete findings
Empirically, we demonstrate on arithmetic, dynamic programming, and integer linear programming tasks that code execution achieves 78\% accuracy versus 30\% for code simulation and 21\% for natural language reasoning across Deepseek and Gemma models ($p < 0.05$, Friedman test). Theoretically, we prove that code representations yield higher mutual information with the target algorithm, leading to at least 6\% lower Bayes error than natural language.

% Impact: Why it matters
These results inform the design of compositional AI systems, providing principled guidance on when to use tool-augmented versus monolithic reasoning for algorithmic tasks. 



\end{abstract}

\vspace{-20pt}
\section{Introduction} \label{intro}

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{fig1.png}
    \caption{Bayesian Inference model showing the \emph{three arms methodology} in \cref{exp:0}. Given an algorithmic problem, we may split it into two steps (1) Translate (2) Execute. The (1) translation $\in \{ \mathrm{Code}, \mathrm{NL }\}$. Then (2) execution $\in \{ \mathrm{LLM \ Reasoning}, \mathrm{Solver \ Execution}\}$. We have three pairs, Arm 1: $\{\mathrm{NL \ Gen},\mathrm{LLM \ Reasoning}\}$, Arm 2: $\{\mathrm{Code \ Gen}, \mathrm{LLM \ Reasoning}\}$, Arm 3: $\{\mathrm{Code \ Gen} , \mathrm{Solver \ Execution}\}$. Typically, the problem is tackled by comparing Arm 1 and Arm 3 in neuro-symbolic literature, which is intractable theoretically, and uncontrolled since multiple variables are changing. By introducing Arm 2, the problem becomes tractable. In the diagram, the \emph{shaded} circles correspond to observed R.V. and \emph{white} correspond to unobserved. The notation for R.V.s is correspondingly used in \cref{prelim}.
    }
    \label{fig:three_arms}
    \vspace{-5pt}
\end{figure}

%Introduce the setting and the problem. Make sure to narrow down the scope. 
Consider the algorithmic task of computing arithmetic operations encoded in natural language. We wish to compute:
\vspace{-5pt}
\begin{equation*}
    p(\texttt{Y=3} | \texttt{X=What is one plus two?})
\end{equation*}
\vspace{-5pt}
The language model forward pass could decide (1) to generate the solution directly with natural language reasoning \cite{wei2022chain} (2) translate the problem into code and use a solver \cite{gao2023pal}. This paper provides empirical evidence that $\text{Arm 1} < \text{Arm 3}$ quantified by end-task accuracy. A body of work shows that this pipeline is generally effective \cite{lyu2023faithful, pan2023logic}, further evidenced by the empirical success of tool-use\footnote{Here we primarily refer to solver-based tool-use as opposed to knowledge-intensive tool-use like RAG}. However, little progress has been made on explaining \emph{why} solver-based tools lead to higher end-task accuracy than natural language reasoning.

%Why the problem is hard theoretically. 
One might be tempted to prove a statistical advantage by showing that sample complexity to learn code is less than natural language because code is structured, but will quickly find that this problem becomes intractable due to the hardness of capturing natural language under a mathematical framework. The same problem arises when attempting to use approximation theory to provide evidence that DNNs can better learn compositional or structured languages (like code) than natural language. Broadly speaking, the problem is challenging because the inputs and outputs are different, one is a structured language, the other is an unstructured language. This drastically complicates comparison. Or, one might be tempted to show a computational advantage by showing that code unlocks a new level of expressivity \cite{merrill2023expressive}. However, one will find that whether using a solver or not will not overcome the hardness of the problem. E.g., we can prove by contradiction that using a solver will not allow us to better solve NP-Hard problems, unless P=NP.  

%What we did 
As a solution, we leverage a Bayesian Inference paradigm to reason about the two different settings \cite{xie2021explanation}. Doing so, enables us to break down the algorithmic reasoning pipeline into two distinct phases (1) Translation $\in \{ \mathrm{Code}, \mathrm{NL }\}$  (2) Execution $\in \{ \mathrm{LLM \ Reasoning}, \mathrm{Solver \ Execution}\}$ \cite{lyu2023faithful, pan2023logic}. Enumerating the valid combinations, we obtain three pairs, Arm 1: $\{\mathrm{NL \ Gen},\mathrm{LLM \ Reasoning}\}$, Arm 2: $\{\mathrm{Code \ Gen}, \mathrm{LLM \ Reasoning}\}$, Arm 3: $\{\mathrm{Code \ Gen} , \mathrm{Solver \ Execution}\}$. Introducing Arm 2 makes the problem tractable. 

%Why the problem hasn't been solved empirically. 
Empirically, this framework enables controlled comparisons. A rigorous comparison has not been instantiated because it is hard to control the experiment and determine what representation is actually being used---whether code, natural language, or something else. We overcome this by verbalizing the representation using Chain-of-Thought. We provide statistical evidence for the alternative hypothesis $\text{Arm 3} > \text{Arm 2} > \text{Arm 1}$ on (Gemma, Deepseek, Llama), we demonstrate that generating code and executing leads to 78\% accuracy on (ILP, DP, Arithmetic) tasks, over 30\% for code simulation and just over 20\% for natural language reasoning across 5 models. Our results are computed over 5 seeds, and a Friedman Chi-square gives results a test statistic of 9277.32 and 8369.34 for deepseek and llama, enabling us to reject the null in favor of the alternative. 

Theoretically, we first compare $\text{Arm 1} < \text{Arm 2}$. Bayesian Inference shows us that the LLM implicitly does multi-class classification to the right algorithm. We utilize information theory to capture natural language and code under the same framework, forgoing using grammars or other mathematical frameworks that are intractable. We reduce the comparison of Bayes Error to that of comparing cross-entropy. A intermediate step using mutual information makes the proof interpretable: we prove that the mutual information between the CoT and the final answer is higher when conditioned on code representations over natural language representations. We variationally lower bound the mutual information using cross-entropy of a proposal distribution parameterized by a logistic regression. Since we only care about orderings, we subtract to overcome the intractability of estimating the differential entropy, reducing the comparison of mutual information to that of cross-entropy. The cross-entropy is measured empirically using logistic regression on TF-IDF features and Bert-base-uncased features, showing that code has lower cross-entropy than NL and achieves higher accuracy when classifying the correct algorithm. The difference is statistically significant (F-test, $p < 0.05$). %no need to say the details of reduction and not using LLM uncalibrated probs? 

Then we compare $\text{Arm 2} < \text{Arm 3}$. This difference is easily explained using a communication channel model of LLM forward-pass. We show that in the case where Arm 2 > Arm 3, i.e. when the code generated is not executable or wrong, yet the LLM reasoning obtains the correct answer, that this occurs rarely. In other words, generally $\text{Arm 3} > \text{Arm 2}$. 

Piecing these results together, we show that $\text{Arm 1} < \text{Arm 2} < \text{Arm 3}$, verifying the hypothesis.

% % Claim: Statistical Learning and Computational Learning Hard to deal with our setting. Therefore we need 3 arms. Explain why its hard, and what the gap is in literature. 
%  The reason for this is because comparing structured programming languages and unstructured natural language is challenging. Typically, statistical learning theory is concerned with learning a hypothesis class mapping $\mathcal{X} \to \mathcal{Y}$. We compare two learning algorithms and see how many samples of $\mathcal{X}$ are required to achieve a loss $L$. What if now, the $X$ are different across learning algorithms? This makes comparison between NL (1) and Code (2) \emph{mutually} intractable. 

% % Computational Challenges
% From a computational perspective, we may be curious whether code representations improve expressivity, quantified by less CoT iterations to achieve a certain loss. As it turns out, it is again hard to formalize the comparison, since a fundamental problem is trying to capture natural language in a mathematical structure, e.g. a grammar, which overcomes its ambiguity while maintain expressivity. 

% Solution

% Experimental Results / Phenomenon


% Explanation

Understanding this problem is crucial as we move towards compositional AI systems rather than monolithic architectures.

Our main contributions are:
\begin{enumerate}
    \item A \textbf{three-arm framework} for tractable comparison between code and natural language representations via an intermediary (code generation with LLM execution).
    \item A \textbf{theoretical explanation} based on Bayesian inference showing that code yields higher mutual information with target algorithms, leading to lower Bayes error.
    \item \textbf{Empirical validation} demonstrating that code execution achieves 78\% accuracy versus 21\% for natural language reasoning across arithmetic, DP, and ILP tasks ($p < 0.05$).
\end{enumerate}


\section{Background and Related Work}

\textbf{Neuro-symbolic Learning.} This paper builds on research in neuro-symbolic integration \cite{graves_neural_2014, velickovic_neural_2021, reed_neural_2016, graves_hybrid_2016}, which combines neural networks with symbolic reasoning systems. These approaches are motivated by cognitive science \cite{schneider_controlled_2003, risko_cognitive_2016, anderson_neural_2010}, hierarchical reinforcement learning \cite{kolter_hierarchical_2007, dietterich_hierarchical_2000}, and compositionality research \cite{hudson_compositional_2018, hupkes_compositionality_2020, andreas_neural_2017, poggio2017and}. An orthogonal line of work explores direct execution of algorithms by neural networks \cite{velickovic_neural_2021, mahdavi_towards_2023, ibarz_generalist_2022, yan_neural_2020}. Unlike these approaches that focus on \emph{how} to integrate neural and symbolic components, our work addresses \emph{why} symbolic execution outperforms neural reasoning for algorithmic tasks.

\textbf{LLM Reasoning.} Recent work has explored various reasoning paradigms for LLMs, including symbolic reasoning \cite{marra_integrating_2019, olausson_linc_2023, han_folio_2024}, chain-of-thought prompting \cite{altabaa2025cot, zelikman_star_2022, merrill_expressive_2024, altabaa_cot_2025}, and in-context learning \cite{xie2021explanation, garg2022can, akyurek2022learning, zhang2024trained}. \citet{xie2021explanation} model in-context learning as implicit Bayesian inference, which we extend to compare different reasoning representations. While prior work demonstrates \emph{that} certain prompting strategies improve performance, we provide a theoretical framework explaining \emph{why} code representations lead to lower Bayes error.

\textbf{LLM Tool-Use.} Tool-augmented LLMs have achieved strong empirical results \cite{shen_llm_2024, schick_toolformer_nodate, qin_toolllm_2023, tang_toolalpaca_2023, parisi_talm_2022}. Code generation for tool-use can be viewed as a form of semantic parsing \cite{shin_few-shot_2022, krishnamurthy_neural_2017, berant_semantic_2013, dong_language_2016} or function calling \cite{puri_codenet_2021, alon_code2vec_2019, chen_neural_2018}. Our work complements this literature by providing theoretical justification for the observed empirical advantages of code-based tool-use over direct natural language reasoning. 

\section{Preliminaries and Setup}\label{prelim}

We describe the generative modeling and assumptions of our framework, before proceeding to the proofs. 

% \textbf{Pretraining Distribution} We assume that first a Bernoulli R.V. $D \in \{\mathrm{Concept, Description}\}$ is drawn, determining whether our document segment is descriptive (i.e. a conversation, description of setting), or a program (i.e. arithmetic task, ILP formulation). If $D=\mathrm{Concept}$, we draw a latent concept $\theta$ from a family of concepts $\Theta$. Consider the task of arithmetic, the space of concepts is 


% Regardless of the outcome of $D$ we draw another Bernoulli R.V. $R \in \{ \mathrm{Code, NL}\}$ that represents the representation of the program or description. The tuple (D,R, $\Theta$) specifies a distribution over our vocabulary $O$. This forms our family of document segments. We first sample a tuple (D, $\theta$, R) from a joint prior $p(D, \theta, R)$. Then, sample a document of length $T$ given this tuple. 
% \[
% p(o_1,...,o_T) = \sum_{R,D}\int_{\theta} p(o_1, ..., o_T|\theta, R,D)p(\theta, R,D) d\theta
% \]
% We assume that $p(o_1,...,o_T|\theta,R,D)$ is a Hidden Markov Model (HMM), where ($\theta$,R,D) specifies the transition probabilities of HMM hidden state $h_1,...,h_n \in \mc H$.

% \textbf{Prompt Distribution} Suppose we wanted to induce a $\mathrm{Repr} \in \{\mathrm {Code}, \mathrm{NL}\}$ representation chain of thought. Our prompts are natural language encodings of the problems $\theta^*$, represented by segments sampled independently from the conditional HMM distribution $X \sim P(o_{1:{T-1}}| \theta^*,R=\mathrm{NL}, D=\mathrm{Program})$, $Y \sim P(o_{T}| \theta^*, D=\mathrm{Program})$. Y is the gold answer to predict, and we assume that $Y = \mathrm{eval}(\theta^*$), (e.g. eval is a python interpreter).



\textbf{Task.}
On a high level, we aim to show that Bayes error for code is less than for natural language: $p(\hat y \neq y^* | x, \mathrm{Code}) < p(\hat y \neq y^* | x, \mathrm{NL})$. 


\textbf{Modeling } To do this, we model LLM forward pass as posterior inference, a paradigm similar to implicit Bayesian inference in in-context learning by \citet{xie2021explanation}. 

Let $y,x$ be the inputs and outputs respectively. $x$ is defined as a natural language encoding of some algorithmic problem. Let the random variable $R \in \{\mathrm{Code}, \mathrm{NL}\}$ be the chain of thought representation. The random variable $\gamma$ represents the latent algorithm and the world variables. The latent algorithms are a piece of code (as opposed to a string describing the algorithm), whereas the world variables are inputs the algorithm takes in. Knowing $\gamma$ means we have all the information we need to solve the problem, and thus $\gamma$ is a sufficient statistic for $y$. For example, $\gamma = \{ \mathrm{addition}, 2,4\}$. We first assume that we perform this multi-class classification to the right algorithm implicitly through inferring $\gamma$, then condition on this to produce the right answer. 

\begin{align}
     & p(y|x,r_{i}) =  \int_{\gamma \in \Gamma}  p(y| x,\gamma, r_i)p(\gamma | x, r_i)  d\gamma
\end{align}

In the context of reasoning models, we suppose that the chain-of-thought helps to infer $\gamma$. 

\begin{align}
     & p(y|x,r_{i}) =  \int_{\gamma \in \Gamma} \int_z p(y| x,\gamma, r_i)p(\gamma |z, x, r_i) p(z|x,r_i) d\gamma
\end{align}



\section{Experiment: Three Arms Comparison}\label{exp:0}

This section presents our primary empirical evaluation comparing the three reasoning arms: (1) natural language generation with LLM reasoning, (2) code generation with LLM simulation, and (3) code generation with solver execution.

\subsection{Hypothesis}

We hypothesize that the three arms exhibit a strict ordering in terms of end-task accuracy:
\begin{equation}
    \text{Acc}(\text{Arm 1}) < \text{Acc}(\text{Arm 2}) < \text{Acc}(\text{Arm 3})
\end{equation}
This hypothesis is grounded in our theoretical framework (\cref{prelim}): code representations should yield higher mutual information with target algorithms than natural language, and deterministic solver execution should outperform stochastic LLM simulation.

\subsection{Evaluation Methodology}

\textbf{Tasks.} We evaluate on three families of algorithmic problems spanning different computational paradigms:
\begin{itemize}[leftmargin=*]
    \item \textbf{Arithmetic}: Addition, subtraction, and multiplication of multi-digit integers.
    \item \textbf{Dynamic Programming (DP)}: Rod-cutting, longest common subsequence (LCS), and 0-1 knapsack problems.
    \item \textbf{Integer Linear Programming (ILP)}: Production planning, partitioning, and assignment problems.
\end{itemize}

\textbf{Difficulty Scaling.} Each task is parameterized by a hardness parameter $\tau$ that controls problem complexity. For arithmetic, $\tau$ specifies the number of digits (e.g., $\tau=2$ denotes two-digit operands sampled uniformly). For DP problems, $\tau$ determines the table dimensions ($\tau \times \tau$). For ILP, $\tau$ specifies the number of constraints.

\textbf{Models.} We evaluate two instruction-tuned models: Deepseek-Coder and Gemma-2. These models represent different architectural choices and training distributions while both supporting code generation and natural language reasoning.

\textbf{Experimental Protocol.} For each model-task combination, we generate 2,000 samples per seed across 5 random seeds (seeds $\in \{0,1,2,3,4\}$), yielding 10,000 samples per condition. All arms receive equivalent one-shot prompts to ensure fair comparison. For Arm 3, since solver execution does not condition on the prompt during execution, we include an additional control where Arm 2 also excludes the prompt during LLM simulation.

\textbf{Metrics.} We report accuracy as the primary metric, computed as the proportion of samples where the generated answer matches the ground truth.

\subsection{Results}

\cref{fig:main} presents the aggregate accuracy across all tasks and models. Code execution (Arm 3) achieves 78\% mean accuracy, substantially outperforming code simulation (Arm 2) at 30\% and natural language reasoning (Arm 1) at 21\%. This ordering holds consistently across task families and models.


\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{main.png}
    \caption{Accuracy comparison across the three reasoning arms for Deepseek-Coder and Gemma-2 models. Code execution (Arm 3) consistently outperforms code simulation (Arm 2) and natural language reasoning (Arm 1) across all task families: arithmetic, dynamic programming, and integer linear programming.}
    \label{fig:main}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{line.png}
    \caption{Accuracy as a function of problem difficulty (hardness parameter $\tau$). As task complexity increases, the performance gap between arms widens, with code execution maintaining higher accuracy at greater difficulty levels.}
    \label{fig:line}
\end{figure}

\subsection{Statistical Analysis}

To validate our hypothesis, we employ non-parametric statistical tests that make no assumptions about the underlying data distribution.

\textbf{Friedman Test.} We apply the Friedman chi-square test to assess whether the three arms differ significantly in their accuracy distributions. The null hypothesis posits equal median accuracy across all arms. For Deepseek-Coder, we obtain a test statistic of $\chi^2 = 9277.32$; for Gemma-2, $\chi^2 = 8369.34$. Both yield $p < 10^{-10}$ (numerical underflow), providing strong evidence to reject the null hypothesis.

\textbf{Pairwise Comparisons.} To establish the ordering between arms, we conduct pairwise McNemar tests on contingency tables of correct/incorrect predictions. \cref{fig:pval} shows the resulting $p$-values for all pairwise comparisons. The differences between Arm 1 vs. Arm 2 ($p = 9.38 \times 10^{-21}$), Arm 2 vs. Arm 3 ($p < 10^{-50}$), and Arm 1 vs. Arm 3 ($p < 10^{-50}$) are all statistically significant at $\alpha = 0.05$.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{pval.png}
    \caption{Pairwise $p$-values from McNemar tests comparing accuracy between arms. All comparisons are statistically significant ($p < 0.05$), confirming the ordering Arm 1 $<$ Arm 2 $<$ Arm 3. We use non-parametric tests (Kruskal-Wallis for overall comparison, McNemar for pairwise) to avoid distributional assumptions.}
    \label{fig:pval}
\end{figure}

\subsection{Analysis and Interpretation}

\textbf{Finding 1: Execution is the Primary Bottleneck.} The large gap between Arm 2 (30\%) and Arm 3 (78\%) indicates that the execution phase---not the translation phase---is the primary source of errors. When the LLM generates correct code, the solver executes it perfectly. However, when the LLM must simulate code execution, it frequently produces incorrect results even from correct code.

\textbf{Finding 2: Code Outperforms Natural Language as a Reasoning Representation.} The consistent advantage of Arm 2 over Arm 1 (30\% vs. 21\%) demonstrates that code serves as a superior intermediate representation for algorithmic reasoning, even when both are executed by the same LLM. This supports our theoretical prediction that code yields higher mutual information with the target algorithm.

\textbf{Finding 3: The Gap Widens with Task Difficulty.} As shown in \cref{fig:line}, the performance differential between arms increases with the hardness parameter $\tau$. For simple problems ($\tau \leq 2$), all three arms achieve reasonable accuracy. However, as difficulty scales, Arm 3 degrades gracefully while Arms 1 and 2 deteriorate rapidly. This suggests that symbolic execution provides robustness benefits that become increasingly important for complex algorithmic tasks.

\section{A Bayesian Explanation}

In this section we explain why the code representations $R = \mathrm{Code}$ has lower Bayes error rate $P^*$ than natural language representations $R = \mathrm{NL}$. This amounts to proving that $\text{Arm 3} > \text{Arm 1}$.

% Why is it mutually intractable. 
Directly showing $\text{Arm 3} > \text{Arm 1}$ is mutually intractable, but by introducing an intermediate step in Arm 2, the comparison between Arm 1 and Arm 2 is tractable, and Arm 2 and Arm 3 are tractable. Bayesian inference captures the breakdown nicely via marginalization. Furthermore, by moving into the realm of probabilistic inference, we can introduce information theory \cite{ton2024understanding, altabaa2025cot} as a unifying framework, sidestepping the intractability of mathematical frameworks attempting to capture natural language, e.g. CFG, CCGs. We know that parsing natural language to one of these frameworks optimally is NP-complete.


This section explains a theory that sets up the experiments later that follow. First, we show that $\text{Arm 1} < \text{Arm 2}$, then $\text{Arm 2} < \text{Arm 3}$

\subsection{Arm 1 $<$ Arm 2}

We being by defining a few lemmas, proving them and showing why they are necessary. 

\begin{lemma} \label{lem:1}
    For markov chain $Z_r \to \gamma \to Y$, the following equality holds:
    $\mc I(Y, Z_{\mathrm{Code}}) - \mc I(Y, Z_{\mathrm{NL}}) = \mc I(\gamma, Z_{\mathrm{Code}}) - \mc I(\gamma, Z_{\mathrm{NL}})$. 
\end{lemma}

\begin{proof}
    $ [\mc I(\gamma, Z_{\mathrm{Code}}) + \mc I(Y, \gamma) - H(\gamma)] - [\mc I(\gamma, Z_{\mathrm{NL}}) + \mc I(Y, \gamma) - H(\gamma)] $
    \[\underbrace{\leq}_{\text{Chain Rule}} \mc I (Y, Z_{\mathrm{Code}}) - \mc I (Y, Z_{\mathrm{NL}})\] \[\underbrace{\leq}_{DPI} \mc I(\gamma, Z_{\mathrm{Code}}) - I(\gamma, Z_{\mathrm{NL}})\]
    $ \implies \mc I(\gamma, Z_{\mathrm{Code}}) - \mc I(\gamma, Z_{\mathrm{NL}})$ 
    \[\underbrace{\leq}_{\text{Chain Rule}} \mc I (Y, Z_{\mathrm{Code}}) - \mc I (Y, Z_{\mathrm{NL}})\] \[ \underbrace{\leq}_{DPI} \mc I(\gamma, Z_{\mathrm{Code}}) - I(\gamma, Z_{\mathrm{NL}})\]
    $ \implies \mc I (Y, Z_{\mathrm{Code}}) - \mc I (Y, Z_{\mathrm{NL}})$
    \[= \mc I(\gamma, Z_{\mathrm{Code}}) - I(\gamma, Z_{\mathrm{NL}})\]
\end{proof}

Here we show that the mutual information between Y and Z is the same as the mutual information between $\gamma$ and Z. This is useful because we don't need to use the conditional distribution $P(y|Z)$ which is generally uncalibrated for LLMs, and hard to obtain, since LLMs may generate prose and not just the final answer. We may instead work with $P(\gamma|Z)$, which can later be estimated variationally by a proposal distribution. 

\begin{lemma} [Variational Lower Bound] \label{lem:2}
    $\mc I( \gamma, Z_{r}) \geq H(\gamma) + \max\limits_\theta \mb E_{\gamma,z} q(\gamma| z,\theta)$ where $\theta$ specifies the model drawn from our parametric family, and $q$ is some valid probability distribution \citep{poole2019variational}. 
\end{lemma} 

We can lower bound the mutual information with the cross entropy, which we use in experiments \cref{exp:1}. We apply this lemma in \cref{cor:1}.

% \begin{proof}
%     Start with the definition of Mutual Information. 
%     \begin{align}
%         & \mc I ( \gamma ,Z) = H(\gamma) - \mb E_Z H(\gamma|Z=z) \\
%         & = H(\gamma) + \mb E_Z \mb E_{\gamma | Z} \log p(\gamma | Z=z) \\ %&& \text{Since } H(\gamma|Z) = - \sum_Z p(\gamma |Z) \log p(\gamma|Z) \\
%         & = H(\gamma) + \mb E_{\gamma, Z} \log \frac{p(\gamma|Z)q(\gamma |Z)}{q(\gamma|Z)} \\ %&& \text{Variational Trick} \\
%         & = H(\gamma) + \mb E_{\gamma,Z} \log q(\gamma  | Z) + \mb E_{\gamma,Z} \frac{p(\gamma|Z)}{q(\gamma|Z)} \\
%         &= H(\gamma) +\mb E_{\gamma,Z} \log q(\gamma,Z) + KL(q(\gamma|Z) || p(\gamma|Z)) \\
%         & \geq H(\gamma) + \mb E_{\gamma,Z} \log q(\gamma|Z)
%     \end{align}
%     Therefore, to get the tightest bound we can write, 
%     $  \mc I ( \gamma ,Z)\geq H(\gamma) + \max\limits_\theta \mb E_{\gamma,Z} \log q(\gamma|Z, \theta)$
% \end{proof}

\begin{theorem} \label{thm:1}
    For Markov Chain $Z_r \to \gamma \to Y$, higher mutual information $\mc I(\gamma, Z_{\mathrm{Code}}) > \mc I(\gamma, Z_{\mathrm{NL}})$ implies Bayes Error \[ P^*_{\mathrm{Code}} =\min_{\hat Y(\cdot) } P[ \hat Y(Z_{\mathrm{Code}}) = Y^*] \] \[ < \min_{\hat Y(\cdot) }  Pr[\hat Y(Z_{\mathrm{NL}}) = Y^*] = P^*_{\mathrm{NL}}\] 
\end{theorem}

%% Cross entropy estimate. 

%% Train a logistic regression / neural network to do this? 

%% Deciding when to use tools, constrained generation? 

\begin{proof}
    Start with Fano's Inequality, 
    \begin{align}
             & H(Y|Z_r) \leq h(P^*_r) + P^*_r \log_2(K-1) \\
             & = H(Y) - \mc I(Y, Z_r) \leq h(P^*_r) + P^*_r \log_2(K-1) \\
             & =  \frac{H(Y) - \mc I(Y, Z_r) -  h(P^*_r)}{\log_2(K-1)} \leq P^*_r \\
             & =  \frac{H(Y) - \mc I(Y, Z_r) - 1}{\log_2(K-1)} \leq P^*_r  % (h(P^*_r) \leq 1)
    \end{align}

    Take the difference of $P^*_{\mathrm{NL}} - P^*_{\mathrm{Code}}   \geq  \frac{ \mc I(Y, Z_{\mathrm{Code}}) - \mc I(Y, Z_{\mathrm{NL}}) }{ \log_2 (K-1)} \underbrace{=}_{Lemma \; 1} \frac{ \mc I( \gamma, Z_{\mathrm{Code}}) - \mc I(\gamma, Z_{\mathrm{NL}}) }{ \log_2 (K-1)} $. 
\end{proof}

\begin{corollary} \label{cor:1}
    Let $H_{CE}(r) = -\max_\theta \mb E_{\gamma,z} \log q(\gamma| z,\theta, r)$ denote the minimum cross-entropy achievable for representation $r$. When the cross-entropy for code is lower than for natural language:
    \begin{equation}
        H_{CE}(\mathrm{Code}) < H_{CE}(\mathrm{NL}),
    \end{equation}
    we have that the Bayes error for code is lower:
    \begin{equation}
        P^*_{\mathrm{Code}} < P^*_{\mathrm{NL}}.
    \end{equation}
\end{corollary}

\begin{proof}
    See \cref{app:cor1_proof}.
\end{proof}

\subsection{Arm 2 $<$ Arm 3}
To explain why Arm 2 $<$ Arm 3, we model LLM forward pass as stochastic inference. The translation stage is the same, and the only difference is the execution stage. We model the inference as communication of information about the problem through a channel. The channel is noisy for LLM execution, and deterministic for solver inference. Therefore, the deterministic solver inference has exactly mutual information of 1 between the CoT Z and Y, and is an upper bound to the stochastic inference. 

\section{Experimental Validation of Theory} \label{exp:1}

This section empirically validates the theoretical predictions from \cref{cor:1}. We design experiments to test whether code representations yield higher mutual information with target algorithms, thereby explaining the performance gap observed in \cref{exp:0}.

\subsection{Validating Arm 1 $<$ Arm 2: Mutual Information Estimation}

\textbf{Hypothesis.} According to \cref{cor:1}, if code representations achieve lower cross-entropy when predicting the latent algorithm $\gamma$, then code should yield lower Bayes error. We test this by estimating the cross-entropy for both representations.

\textbf{Methodology.} We estimate cross-entropy using a plug-in estimator based on logistic regression, motivated by recent evidence that linear classifiers emerge during transformer inference \cite{bai2023transformers}. The experimental setup is as follows:

\begin{itemize}[leftmargin=*]
    \item \textbf{Features}: We extract features from chain-of-thought traces using two methods: (1) TF-IDF vectors and (2) BERT embeddings (bert-base-uncased).
    \item \textbf{Labels}: For each CoT, we construct the ground-truth label $\gamma$ by combining the algorithm type and discretized world variables. World variables are binned in increments of 10 to address sparsity, yielding $K = 761$ classes.
    \item \textbf{Model}: Logistic regression trained with SAGA solver for 20 iterations on an 80/20 train/test split.
    \item \textbf{Data}: CoT traces from the Deepseek-Coder experiments in \cref{exp:0}.
\end{itemize}

\textbf{Results.} \cref{fig:vlb} shows that code representations achieve significantly lower cross-entropy and higher classification accuracy than natural language representations for both feature types (F-test, $p < 0.05$). The figure displays the negative log-likelihood (cross-entropy) on the test set, computed as $H_{CE} = -\frac{1}{N}\sum_i \log q(\gamma_i | z_i, \theta^*)$ where $\theta^*$ are the fitted logistic regression parameters. Lower values indicate better discriminability of the underlying algorithm from the CoT representation.

\textbf{Quantifying the Bayes Error Improvement.} From our cross-entropy estimates, we compute lower bounds on mutual information:
\begin{align}
    \mc I(\gamma, Z_{\mathrm{Code}}) &\geq H(\gamma) + L_{\mathrm{Code}} = 4.4952 \\
    \mc I(\gamma, Z_{\mathrm{NL}}) &\geq H(\gamma) + L_{\mathrm{NL}} = 3.8467
\end{align}

Applying \cref{thm:1} with $K = 761$ classes:
\begin{align}
    P^*_{\mathrm{NL}} - P^*_{\mathrm{Code}} &\geq \frac{\mc I(\gamma, Z_{\mathrm{Code}}) - \mc I(\gamma, Z_{\mathrm{NL}})}{\log_2(K-1)} \nonumber \\
    &\geq \frac{4.4952 - 3.8467}{\log_2(760)} = 0.0678
\end{align}

This yields a \textbf{lower bound of 6.78\% improvement} in Bayes error rate when using code over natural language. Notably, our empirical results in \cref{exp:0} show a 9\% improvement (30\% vs. 21\%), which exceeds this theoretical lower bound as expected. 



\subsection{Validating Arm 2 $<$ Arm 3: Recovery Analysis} \label{exp:2}

\textbf{Hypothesis.} Solver execution (Arm 3) should outperform LLM simulation (Arm 2) because the solver is deterministic: given correct code, it always produces the correct answer. The only scenario where Arm 2 could outperform Arm 3 is when the generated code is incorrect, yet the LLM ``recovers'' by reasoning its way to the correct answer despite the flawed code. We hypothesize that such recovery events are rare.

\textbf{Methodology.} Using the data from \cref{exp:0}, we identify cases where:
\begin{enumerate}[leftmargin=*]
    \item Arm 3 produces an incorrect answer (implying incorrect code generation), and
    \item Arm 2 produces the correct answer (implying successful LLM recovery).
\end{enumerate}
We compute the \emph{recovery rate} as the proportion of Arm 3 failures where Arm 2 succeeds.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{vlb.png}
    \caption{Cross-entropy comparison between code and natural language representations. Code achieves significantly lower cross-entropy (higher mutual information bound) than NL for both TF-IDF and BERT features, validating \cref{cor:1}.}
    \label{fig:vlb}
\end{figure}

\textbf{Correlation Analysis.} To validate that mutual information predicts task performance, we examine the relationship between our MI estimates and observed accuracy. For each task-representation pair, we compute: (1) the MI lower bound from the logistic regression cross-entropy, and (2) the empirical accuracy from \cref{exp:0}. We aggregate results across task families (arithmetic, DP, ILP) and representations (code, NL).

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{mi_v_acc.png}
    \caption{Relationship between estimated mutual information and task accuracy. Each point represents a task-representation pair. Higher mutual information (lower cross-entropy) correlates with improved accuracy ($r = 0.73$, $p < 0.01$), supporting the theoretical prediction that code's informational advantage translates to performance gains.}
    \label{fig:mi_v_acc}
\end{figure}

\textbf{Results.} \cref{fig:recovery_final} presents the recovery analysis across all tasks and models. The recovery rate remains consistently low (typically $< 5\%$), indicating that LLM simulation rarely compensates for code generation errors. This confirms that Arm 3's advantage stems from reliable solver execution rather than Arm 2's inability to reason about code.

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{recovery_final.png}
    \caption{Recovery rate analysis: proportion of cases where LLM simulation (Arm 2) produces the correct answer despite incorrect code (Arm 3 failure). Recovery rates remain below 5\% across all task families and models, confirming that solver execution dominates LLM simulation.}
    \label{fig:recovery_final}
\end{figure}

\textbf{Scaling Analysis.} \cref{fig:recovery_powerlaw} examines how recovery rate varies with task difficulty. We observe a power-law decay: as problems become harder, recovery becomes increasingly unlikely. This occurs because complex problems require longer, more intricate reasoning chains where errors compound---the LLM cannot ``guess'' its way to the correct answer when the underlying algorithm is non-trivial.

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{recovery_powerlaw.png}
    \caption{Recovery rate as a function of task complexity (hardness parameter $\tau$). Recovery follows a power-law decay, indicating that the advantage of symbolic execution over LLM simulation grows with problem difficulty.}
    \label{fig:recovery_powerlaw}
\end{figure}

\textbf{Distribution Analysis.} To visualize the full accuracy distributions beyond summary statistics, we compute kernel density estimates (KDE) for each arm. For each model-task-seed combination, we compute the per-sample accuracy, yielding a distribution of accuracies per arm. We apply Gaussian KDE with Scott's bandwidth selection rule to smooth the empirical distributions.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{kde.png}
    \caption{Kernel density estimation of accuracy distributions across the three arms. Arm 3 (code execution) exhibits a mode near 100\% accuracy, while Arms 1 and 2 show broader, lower-accuracy distributions. The bimodal structure of Arm 3 reflects the dichotomy between correct and incorrect code generation---when code is correct, execution is perfect.}
    \label{fig:kde}
\end{figure}

\textbf{Interpretation.} These results validate the channel model from Section 5.2: solver execution acts as a noiseless channel (deterministic mapping from code to output), while LLM simulation acts as a noisy channel that introduces errors even when the ``transmitted'' code is correct. The low recovery rate confirms that this channel noise is the primary driver of Arm 2's inferior performance.

\subsection{Summary}

Combining the results from Sections 6.1 and 6.2, we establish the complete ordering:
\begin{equation}
    \text{Arm 1} < \text{Arm 2} < \text{Arm 3}
\end{equation}
The gap between Arms 1 and 2 is explained by code's higher mutual information with the target algorithm (\cref{cor:1}). The gap between Arms 2 and 3 is explained by the deterministic vs. stochastic execution channel. Together, these provide a principled theoretical account of why code execution outperforms natural language reasoning for algorithmic tasks.

\section{Discussion}

\textbf{Summary.} This paper addresses whether algorithmic problems encoded in natural language should be solved via direct reasoning or by translating to code and executing with a solver. Our three-arm framework demonstrates that code execution consistently outperforms both code simulation and natural language reasoning, with theoretical backing from information theory.

\textbf{Interpretation.} The theoretical analysis reveals that code representations yield higher mutual information with target algorithms than natural language representations. This explains the empirical observation: code serves as a more discriminative intermediate representation for implicit algorithm classification during LLM inference. The Bayesian framework makes this comparison tractable by decomposing the problem into translation and execution phases.

\textbf{Limitations.} Our study focuses on algorithmic tasks (arithmetic, DP, ILP) where ground truth is well-defined. The results may not generalize to open-ended reasoning tasks without clear algorithmic structure. Additionally, our theoretical bounds are asymptotic---the 6\% Bayes error improvement is a lower bound that may not reflect finite-sample performance. Finally, we evaluate on a limited set of models (Deepseek, Gemma); behavior may differ for other architectures.

\textbf{Broader Impact.} These findings have practical implications for AI system design: for algorithmically structured problems, compositional systems with symbolic execution should be preferred over monolithic neural reasoning. This supports the growing trend toward tool-augmented LLMs.

\section{Conclusion}

We introduced a three-arm framework that enables tractable comparison between code and natural language representations for algorithmic reasoning. By modeling LLM inference as Bayesian inference, we proved that code representations yield higher mutual information with target algorithms, leading to lower Bayes error. Empirically, code execution achieves 78\% accuracy compared to 21\% for natural language reasoning across arithmetic, dynamic programming, and integer linear programming tasks.

An interesting direction for future work is understanding \emph{why} code has higher mutual information---whether this emerges from pretraining data distributions or from inherent structural properties of programming languages. Our framework provides a foundation for such investigations.

\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix

\section{Proof of Corollary 1} \label{app:cor1_proof}

\begin{proof}
Let $L_r = \max_\theta \mb E_{\gamma,z} \log q(\gamma| z,\theta, r)$ denote the optimal expected log-likelihood for representation $r$. Note that $H_{CE}(r) = -L_r$.

From \cref{lem:2}, the mutual information is lower bounded by:
\begin{align}
    \mc I(\gamma, Z_r) \geq H(\gamma) + L_r.
\end{align}

Taking the difference between code and NL:
\begin{align}
    &\mc I(\gamma, Z_{\mathrm{Code}}) - \mc I(\gamma, Z_{\mathrm{NL}}) \nonumber \\
    &\quad \geq [H(\gamma) + L_C] - [H(\gamma) + L_{NL}] \nonumber \\
    &\quad = L_C - L_{NL} = H_{CE}(\mathrm{NL}) - H_{CE}(\mathrm{Code}).
\end{align}

When $H_{CE}(\mathrm{Code}) < H_{CE}(\mathrm{NL})$, equivalently $L_C > L_{NL}$, the RHS is positive:
\begin{equation}
    \mc I(\gamma, Z_{\mathrm{Code}}) > \mc I(\gamma, Z_{\mathrm{NL}}).
\end{equation}

Applying \cref{thm:1}:
\begin{align}
    P^*_{\mathrm{NL}} - P^*_{\mathrm{Code}} \geq \frac{\mc I(\gamma, Z_{\mathrm{Code}}) - \mc I(\gamma, Z_{\mathrm{NL}})}{\log_2(K-1)} > 0.
\end{align}

Therefore, $P^*_{\mathrm{Code}} < P^*_{\mathrm{NL}}$.
\end{proof}

\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
