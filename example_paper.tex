%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{url}
\usepackage[most]{tcolorbox}
\usepackage{wrapfig}
\usepackage{enumitem} %for leftmargin
\usepackage{tabularx} % For adjustable-width columns
\usepackage{caption}
\usepackage{longtable}
\usepackage{tabularx}
\usepackage{ltablex}
\usepackage{multirow}
\usepackage{mathtools}
\usepackage[table]{xcolor}
\usepackage{makecell}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{bbm}
\usepackage{amsmath,amsthm,amssymb}
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\newcommand{\authorcomment}[3]{%
  \textcolor{#2}{[\textbf{#1}: #3]}%
}
\newcommand{\yu}[1]{\authorcomment{Yu}{teal}{#1}}


% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such (already loaded above)
% \usepackage{cleveref}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}{Lemma}[theorem]
\newtheorem{corollary}{Corollary}[theorem]

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{An Explanation In Support Of Neuro-Symbolic Language Models for Scaling Algorithmic Reasoning}

\newcommand{\mc}{\mathcal}
\newcommand{\mb}{\mathbb}
\newcommand{\e}{\epsilon}
\newcommand{\g}{\Gamma}
\newcommand{\del}{\delta}
\newcommand{\li}[1]{\lim\limits_{#1 \to \infty}} %limit to infinity
\newcommand{\lz}[1]{\lim\limits_{#1 \to 0}} %limit to zero
\newcommand{\s}[2]{\sum\limits_{#1=1}^{#2}} %sum
\newcommand{\p}[2]{\prod\limits_{#1=1}^{#2}} %sum

\newcommand{\Hc}{\mathcal{H}} %hypothesis class
\newcommand{\N}{\mathbb{N}_+} %positive naturals
\newcommand{\Rd}{\mathbb{R}} %real numbers domain


\newcommand{\Sig}{|\Sigma|} %alphabet

\begin{document}

\twocolumn[
\icmltitle{An Explanation In Support Of Neuro-Symbolic Language Models for Scaling Algorithmic Reasoning}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Terry Tong}{yyy}
\icmlauthor{Yu Feng}{yyy}
\icmlauthor{Surbhi Goel}{yyy}
\icmlauthor{Dan Roth}{yyy}
% \icmlauthor{Firstname5 Lastname5}{yyy}
% \icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
% \icmlauthor{Firstname7 Lastname7}{comp}
%\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{University of Pennsylvania}
% \icmlaffiliation{comp}{Company Name, Location, Country}
% \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Terry Tong}{tongt1@seas.upenn.edu}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Neuro-symbolic AI, Tool Use, Bayesian Inference, Algorithmic Reasoning, Code Generation}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
\vspace{-2pt}
% Context: Establish the problem domain
Large language models can solve algorithmic problems either through direct natural language reasoning or by generating executable code delegated to an external solver. However, little theoretical progress has been made on explaining \emph{why} code-based approaches consistently outperform natural language reasoning.

% Gap + Contribution: What's missing and what we provide
We introduce a three-arm framework that makes this comparison tractable by introducing an intermediary step---code generation with LLM-based execution---enabling pairwise theoretical analysis via Bayesian inference and information theory.

% Results: Concrete findings
Empirically, we demonstrate on arithmetic, dynamic programming, and integer linear programming tasks that code execution achieves 78\% accuracy versus 30\% for code simulation and 21\% for natural language reasoning across Deepseek and Gemma models ($p < 0.05$, Friedman test). Theoretically, we prove that code representations yield higher mutual information with the target algorithm, leading to at least 6\% lower Bayes error than natural language.

% Impact: Why it matters
These results inform the design of compositional AI systems, providing principled guidance on when to use tool-augmented versus monolithic reasoning for algorithmic tasks. Our framework offers a unified perspective on the tool-use versus direct-reasoning tradeoff. 



\end{abstract}

\vspace{-20pt}
\section{Introduction} \label{intro}

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{fig1(2).png}
    \caption{Bayesian Inference model showing the \emph{three arms methodology} in \cref{exp:0}. Given an algorithmic problem, we may split it into two steps (1) Translate (2) Execute. The (1) translation $\in \{\mathrm{Code}, \mathrm{NL}\}$. Then (2) execution $\in \{\mathrm{LLM~Reasoning}, \mathrm{Solver~Execution}\}$. We have three pairs, Arm~1: $\{\mathrm{NL~Gen}, \mathrm{LLM~Reasoning}\}$, Arm~2: $\{\mathrm{Code~Gen}, \mathrm{LLM~Reasoning}\}$, Arm~3: $\{\mathrm{Code~Gen}, \mathrm{Solver~Execution}\}$. Typically, the problem is tackled by comparing Arm~1 and Arm~3 in neuro-symbolic literature, which is intractable theoretically, and uncontrolled since multiple variables are changing. By introducing Arm~2, the problem becomes tractable. In the diagram, the \emph{shaded} circles correspond to observed R.V.\ and \emph{white} correspond to unobserved. The notation for R.V.s is correspondingly used in \cref{method}.
    }
    \label{fig:three_arms}
    \vspace{-5pt}
\end{figure}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{arm_prompts.png}
    \caption{Prompt templates for the three-arm evaluation framework. Arm~1 instructs the model to reason purely in natural language without code. Arm~2 instructs the model to generate code and simulate its execution. Arm~3 uses the same code generation prompt but executes the output in a Python runtime rather than simulating.}
    \label{fig:arm_prompts}
\end{figure*}

%Introduce the setting and the problem. Make sure to narrow down the scope. 
Consider the algorithmic task of computing arithmetic operations encoded in natural language. We wish to compute:
\vspace{-5pt}
\begin{equation*}
    p(\texttt{Y=3} | \texttt{X=What is one plus two?})
\end{equation*}
\vspace{-5pt}
The language model forward pass could decide (1) to generate the solution directly with natural language reasoning \cite{wei2022chain} (2) translate the problem into code and use a solver \cite{gao2023pal}. This paper provides empirical evidence that $\mathrm{Arm~1} < \mathrm{Arm~3}$ quantified by end-task accuracy. A body of work shows that this pipeline is generally effective \cite{lyu2023faithful, pan2023logic}, further evidenced by the empirical success of tool-use\footnote{Here we primarily refer to solver-based tool-use as opposed to knowledge-intensive tool-use like RAG}. However, little progress has been made on explaining \emph{why} solver-based tools lead to higher end-task accuracy than natural language reasoning.

%Why the problem is hard theoretically. 
One might be tempted to prove a statistical advantage by showing that sample complexity to learn code is less than natural language because code is structured, but will quickly find that this problem becomes intractable due to the hardness of capturing natural language under a mathematical framework. The same problem arises when attempting to use approximation theory to provide evidence that DNNs can better learn compositional or structured languages (like code) than natural language. Broadly speaking, the problem is challenging because the inputs and outputs are different, one is a structured language, the other is an unstructured language. This drastically complicates comparison. Or, one might be tempted to show a computational advantage by showing that code unlocks a new level of expressivity \cite{merrill2023expressive}. However, one will find that whether using a solver or not will not overcome the hardness of the problem. E.g., we can prove by contradiction that using a solver will not allow us to better solve NP-Hard problems, unless P=NP.  

%What we did 
As a solution, we leverage a Bayesian Inference paradigm to reason about the two different settings \cite{xie2021explanation}. Doing so, enables us to break down the algorithmic reasoning pipeline into two distinct phases (1) Translation $\in \{\mathrm{Code}, \mathrm{NL}\}$  (2) Execution $\in \{\mathrm{LLM~Reasoning}, \mathrm{Solver~Execution}\}$ \cite{lyu2023faithful, pan2023logic}. Enumerating the valid combinations, we obtain three pairs, Arm~1: $\{\mathrm{NL~Gen}, \mathrm{LLM~Reasoning}\}$, Arm~2: $\{\mathrm{Code~Gen}, \mathrm{LLM~Reasoning}\}$, Arm~3: $\{\mathrm{Code~Gen}, \mathrm{Solver~Execution}\}$. Introducing Arm~2 makes the problem tractable. 

%Why the problem hasn't been solved empirically. 
Empirically, this framework enables controlled comparisons. A rigorous comparison has not been instantiated because it is hard to control the experiment and determine what representation is actually being used---whether code, natural language, or something else. We overcome this by verbalizing the representation using Chain-of-Thought. We provide statistical evidence for the alternative hypothesis $\mathrm{Arm~3} > \mathrm{Arm~2} > \mathrm{Arm~1}$ on (Gemma, Deepseek, Llama), we demonstrate that generating code and executing leads to 78\% accuracy on (ILP, DP, Arithmetic) tasks, over 30\% for code simulation and just over 20\% for natural language reasoning across 5 models. Our results are computed over 5 seeds, and a Friedman Chi-square gives results a test statistic of 9277.32 and 8369.34 for deepseek and llama, enabling us to reject the null in favor of the alternative. 

Theoretically, we first compare $\mathrm{Arm~1} < \mathrm{Arm~2}$. Bayesian Inference shows us that the LLM implicitly does multi-class classification to the right algorithm. We utilize information theory to capture natural language and code under the same framework, forgoing using grammars or other mathematical frameworks that are intractable. We reduce the comparison of Bayes Error to that of comparing cross-entropy. A intermediate step using mutual information makes the proof interpretable: we prove that the mutual information between the CoT and the final answer is higher when conditioned on code representations over natural language representations. We variationally lower bound the mutual information using cross-entropy of a proposal distribution parameterized by a logistic regression. Since we only care about orderings, we subtract to overcome the intractability of estimating the differential entropy, reducing the comparison of mutual information to that of cross-entropy. The cross-entropy is measured empirically using logistic regression on TF-IDF features and Bert-base-uncased features, showing that code has lower cross-entropy than NL and achieves higher accuracy when classifying the correct algorithm. The difference is statistically significant (F-test, $p < 0.05$). %no need to say the details of reduction and not using LLM uncalibrated probs? 

Then we compare $\mathrm{Arm~2} < \mathrm{Arm~3}$. This difference is easily explained using a communication channel model of LLM forward-pass. We show that in the case where Arm~2 $>$ Arm~3, i.e. when the code generated is not executable or wrong, yet the LLM reasoning obtains the correct answer, that this occurs rarely. In other words, generally $\mathrm{Arm~3} > \mathrm{Arm~2}$. 

Piecing these results together, we show that $\mathrm{Arm~1} < \mathrm{Arm~2} < \mathrm{Arm~3}$, verifying the hypothesis.

% % Claim: Statistical Learning and Computational Learning Hard to deal with our setting. Therefore we need 3 arms. Explain why its hard, and what the gap is in literature. 
%  The reason for this is because comparing structured programming languages and unstructured natural language is challenging. Typically, statistical learning theory is concerned with learning a hypothesis class mapping $\mathcal{X} \to \mathcal{Y}$. We compare two learning algorithms and see how many samples of $\mathcal{X}$ are required to achieve a loss $L$. What if now, the $X$ are different across learning algorithms? This makes comparison between NL (1) and Code (2) \emph{mutually} intractable. 

% % Computational Challenges
% From a computational perspective, we may be curious whether code representations improve expressivity, quantified by less CoT iterations to achieve a certain loss. As it turns out, it is again hard to formalize the comparison, since a fundamental problem is trying to capture natural language in a mathematical structure, e.g. a grammar, which overcomes its ambiguity while maintain expressivity. 

% Solution

% Experimental Results / Phenomenon


% Explanation
\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{main.png}
    \caption{Accuracy scaling across task difficulty ($\tau$) for Arithmetic, Dynamic Programming, and ILP tasks. Arm~3 (code execution) maintains high accuracy as problems get harder, while Arms~1 and~2 degrade. The widening gap demonstrates that solver execution becomes increasingly advantageous for challenging algorithmic problems.}
    \label{fig:main}
\end{figure*}

Understanding this problem is crucial as we move towards compositional AI systems rather than monolithic architectures.

Our main contributions are:
\begin{enumerate}
    \item A \textbf{three-arm framework} for tractable comparison between code and natural language representations via an intermediary (code generation with LLM execution).
    \item A \textbf{theoretical explanation} based on Bayesian inference showing that code yields higher mutual information with target algorithms, leading to lower Bayes error.
    \item \textbf{Empirical validation} demonstrating that code execution achieves 78\% accuracy versus 21\% for natural language reasoning across arithmetic, DP, and ILP tasks ($p < 0.05$).
\end{enumerate}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{line.png}
    \caption{Average accuracy across arms for all models and tasks. Code execution (Arm~3) outperforms code simulation (Arm~2), which outperforms natural language reasoning (Arm~1) on CLRS$_{30}$ (n=500), NPHardEval (n=270), and Fine-Grained evaluation (n=270) benchmarks across 3 seeds. Statistical significance measured by Wilcoxon signed-rank test between adjacent arms. Error bars show bootstrapped Wilson confidence intervals at the model level.}
    \label{fig:line}
\end{figure}

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{fig_prompts_combined.png}
    \caption{Recovery rate analysis: proportion of cases where LLM simulation (Arm~2) produces the correct answer despite incorrect code (Arm~3 failure). Recovery rates remain below 5\% across all task families and models, confirming that solver execution dominates LLM simulation.}
    \label{fig:recovery_final}
\end{figure*}


\section{Evaluation Framework}\label{method}
We formalize our central claim as $\mathrm{Acc}(\mathrm{Arm~1}) \leq \mathrm{Acc}(\mathrm{Arm~2}) < \mathrm{Acc}(\mathrm{Arm~3})$. The following sections detail how we break down the problem and evaluate pairwise $\mathrm{Acc}(\mathrm{Arm~1}) \leq \mathrm{Acc}(\mathrm{Arm~2})$ and then $\mathrm{Acc}(\mathrm{Arm~2}) \leq \mathrm{Acc}(\mathrm{Arm~3})$.

\subsection{$\mathrm{Acc}(\mathrm{Arm~2}) \leq \mathrm{Acc}(\mathrm{Arm~3})$}
Between Arms~2 and~3, the first stage of the inference pipeline is held constant: both arms use identical generated code. We treat LLM-based code simulation (Arm~2) as the baseline and compare it against code execution in a Python runtime (Arm~3). A potential confound arises because code execution does not observe the original prompt, whereas LLM simulation does---the model could shortcut directly from the prompt to the answer without faithfully simulating the code. To control for this, we introduce Arm~2.5, which masks the prompt before simulation, forcing the model to rely solely on the generated code. 

\subsection{$\mathrm{Acc}(\mathrm{Arm~1}) \leq \mathrm{Acc}(\mathrm{Arm~2})$}
We control the execution part of our inference (both llm simulation), intervening in the first section by replacing natural language (baseline) generation with code generation and observing the differences. Chaining together these results with 3.1, we can validate our claim. 



\subsection{Experiments} 

\textbf{Data.} We use the CLRS 30 Benchmark (n=500), NPHardEval Benchmark (n=270), and a custom fine-grained evaluation suite (n=270), across three seeds. We define our own task suite---Arithmetic, Dynamic Programming, Integer Linear Programming (ILP)---to modulate hardness with parameter $\tau$. For arithmetic, $\tau$ controls digit length; for DP, it controls table dimensionality; for ILP, it controls the constraint matrix size. These classical algorithms provide sufficient coverage to support general claims. 

\textbf{Models.} We select frontier models (Claude, GPT-4o, Gemini 2.5) as well as open-source models (Mistral, Llama, Qwen). Since we require structured output, we filter out models that give $>$50\% JSON Parse Error, since this is indicative of instruction-following failures, rather than outright lack of coding fidelity. 

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{discrimination_by_task.png}
    \caption{Recovery rate analysis: proportion of cases where LLM simulation (Arm~2) produces the correct answer despite incorrect code (Arm~3 failure). Recovery rates remain below 5\% across all task families and models, confirming that solver execution dominates LLM simulation.}
    \label{fig:recovery_final}
\end{figure*}

\textbf{Generating Code and Reasoning Traces.}
We prompt the LLM in Arm~1 to never use any code in its reasoning, and to give a structured output of the rationale and answer to the algorithmic problem. Similarly for Arm~2, we prompt the LLM to use code in its reasoning, generating a structured output that contains a piece of code, and an attempt at simulating the execution of that piece of code in natural language, followed by a final answer. For Arm~3, we take the generated function (no prompt), and execute it in a python3 runtime. Models have access to native python packages, and numpy, pandas, scipy, PuLP, and pytorch. \cref{fig:arm_prompts} illustrates the prompt templates used across all three arms. 



\textbf{Arm~1 $\leq$ Arm~2 $<$ Arm~3.} \cref{fig:line} demonstrates statistically significant gaps between all three arms ($p < 0.05$, Wilcoxon signed-rank test). Notably, the controlled simulation condition (Arm~2.5, prompt masked) performs worse than the standard simulation (Arm~2), confirming that models exploit prompt information to shortcut directly to answers rather than faithfully simulating code execution. This validates our experimental control: the gap between Arms~2 and~3 reflects genuine differences in execution fidelity, not confounds from prompt access. 

\textbf{Advantages of Arm~3 emerge as tasks get harder.} \cref{fig:main} shows fine-grained scaling behavior across task difficulty. As problem hardness $\tau$ increases, Arm~3 (code execution) maintains robust accuracy while Arms~1 and~2 degrade substantially. The gap widens with difficulty: at the highest $\tau$, Arm~3 outperforms Arm~2 by over 40 percentage points on ILP and DP tasks. For arithmetic, code simulation (Arm~2) performs relatively better at low $\tau$, likely because models have memorized digit-symbol relationships during pretraining. However, this advantage vanishes as digit length increases beyond memorized patterns.

\section{Evaluating Translated NL and NL Distributional Similarity.} 

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{translation_additivity_plot.png}
    \caption{Recovery rate analysis: proportion of cases where LLM simulation (Arm~2) produces the correct answer despite incorrect code (Arm~3 failure). Recovery rates remain below 5\% across all task families and models, confirming that solver execution dominates LLM simulation.}
    \label{fig:translation_additivity}
\end{figure}




One key hypothesis is that natural language reasoning follows a deeper algorithmic procedure encoded in its representations. If this is the case, it would make sense to surface the code and send it to an executor. We test whether natural language reasoning generated by information contained in the code alone can be distributionally similar to natural language reasoning generated from the prompt alone in Arm~1. Then, we test whether they are functionally similar. 

\subsection{Distributional Similarity between Translated NL and original NL}
We show that the distribution of traces produced directly from the reasoning model can be approximated by post-processing the code with a fixed function across tasks and models. For each task instance $x$ drawn from a held-out test distribution over 21 different algorithmic problems in CLRS, we consider two conditional distributions: Arm~1: $p_{NL} (\cdot \mid x)$, original NL traces generated by an LLM (Gemini 2.0 Flash in our case) and Arm~2 Translated: $p_{Translated} (\cdot \mid x)$, NL traces obtained by first generating code by an LLM, then translating that code into natural language using a fixed translator. 

\textbf{Evaluation Setup}  We formulate a binary classification task in which a powerful judge model (Claude Opus 4.5) is given a problem instance x, and a single reasoning trace z, and must predict whether z was generated by Arm~1, or the translated Arm~2. This setup corresponds to discriminating between the joint distributions $p(x)p_{NL}(z \mid x)$ and $p(x)p_{Translated}(z \mid x)$. For each evaluation task x, we construct two samples $(x, z_{NL})$, where $z_{NL} \sim p_{NL}(\cdot \mid X)$, labelled Original, and $(x, \hat z_{NL})$ where $\hat z_{NL} \sim p_{Translated} ( \cdot \mid x)$, labelled Translated. The label counts are balanced on the test set, and the judge's accuracy, and AUC are computed on held-out tasks (disjoint from any in-context prompting examples).

In the evaluation, all generation parameters are fixed, model, prompt, and decoding hyperparameters, and the only variability is the seed for both the original Arm~1 and translated Arm~2, i.e. we compare samples from two fixed conditionals given a task instance. 

We ensure the judge has enough discriminative power by asking it to classify between raw code and the native NL reasoning. High accuracy would indicate that the performance results on the main evaluation is not due to an underpowered judge. We prompt the judge zero-shot, instructing it that there are original natural language reasoning traces and translated ones that it must distinguish. 

The translator, gemini-2.0 flash, is given 10 in-context pairs between code in Arm~2 and original natural language reasoning in Arm~1, and asked to learn the mapping and apply on new instances. The 10 in-context examples are 10 different tasks, which are disjoint from the 21 other tasks in CLRS benchmark that we evaluate on. 



\textbf{Results.} Across 2000 samples, we find that the accuracy of the prompt is 49.4\% $\in [47.2\%. 51.5\%]$ Wilson CI, obtaining a 0.479 AUC. The control accuracy is $79.0\%$ so the discriminator is calibrated. We include per-task analysis in the appendix. We see that this is generally true across different models with only a few exceptions between rod cutting and the kruskal algorithm problems. 

% We hypothesize that the natural language reasoning in Arm 1 can be obtained by post-processing Arm 2. We fix a post-processer P, which is an LLM with a fixed prompt in our evaluation. The prompt in P has 10 in-context examples that shows how to translate between code and the original reasoning. We fix these across different tasks, and evaluate on held-out tasks in the CLRS 30 benchmark data (n=1500). We evaluate with a fixed translator (GPT 5.2 Pro) and a fixed evaluator (Claude Opus 4).

\subsection{Functional Similarity between Translated NL and original NL}

We wish to verify whether the translated natural language reaosning from the experiment before has the same functionality as the original natural language reasoning produced in Arm~1.

\textbf{Evaluation Setup}. Given a task instance $x$, we prompt the target language model (gemini) three ways. 1. Baseline: $x$ (question only), 2. Arm~1: $x || z_{NL}$ 3. Translated Arm~2: $x || \hat z_{NL}$ where $\hat z_{NL}$ is obtained by translating code to natural language. If the translated NL loses information relative to the original NL, then conditioning on $\hat z_{NL}$ should yield worse performance than conditioning on $z_{NL}$. We run on 1000 samples and report results on held-out tasks disjoint from the ICL example tasks. The same prompt is used for the translator in the previous experiment as the one used here. 

\textbf{Results}. We fail to reject the null hypothesis on 1000 samples, and see that using the original Arm~1 and the translated Arm~2 concatenated to the same original task prompt yield the same results. 

\textbf{Qualitative Analysis} In observing the LLM responses, we notice the model's have a propensity to follow similar reasoning. This happens across individual models, but problem solving methodology is surprisingly similar across models. Show figure here. 





% We wish to verify whether the translated NL and the NL are similar in distribution. By taking a piece of code, then translating and simulating it into natural language reasoning, we get a piece of CoT that is similar to the original. We ask whether this is distinguishable, in order to verify assumption 2. Assumption 1 is validated by design, since we can create a translator T via prompting an LLM. 

% Since the translator $T(Z_nl | Z_code)$ is independent of the task X, we prompt the translator LLM (GPT 5.2) to not see the task, but rather use a fixed prompt with in-context examples from adjacent translating tasks (5-shot). During the translation, test examples' problem tasks are not shown to the translator, only the fixed prompt and piece of code. The evaluation is conducted on held-out tasks not seen during the tuning of the prompt, and the prompt is frozen after tuning. Translator is applied uniformly across tasks, prompts, and decoding params. 




\section{Statistical and Information Theoretic Foundations of Algorithmic Reasoning}

We claim that natural language reasoning is a garbling of code \cite{blackwell1953equivalent} under a noisy channel paradigm of inference, leading code reasoning to be at least as good as NL reasoning.

\textbf{Setup.}
Let $X \sim p(x)$ denote the task instance (problem + inputs), drawn from a representative test distribution. Let $\mathcal{Y}$ be an output space (e.g., answer strings), and let $\ell : \mathcal{Y} \times X \to [0,1]$ be a bounded and measurable loss function (0--1 binary loss). In Arm~1 and Arm~2, each arm corresponds to an intermediate representation $Z$ (e.g. CoT) produced by channel $p(z \mid x)$ (e.g. LLM), and then choosing an output $Y$ via a randomized decision rule $\delta(y \mid x, z)$ (i.e. LLM test-time reasoning).

For any CoT observation $Z$, define the Bayes risk:
\begin{align*}
R^*(Z) &:= \inf_{\delta} \, \mathbb{E}[\ell(Y, X)], \\
&\quad X \sim p, \; Z \sim p(\cdot \mid X), \; Y \sim \delta(\cdot \mid X, Z).
\end{align*}

In the first arm, we observe $Z_{\mathrm{NL}} \sim p_{\mathrm{NL}}(\cdot \mid X)$.
For the second arm, we have $Z_{\mathrm{Code}} \sim p_{\mathrm{Code}}(\cdot \mid X)$.

We show that $R^*(Z_{\mathrm{Code}}) \leq R^*(Z_{\mathrm{NL}}) + O(\varepsilon)$ for some negligible $\varepsilon$.

\subsection{Assumptions}

\textbf{Assumption 1.}
We assume there exists a stochastic kernel $T$ such that the Markov chain $X \to Z_{\mathrm{Code}} \to \hat{Z}_{\mathrm{NL}}$ is representative of CoT, with the final decision stage being $\delta(y \mid X, \hat{Z}_{\mathrm{NL}})$. That is,
\begin{align*}
\hat{Z}_{\mathrm{NL}} &\sim p_{\mathrm{translation}}(\cdot \mid x), \; p_{\mathrm{translation}}(z \mid X)  \\
&:= \int T(z \mid z_{\mathrm{Code}}) \, p_{\mathrm{Code}}(z_{\mathrm{Code}} \mid x) \, \mathrm{d}z_{\mathrm{Code}}.
\end{align*}

\textbf{Assumption 2.}
We assume that the original NL reasoning chain of thought is close to the translated NL on average. Let $p_{\mathrm{NL}}$ be the Arm~1 channel and $p_{\mathrm{translated}}(\cdot \mid x)$ be the translated NL channel. Assume an average conditional TV bound:
\[
\mathbb{E}_{X \sim p} \bigl[
d_{\mathrm{TV}}\bigl(p_{\mathrm{NL}}(\cdot \mid X), \, p_{\mathrm{translated}}(\cdot \mid X)\bigr)
\bigr] \leq \varepsilon,
\]
where
\[
d_{\mathrm{TV}}(P, Q) = \sup_{B} |P(B) - Q(B)|.
\]

In other words, averaged over task instances, the NL trace produced by Arm~1 is close in distribution to the NL traces obtained by translating the code trace (Arm~2) using the translator $T$ (Markov kernel).

\subsection{Proof}

\begin{proof}
Under Assumptions~1--2, for the bounded loss $\ell \in [0,1]$,
\[
R^*(Z_{\mathrm{Code}}) \leq R^*(Z_{\mathrm{NL}}) + \varepsilon.
\]

\textbf{Step 1: Simulate NL from code via translation.}
Here we first translate the input problem into CoT, then execute the CoT:
\[
\delta_{\mathrm{Code}}(y \mid x, z_{\mathrm{Code}})
:= \int \underbrace{\delta_{\mathrm{NL}}(y \mid x, z)}_{\text{Execute}} \,
\underbrace{T(z \mid z_{\mathrm{Code}})}_{\text{Translate}} \, \mathrm{d}z.
\]

Let $Y_{\mathrm{Code}} \sim \delta_{\mathrm{Code}}(\cdot \mid X, Z_{\mathrm{Code}})$. Let $\hat{Y}_{\mathrm{translated}} \sim \delta_{\mathrm{NL}}(\cdot \mid X, \hat{Z}_{\mathrm{NL}})$, where $\hat{Z}_{\mathrm{NL}}$ is produced from $Z_{\mathrm{Code}}$ via the translator kernel $T$.

The joint distributions $(X, Y_{\mathrm{Code}})$ and $(X, \hat{Y}_{\mathrm{translated}})$ are the same. Thus,
\[
\mathbb{E}[\ell(Y_{\mathrm{Code}}, X)] = \mathbb{E}[\ell(\hat{Y}_{\mathrm{translated}}, X)].
\]

This is because conditional on $X = x$, sampling $Z_{code} \sim p_{code}(\cdot \mid x)$, then $\hat Z_{nl} \sim T(\cdot \mid Z_{code})$, then $Y \sim \delta_{NL} (\cdot \mid x, \hat Z_{NL})$ induces the same conditional distribution over Y as $Y \sim \delta_{code} (\cdot \mid x, Z_{code})$.

\textbf{Step 2: Substitute translated NL and original NL via TV lemma.}

\begin{lemma}[TV Lemma]
Let $X \sim p(x)$. Let $Z \mid X = x \sim P_x$ and $Z' \mid X = x \sim Q_x$. Let $g(x, z) \in [0, 1]$ be measurable. Then
\[
\mathbb{E}[g(X, Z)] - \mathbb{E}[g(X, Z')] \leq \mathbb{E}_X \bigl[ d_{\mathrm{TV}}(P_X, Q_X) \bigr].
\]
\end{lemma}

Suppose we have $Y_{\mathrm{NL}} \sim \delta_{\mathrm{NL}}(\cdot \mid Z_{\mathrm{NL}})$ under the actual channel $p_{\mathrm{NL}}(\cdot \mid x)$. For each $x$ and trace $z$, define $g(x, z) := \mathbb{E}_{y \sim \delta(\cdot \mid z)}[\ell(y, x)]$.

Then $g(x, z) \in [0, 1]$. Note that
\begin{align*}
\mathbb{E}[\ell(Y_{\mathrm{NL}}, X)] &= \mathbb{E}[g(X, Z_{\mathrm{NL}})], \\
\mathbb{E}[\ell(\hat{Y}_{\mathrm{translated}}, X)] &= \mathbb{E}[g(X, \hat{Z}_{\mathrm{NL}})].
\end{align*}

Applying the TV lemma with $P_x = p_{\mathrm{NL}}(\cdot \mid x)$ and $Q_x = p_{\mathrm{translated}}(\cdot \mid x)$:
\begin{align*}
&\bigl| \mathbb{E}[\ell(Y_{\mathrm{NL}}, X)] - \mathbb{E}[\ell(\hat{Y}_{\mathrm{translated}}, X)] \bigr| \\
&\quad = \bigl| \mathbb{E}[g(X, Z_{\mathrm{NL}})] - \mathbb{E}[g(X, \hat{Z}_{\mathrm{NL}})] \bigr| \\
&\quad \leq \mathbb{E}_X \bigl[ d_{\mathrm{TV}}(p_{\mathrm{NL}}(\cdot \mid X), \, p_{\mathrm{translated}}(\cdot \mid X)) \bigr]
\leq \varepsilon.
\end{align*}

Therefore, rearranging gives
\[
\mathbb{E}[\ell(\hat{Y}_{\mathrm{translated}}, X)] \leq \mathbb{E}[\ell(Y_{\mathrm{NL}}, X)] + \varepsilon.
\]
Thus,
\[
\mathbb{E}[\ell(Y_{\mathrm{Code}}, X)]
= \mathbb{E}[\ell(\hat{Y}_{\mathrm{translated}}, X)]
\leq \mathbb{E}[\ell(Y_{\mathrm{NL}}, X)] + \varepsilon.
\]

Since this holds for arbitrary NL rule $\delta_{\mathrm{NL}}$, taking the infimum over $\delta_{\mathrm{NL}}$ on the right-hand side yields $R^*(Z_{\mathrm{Code}}) \leq R^*(Z_{\mathrm{NL}}) + \varepsilon$.
\end{proof}

\subsection{Arm~2 $<$ Arm~3}

We model the comparison between LLM simulation (Arm~2) and solver execution (Arm~3) under the following assumptions:
\begin{enumerate}[leftmargin=*]
    \item \textbf{Solver correctness.} Given correct code $Z$, the solver deterministically produces the ground-truth answer: $Y_3 = g(X, Z) = Y^*(X)$.
    \item \textbf{Noisy simulation.} LLM simulation adds stochastic noise: $Y_2 \sim N(\cdot \mid Y_3, X, Z)$ for some noise kernel $N$.
    \item \textbf{0-1 loss.} We evaluate under $\ell(y, x) = \mathbf{1}\{y \neq Y^*(x)\}$.
\end{enumerate}

Under these assumptions, the solver achieves zero risk ($R_3 = 0$) whenever the generated code is correct. In contrast, LLM simulation incurs positive risk ($R_2 > 0$) whenever $\Pr[Y_2 \neq Y_3] > 0$, which occurs empirically due to execution errors in mental simulation. Therefore, $R_3 < R_2$.

The only scenario where Arm~2 could outperform Arm~3 is when the generated code is \emph{incorrect}, yet the LLM ``recovers'' by reasoning to the correct answer despite the flawed code. We empirically quantify this recovery rate below. 

\textbf{Recovery reduces as tasks get harder.}
To further reinforce this result, we rule out the possibilities of recovery as tasks get harder, eliminating any benefit of running Arm~2:
\begin{enumerate}[leftmargin=*]
    \item Arm~3 produces an incorrect answer (implying incorrect code generation), and
    \item Arm~2 produces the correct answer (implying successful LLM recovery).
\end{enumerate}

\cref{fig:recovery_final} presents the recovery analysis across all tasks and models. The recovery rate remains consistently low (typically $< 5\%$), indicating that LLM simulation rarely compensates for code generation errors. This confirms that Arm~3's advantage stems from reliable solver execution rather than Arm~2's inability to reason about code.

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{recovery_final.png}
    \caption{Recovery rate analysis: proportion of cases where LLM simulation (Arm~2) produces the correct answer despite incorrect code (Arm~3 failure). Recovery rates remain below 5\% across all task families and models, confirming that solver execution dominates LLM simulation.}
    \label{fig:recovery_final}
\end{figure}

% Add optimized prompt in appendix. 


% \begin{theorem}[Deterministic Solver Dominance under $0$--$1$ Loss]
%     \label{thm:deterministic_solver}
%     Let $X \sim p(x)$ be a task instance and let $Z \sim p_{\mathrm{Code}}(\cdot \mid X)$ be a code representation generated from $X$. 
%     Let $Y^*(X)$ denote the ground-truth answer.
    
%     Consider two execution arms:
    
%     \begin{itemize}
%         \item \textbf{Arm~2 (Noisy execution).} The final answer is
%         \[
%         Y_2 \sim p_2(\cdot \mid X, Z).
%         \]
        
%         \item \textbf{Arm~3 (Deterministic solver).} The final answer is
%         \[
%         Y_3 = g(X, Z).
%         \]
%     \end{itemize}
    
%     Assume:
%     \begin{enumerate}
%         \item (\emph{Solver correctness}) The solver output equals the ground-truth answer almost surely:
%         \[
%         Y_3 = Y^*(X) \quad \text{a.s.}
%         \]
%         \item (\emph{Noisy execution}) The noisy execution is obtained by adding noise to the solver output:
%         \[
%         p_2(y \mid X, Z) = N(y \mid Y_3, X, Z)
%         \]
%         for some stochastic kernel $N$.
%     \end{enumerate}
    
%     Let the loss be the $0$--$1$ loss,
%     \[
%     \ell(y, x) = \mathbf{1}\{y \neq Y^*(x)\}.
%     \]
    
%     Then the risks satisfy
%     \[
%     \boxed{
%     \mathbb{E}[\ell(Y_3, X)] \;\le\; \mathbb{E}[\ell(Y_2, X)],
%     }
%     \]
%     with strict inequality whenever
%     \[
%     \mathbb{P}(Y_2 \neq Y_3) > 0.
%     \]
%     \end{theorem}
    

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=1\linewidth]{statistical(1).png}
%     \caption{Bayesian Network of Inference Pipeline.}
%     \label{fig:statistical}
% \end{figure}

% \textbf{Task.}
% On a high level, we aim to show that Bayes error for code is less than for natural language: $p(\hat y \neq y^* | x, \mathrm{Code}) < p(\hat y \neq y^* | x, \mathrm{NL})$. 

% \textbf{Modeling } To do this, we model LLM forward pass as posterior inference, a paradigm similar to implicit Bayesian inference in in-context learning by \citet{xie2021explanation}. 

% Let $y,x$ be the inputs and outputs respectively. $x$ is defined as a natural language encoding of some algorithmic problem. Let the random variable $R \in \{\mathrm{Code}, \mathrm{NL}\}$ be the chain of thought representation. The random variable $\gamma$ represents the latent algorithm and the world variables. The latent algorithms are a piece of code (as opposed to a string describing the algorithm), whereas the world variables are inputs the algorithm takes in. Knowing $\gamma$ means we have all the information we need to solve the problem, and thus $\gamma$ is a sufficient statistic for $y$. For example, $\gamma = \{ \mathrm{addition}, 2,4\}$. We first assume that we perform this multi-class classification to the right algorithm implicitly through inferring $\gamma$, then condition on this to produce the right answer. 

% \begin{align}
%      & p(y|x,r_{i}) =  \int_{\gamma \in \Gamma}  p(y| x,\gamma, r_i)p(\gamma | x, r_i)  d\gamma
% \end{align}

% In the context of reasoning models, we suppose that the chain-of-thought helps to infer $\gamma$. 

% \begin{align}
%      & p(y|x,r_{i}) =  \int_{\gamma \in \Gamma} \int_z p(y| x,\gamma, r_i)p(\gamma |z, x, r_i) p(z|x,r_i) d\gamma
% \end{align}

% In this section we explain why the code representations $R = \mathrm{Code}$ has lower Bayes error rate $P^*$ than natural language representations $R = \mathrm{NL}$. This amounts to proving that $\text{Arm 3} > \text{Arm 1}$.

% This section explains a theory that sets up the experiments later that follow. First, we show that $\mathrm{Arm~1} < \mathrm{Arm~2}$, then $\mathrm{Arm~2} < \mathrm{Arm~3}$

% \subsection{Arm 1 $<$ Arm 2}

% We being by defining a few lemmas, proving them and showing why they are necessary. 

% \begin{lemma} \label{lem:1}
%     For markov chain $Z_r \to \gamma \to Y$, the following equality holds:
%     $\mc I(Y, Z_{\mathrm{Code}}) - \mc I(Y, Z_{\mathrm{NL}}) = \mc I(\gamma, Z_{\mathrm{Code}}) - \mc I(\gamma, Z_{\mathrm{NL}})$. 
% \end{lemma}

% \begin{proof}
%     $ [\mc I(\gamma, Z_{\mathrm{Code}}) + \mc I(Y, \gamma) - H(\gamma)] - [\mc I(\gamma, Z_{\mathrm{NL}}) + \mc I(Y, \gamma) - H(\gamma)] $
%     \[\underbrace{\leq}_{\text{Chain Rule}} \mc I (Y, Z_{\mathrm{Code}}) - \mc I (Y, Z_{\mathrm{NL}})\] \[\underbrace{\leq}_{DPI} \mc I(\gamma, Z_{\mathrm{Code}}) - I(\gamma, Z_{\mathrm{NL}})\]
%     $ \implies \mc I(\gamma, Z_{\mathrm{Code}}) - \mc I(\gamma, Z_{\mathrm{NL}})$ 
%     \[\underbrace{\leq}_{\text{Chain Rule}} \mc I (Y, Z_{\mathrm{Code}}) - \mc I (Y, Z_{\mathrm{NL}})\] \[ \underbrace{\leq}_{DPI} \mc I(\gamma, Z_{\mathrm{Code}}) - I(\gamma, Z_{\mathrm{NL}})\]
%     $ \implies \mc I (Y, Z_{\mathrm{Code}}) - \mc I (Y, Z_{\mathrm{NL}})$
%     \[= \mc I(\gamma, Z_{\mathrm{Code}}) - I(\gamma, Z_{\mathrm{NL}})\]
% \end{proof}

% Here we show that the mutual information between Y and Z is the same as the mutual information between $\gamma$ and Z. This is useful because we don't need to use the conditional distribution $P(y|Z)$ which is generally uncalibrated for LLMs, and hard to obtain, since LLMs may generate prose and not just the final answer. We may instead work with $P(\gamma|Z)$, which can later be estimated variationally by a proposal distribution. 

% \begin{lemma} [Variational Lower Bound] \label{lem:2}
%     $\mc I( \gamma, Z_{r}) \geq H(\gamma) + \max\limits_\theta \mb E_{\gamma,z} q(\gamma| z,\theta)$ where $\theta$ specifies the model drawn from our parametric family, and $q$ is some valid probability distribution \citep{poole2019variational}. 
% \end{lemma} 

% We can lower bound the mutual information with the cross entropy, which we use in experiments \cref{exp:1}. We apply this lemma in \cref{cor:1}.

% \begin{proof}
%     Start with the definition of Mutual Information. 
%     \begin{align}
%         & \mc I ( \gamma ,Z) = H(\gamma) - \mb E_Z H(\gamma|Z=z) \\
%         & = H(\gamma) + \mb E_Z \mb E_{\gamma | Z} \log p(\gamma | Z=z) \\ %&& \text{Since } H(\gamma|Z) = - \sum_Z p(\gamma |Z) \log p(\gamma|Z) \\
%         & = H(\gamma) + \mb E_{\gamma, Z} \log \frac{p(\gamma|Z)q(\gamma |Z)}{q(\gamma|Z)} \\ %&& \text{Variational Trick} \\
%         & = H(\gamma) + \mb E_{\gamma,Z} \log q(\gamma  | Z) + \mb E_{\gamma,Z} \frac{p(\gamma|Z)}{q(\gamma|Z)} \\
%         &= H(\gamma) +\mb E_{\gamma,Z} \log q(\gamma,Z) + KL(q(\gamma|Z) || p(\gamma|Z)) \\
%         & \geq H(\gamma) + \mb E_{\gamma,Z} \log q(\gamma|Z)
%     \end{align}
%     Therefore, to get the tightest bound we can write, 
%     $  \mc I ( \gamma ,Z)\geq H(\gamma) + \max\limits_\theta \mb E_{\gamma,Z} \log q(\gamma|Z, \theta)$
% \end{proof}

% \begin{theorem} \label{thm:1}
%     For Markov Chain $Z_r \to \gamma \to Y$, higher mutual information $\mc I(\gamma, Z_{\mathrm{Code}}) > \mc I(\gamma, Z_{\mathrm{NL}})$ implies Bayes Error \[ P^*_{\mathrm{Code}} =\min_{\hat Y(\cdot) } P[ \hat Y(Z_{\mathrm{Code}}) = Y^*] \] \[ < \min_{\hat Y(\cdot) }  Pr[\hat Y(Z_{\mathrm{NL}}) = Y^*] = P^*_{\mathrm{NL}}\] 
% \end{theorem}

% \begin{proof}
%     Start with Fano's Inequality, 
%     \begin{align}
%              & H(Y|Z_r) \leq h(P^*_r) + P^*_r \log_2(K-1) \\
%              & = H(Y) - \mc I(Y, Z_r) \leq h(P^*_r) + P^*_r \log_2(K-1) \\
%              & =  \frac{H(Y) - \mc I(Y, Z_r) -  h(P^*_r)}{\log_2(K-1)} \leq P^*_r \\
%              & =  \frac{H(Y) - \mc I(Y, Z_r) - 1}{\log_2(K-1)} \leq P^*_r  % (h(P^*_r) \leq 1)
%     \end{align}

%     Take the difference of $P^*_{\mathrm{NL}} - P^*_{\mathrm{Code}}   \geq  \frac{ \mc I(Y, Z_{\mathrm{Code}}) - \mc I(Y, Z_{\mathrm{NL}}) }{ \log_2 (K-1)} \underbrace{=}_{Lemma \; 1} \frac{ \mc I( \gamma, Z_{\mathrm{Code}}) - \mc I(\gamma, Z_{\mathrm{NL}}) }{ \log_2 (K-1)} $. 
% \end{proof}

% \begin{corollary} \label{cor:1}
%     Let $H_{CE}(r) = -\max_\theta \mb E_{\gamma,z} \log q(\gamma| z,\theta, r)$ denote the minimum cross-entropy achievable for representation $r$. When the cross-entropy for code is lower than for natural language:
%     \begin{equation}
%         H_{CE}(\mathrm{Code}) < H_{CE}(\mathrm{NL}),
%     \end{equation}
%     we have that the Bayes error for code is lower:
%     \begin{equation}
%         P^*_{\mathrm{Code}} < P^*_{\mathrm{NL}}.
%     \end{equation}
% \end{corollary}

% \begin{proof}
%     % See \cref{app:cor1_proof}.
%     Let $L_r = \max_\theta \mb E_{\gamma,z} \log q(\gamma| z,\theta, r)$ denote the optimal expected log-likelihood for representation $r$. Note that $H_{CE}(r) = -L_r$.

%     From \cref{lem:2}, the mutual information is lower bounded by:
%     \begin{align}
%         \mc I(\gamma, Z_r) \geq H(\gamma) + L_r.
%     \end{align}

%     Taking the difference between code and NL:
%     \begin{align}
%         &\mc I(\gamma, Z_{\mathrm{Code}}) - \mc I(\gamma, Z_{\mathrm{NL}}) \nonumber \\
%         &\quad \geq [H(\gamma) + L_C] - [H(\gamma) + L_{NL}] \nonumber \\
%         &\quad = L_C - L_{NL} = H_{CE}(\mathrm{NL}) - H_{CE}(\mathrm{Code}).
%     \end{align}

%     When $H_{CE}(\mathrm{Code}) < H_{CE}(\mathrm{NL})$, equivalently $L_C > L_{NL}$, the RHS is positive:
%     \begin{equation}
%         \mc I(\gamma, Z_{\mathrm{Code}}) > \mc I(\gamma, Z_{\mathrm{NL}}).
%     \end{equation}

%     Applying \cref{thm:1}:
%     \begin{align}
%         P^*_{\mathrm{NL}} - P^*_{\mathrm{Code}} \geq \frac{\mc I(\gamma, Z_{\mathrm{Code}}) - \mc I(\gamma, Z_{\mathrm{NL}})}{\log_2(K-1)} > 0.
%     \end{align}

%     Therefore, $P^*_{\mathrm{Code}} < P^*_{\mathrm{NL}}$.
% \end{proof}

% \subsection{Arm~2 $<$ Arm~3}
% To explain why Arm 2 $<$ Arm 3, we model LLM forward pass as stochastic inference. The translation stage is the same, and the only difference is the execution stage. We model the inference as communication of information about the problem through a channel. The channel is noisy for LLM execution, and deterministic for solver inference. Therefore, the deterministic solver inference has exactly mutual information of 1 between the CoT Z and Y, and is an upper bound to the stochastic inference. 

% \subsection{Intractability of Direct Comparison and Insight}
% \textbf{Intractability.}
% % Why is it mutually intractable. 
% Directly showing $\text{Arm 3} > \text{Arm 1}$ is mutually intractable, but by introducing an intermediate step in Arm 2, the comparison between Arm 1 and Arm 2 is tractable, and Arm 2 and Arm 3 are tractable. Bayesian inference captures the breakdown nicely via marginalization. Furthermore, by moving into the realm of probabilistic inference, we can introduce information theory \cite{ton2024understanding, altabaa2025cot} as a unifying framework, sidestepping the intractability of mathematical frameworks attempting to capture natural language, e.g. CFG, CCGs. We know that parsing natural language to one of these frameworks optimally is NP-complete.

% \textbf{Insight.} The gap between Arms~1 and~2 is explained by code's higher mutual information with the target algorithm (\cref{cor:1}). The gap between Arms~2 and~3 is explained by the deterministic vs. stochastic execution channel. Together, these provide a principled theoretical account of why code execution outperforms natural language reasoning for algorithmic tasks.

% % \subsection{Summary}

% % Combining the results from Sections 6.1 and 6.2, we establish the complete ordering:
% % \begin{equation}
% %     \text{Arm 1} < \text{Arm 2} < \text{Arm 3}
% % \end{equation}



% \section{Empirical Validation of Theory} \label{exp:1}

% The intuition behind the theory is that representations that better predict the concepts (lower cross-entropy on test set), lead to better end performance (lower Bayes error). We begin by showing that first code representations have lower cross-entropy on the test set, and then show that this cross-entropy correlates with end task performance. 

% % Give example? 
% \subsection{Code representations achieve lower cross entropy than natural language representations. } We begin by extracting CoT text from our previous evaluation, which are embedded by MPNET (dim=384). For each CoT (n=2000) we construct ground-truth label $\gamma$ by combining the algorithm type and input variables, which intuitively represent all the necessary information required to solve the problem. For each CoT text, we know the information required to generate it by construction. To address the sparsity issue, we bin the input variables on a log scale (\# classes K=781). Then, we run a logistic regression estimator to use the CoT embeddings to predict the label, and compare code versus natural language. Evaluations are reported on the 20/80 test set split. In our experiments, we tried removing comments and the algorithm name mentioned in the CoT's, but found that this made no significant difference. Furthermore, we only use paired data where there was no JSON Parse error, since this would imply the code block was blank. We do not differentiate between correct or incorrect labels on the end task when deciding what data to use, i.e. a data point generated to solve addition with input variables (2,5) may get the wrong answer (e.g. 7), yet the label would still be (+,2,5).

% This section empirically validates the theoretical predictions from \cref{cor:1}. A requirement from theory is that the cross entropy of a classification is lower for code than natural language. Intuitively structured code leads to better representations for prediiction, posessing higher mutual information with the label. We empirically show that cross-entropy values are correlated with final accuracies on end tasks. I.e. better representations help disambiguate concepts, leads to better prediction. 

% \begin{figure}
%     \centering
%     \includegraphics[width=1\linewidth]{vlb.png}
%     \caption{Cross-entropy comparison between code and natural language representations. Code achieves significantly lower cross-entropy (higher mutual information bound) than NL for both TF-IDF and BERT features, validating \cref{cor:1}.}
%     \label{fig:vlb}
% \end{figure}


% \subsection{Cross Entropy is predictive of final performance.} We treat each (task, seed, representation) as a datapoint, and evaluate the cross entropy of the CoT's extracted from the corresponding datapoint and obtain the final end accuracy from the previous performance evaluation. Then, we measure how predictive the cross entropy is of the final performance. 



% We design experiments to test whether code representations yield higher mutual information with target algorithms, thereby explaining the performance gap observed in \cref{exp:0}.

% \subsection{Validating Arm 1 $<$ Arm~2: Mutual Information Estimation}

% \textbf{Hypothesis.} According to \cref{cor:1}, if code representations achieve lower cross-entropy when predicting the latent algorithm $\gamma$, then code should yield lower Bayes error. We test this by estimating the cross-entropy for both representations. 

% \textbf{Methodology.} We estimate cross-entropy using a plug-in estimator based on logistic regression, motivated by recent evidence that linear classifiers emerge during transformer inference \cite{bai2023transformers}. The experimental setup is as follows:

% \begin{itemize}[leftmargin=*]
%     \item \textbf{Features}: We extract features from chain-of-thought traces using two methods: (1) TF-IDF vectors and (2) BERT embeddings (bert-base-uncased).
%     \item \textbf{Labels}: For each CoT, we construct the ground-truth label $\gamma$ by combining the algorithm type and discretized world variables. World variables are binned in increments of 10 to address sparsity, yielding $K = 761$ classes.
%     \item \textbf{Model}: Logistic regression trained with SAGA solver for 20 iterations on an 80/20 train/test split.
%     \item \textbf{Data}: CoT traces from the Deepseek-Coder experiments in \cref{exp:0}.
% \end{itemize}

% \textbf{Results.} \cref{fig:vlb} shows that code representations achieve significantly lower cross-entropy and higher classification accuracy than natural language representations for both feature types (F-test, $p < 0.05$). The figure displays the negative log-likelihood (cross-entropy) on the test set, computed as $H_{CE} = -\frac{1}{N}\sum_i \log q(\gamma_i | z_i, \theta^*)$ where $\theta^*$ are the fitted logistic regression parameters. Lower values indicate better discriminability of the underlying algorithm from the CoT representation.

% \textbf{Quantifying the Bayes Error Improvement.} From our cross-entropy estimates, we compute lower bounds on mutual information:
% \begin{align}
%     \mc I(\gamma, Z_{\mathrm{Code}}) &\geq H(\gamma) + L_{\mathrm{Code}} = 4.4952 \\
%     \mc I(\gamma, Z_{\mathrm{NL}}) &\geq H(\gamma) + L_{\mathrm{NL}} = 3.8467
% \end{align}

% Applying \cref{thm:1} with $K = 761$ classes:
% \begin{align}
%     P^*_{\mathrm{NL}} - P^*_{\mathrm{Code}} &\geq \frac{\mc I(\gamma, Z_{\mathrm{Code}}) - \mc I(\gamma, Z_{\mathrm{NL}})}{\log_2(K-1)} \nonumber \\
%     &\geq \frac{4.4952 - 3.8467}{\log_2(760)} = 0.0678
% \end{align}

% This yields a \textbf{lower bound of 6.78\% improvement} in Bayes error rate when using code over natural language. Notably, our empirical results in \cref{exp:0} show a 9\% improvement (30\% vs. 21\%), which exceeds this theoretical lower bound as expected. 



% \subsection{Validating Arm 2 $<$ Arm~3: Recovery Analysis} \label{exp:2}

% \textbf{Hypothesis.} Solver execution (Arm~3) should outperform LLM simulation (Arm~2) because the solver is deterministic: given correct code, it always produces the correct answer. The only scenario where Arm 2 could outperform Arm 3 is when the generated code is incorrect, yet the LLM ``recovers'' by reasoning its way to the correct answer despite the flawed code. We hypothesize that such recovery events are rare.

% \textbf{Methodology.} Using the data from \cref{exp:0}, we identify cases where:
% \begin{enumerate}[leftmargin=*]
%     \item Arm 3 produces an incorrect answer (implying incorrect code generation), and
%     \item Arm 2 produces the correct answer (implying successful LLM recovery).
% \end{enumerate}
% We compute the \emph{recovery rate} as the proportion of Arm 3 failures where Arm 2 succeeds.



% \textbf{Correlation Analysis.} To validate that mutual information predicts task performance, we examine the relationship between our MI estimates and observed accuracy. For each task-representation pair, we compute: (1) the MI lower bound from the logistic regression cross-entropy, and (2) the empirical accuracy from \cref{exp:0}. We aggregate results across task families (arithmetic, DP, ILP) and representations (code, NL).

% \begin{figure}
%     \centering
%     \includegraphics[width=1\linewidth]{mi_v_acc.png}
%     \caption{Relationship between estimated mutual information and task accuracy. Each point represents a task-representation pair. Higher mutual information (lower cross-entropy) correlates with improved accuracy ($r = 0.73$, $p < 0.01$), supporting the theoretical prediction that code's informational advantage translates to performance gains.}
%     \label{fig:mi_v_acc}
% \end{figure}


% \textbf{Distribution Analysis.} To visualize the full accuracy distributions beyond summary statistics, we compute kernel density estimates (KDE) for each arm. For each model-task-seed combination, we compute the per-sample accuracy, yielding a distribution of accuracies per arm. We apply Gaussian KDE with Scott's bandwidth selection rule to smooth the empirical distributions.

% \begin{figure}
%     \centering
%     \includegraphics[width=1\linewidth]{kde.png}
%     \caption{Kernel density estimation of accuracy distributions across the three arms. Arm 3 (code execution) exhibits a mode near 100\% accuracy, while Arms~1 and~2 show broader, lower-accuracy distributions. The bimodal structure of Arm 3 reflects the dichotomy between correct and incorrect code generation---when code is correct, execution is perfect.}
%     \label{fig:kde}
% \end{figure}

% \textbf{Interpretation.} These results validate the channel model from Section 5.2: solver execution acts as a noiseless channel (deterministic mapping from code to output), while LLM simulation acts as a noisy channel that introduces errors even when the ``transmitted'' code is correct. The low recovery rate confirms that this channel noise is the primary driver of Arm 2's inferior performance.

\section{Related Work and Discussion}

\textbf{Neuro-symbolic Learning.} This paper builds on research in neuro-symbolic integration \cite{graves_neural_2014, velickovic_neural_2021, reed_neural_2016, graves_hybrid_2016}, which combines neural networks with symbolic reasoning systems. These approaches are motivated by cognitive science \cite{schneider_controlled_2003, risko_cognitive_2016, anderson_neural_2010}, hierarchical reinforcement learning \cite{kolter_hierarchical_2007, dietterich_hierarchical_2000}, and compositionality research \cite{hudson_compositional_2018, hupkes_compositionality_2020, andreas_neural_2017, poggio2017and}. An orthogonal line of work explores direct execution of algorithms by neural networks \cite{velickovic_neural_2021, mahdavi_towards_2023, ibarz_generalist_2022, yan_neural_2020}. Unlike these approaches that focus on \emph{how} to integrate neural and symbolic components, our work addresses \emph{why} symbolic execution outperforms neural reasoning for algorithmic tasks.

\textbf{LLM Reasoning.} Recent work has explored various reasoning paradigms for LLMs, including symbolic reasoning \cite{marra_integrating_2019, olausson_linc_2023, han_folio_2024}, chain-of-thought prompting \cite{altabaa2025cot, zelikman_star_2022, merrill_expressive_2024, altabaa_cot_2025}, and in-context learning \cite{xie2021explanation, garg2022can, akyurek2022learning, zhang2024trained}. \citet{xie2021explanation} model in-context learning as implicit Bayesian inference, which we extend to compare different reasoning representations. While prior work demonstrates \emph{that} certain prompting strategies improve performance, we provide a theoretical framework explaining \emph{why} code representations lead to lower Bayes error.

\textbf{LLM Tool-Use.} Tool-augmented LLMs have achieved strong empirical results \cite{shen_llm_2024, schick_toolformer_nodate, qin_toolllm_2023, tang_toolalpaca_2023, parisi_talm_2022}. Code generation for tool-use can be viewed as a form of semantic parsing \cite{shin_few-shot_2022, krishnamurthy_neural_2017, berant_semantic_2013, dong_language_2016} or function calling \cite{puri_codenet_2021, alon_code2vec_2019, chen_neural_2018}. Our work complements this literature by providing theoretical justification for the observed empirical advantages of code-based tool-use over direct natural language reasoning. 

% \section{Discussion}

% \textbf{Summary.} This paper addresses whether algorithmic problems encoded in natural language should be solved via direct reasoning or by translating to code and executing with a solver. Our three-arm framework demonstrates that code execution consistently outperforms both code simulation and natural language reasoning, with theoretical backing from information theory.

% \textbf{Interpretation.} The theoretical analysis reveals that code representations yield higher mutual information with target algorithms than natural language representations. This explains the empirical observation: code serves as a more discriminative intermediate representation for implicit algorithm classification during LLM inference. The Bayesian framework makes this comparison tractable by decomposing the problem into translation and execution phases.

\section{Conclusion}

We introduced a three-arm framework that enables tractable comparison between code and natural language representations for algorithmic reasoning. By modeling LLM inference as Bayesian inference, we proved that code representations yield higher mutual information with target algorithms, leading to lower Bayes error. Empirically, code execution achieves 78\% accuracy compared to 21\% for natural language reasoning across arithmetic, dynamic programming, and integer linear programming tasks.

An interesting direction for future work is understanding \emph{why} code has higher mutual information---whether this emerges from pretraining data distributions or from inherent structural properties of programming languages. Our framework provides a foundation for such investigations.

\textbf{Limitations.} 

Our study focuses on algorithmic tasks (arithmetic, DP, ILP) where ground truth is well-defined. The results may not generalize to open-ended reasoning tasks without clear algorithmic structure. Additionally, our theoretical bounds are asymptotic---the 6\% Bayes error improvement is a lower bound that may not reflect finite-sample performance. Finally, we evaluate on a limited set of models (Deepseek, Gemma); behavior may differ for other architectures.

\textbf{Future Work.} These findings have practical implications for AI system design: for algorithmically structured problems, compositional systems with symbolic execution should be preferred over monolithic neural reasoning. This supports the growing trend toward tool-augmented LLMs.


\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix

% \section{Proof of Corollary 1} \label{app:cor1_proof}

% \begin{proof}
% Let $L_r = \max_\theta \mb E_{\gamma,z} \log q(\gamma| z,\theta, r)$ denote the optimal expected log-likelihood for representation $r$. Note that $H_{CE}(r) = -L_r$.

% From \cref{lem:2}, the mutual information is lower bounded by:
% \begin{align}
%     \mc I(\gamma, Z_r) \geq H(\gamma) + L_r.
% \end{align}

% Taking the difference between code and NL:
% \begin{align}
%     &\mc I(\gamma, Z_{\mathrm{Code}}) - \mc I(\gamma, Z_{\mathrm{NL}}) \nonumber \\
%     &\quad \geq [H(\gamma) + L_C] - [H(\gamma) + L_{NL}] \nonumber \\
%     &\quad = L_C - L_{NL} = H_{CE}(\mathrm{NL}) - H_{CE}(\mathrm{Code}).
% \end{align}

% When $H_{CE}(\mathrm{Code}) < H_{CE}(\mathrm{NL})$, equivalently $L_C > L_{NL}$, the RHS is positive:
% \begin{equation}
%     \mc I(\gamma, Z_{\mathrm{Code}}) > \mc I(\gamma, Z_{\mathrm{NL}}).
% \end{equation}

% Applying \cref{thm:1}:
% \begin{align}
%     P^*_{\mathrm{NL}} - P^*_{\mathrm{Code}} \geq \frac{\mc I(\gamma, Z_{\mathrm{Code}}) - \mc I(\gamma, Z_{\mathrm{NL}})}{\log_2(K-1)} > 0.
% \end{align}

% Therefore, $P^*_{\mathrm{Code}} < P^*_{\mathrm{NL}}$.
% \end{proof}

\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
